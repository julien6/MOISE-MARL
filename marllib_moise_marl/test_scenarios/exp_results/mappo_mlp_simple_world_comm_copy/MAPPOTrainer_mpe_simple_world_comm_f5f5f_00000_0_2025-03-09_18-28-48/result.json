{"episode_reward_max": -38.08425587795351, "episode_reward_min": -105.16393252669111, "episode_reward_mean": -71.62409420232231, "episode_len_mean": 25.0, "episode_media": {}, "episodes_this_iter": 2, "policy_reward_min": {"policy_leadadversary_": -2.399035557624381, "policy_adversary_": -2.2223381552451884, "policy_agent_": -96.35415811344997}, "policy_reward_max": {"policy_leadadversary_": 8.302786016962129, "policy_adversary_": 9.161857338996002, "policy_agent_": -0.7163725824305123}, "policy_reward_mean": {"policy_leadadversary_": 2.951875229668874, "policy_adversary_": 3.1246761346095298, "policy_agent_": -41.974998917909886}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-105.16393252669111, -38.08425587795351], "episode_lengths": [25, 25], "policy_policy_leadadversary__reward": [-2.399035557624381, 8.302786016962129], "policy_policy_adversary__reward": [-1.6741618220129826, -2.2223381552451884, -1.7978662959280882, 6.195230400528265, 9.085335341319167, 9.161857338996002], "policy_policy_agent__reward": [-0.7163725824305123, -96.35415811344997, -3.617249732269532, -67.21221524348954]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6773518581016391, "mean_inference_ms": 2.8134841544955385, "mean_action_processing_ms": 0.11337972154804306, "mean_env_wait_ms": 3.3365604924220666, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 1, "timesteps_total": 50, "timesteps_this_iter": 0, "agent_timesteps_total": 300, "timers": {"sample_time_ms": 403.025, "sample_throughput": 124.062, "load_time_ms": 0.718, "load_throughput": 69649.684, "learn_time_ms": 83.801, "learn_throughput": 596.65, "update_time_ms": 2.22}, "info": {"learner": {"policy_leadadversary_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 0.2, "cur_lr": 0.0005, "total_loss": 2.9197115898132324, "policy_loss": -0.009634578675031591, "vf_loss": 2.9592673778533936, "vf_explained_var": 0.0035300254821777344, "kl": 0.00017146053805017214, "entropy": 2.995542049407959, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_adversary_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 0.19999999999999998, "cur_lr": 0.0005, "total_loss": 2.7280143102010093, "policy_loss": -0.014610538880030314, "vf_loss": 2.7724631230036416, "vf_explained_var": 0.005170365174611409, "kl": 0.0005667152972995693, "entropy": 2.9951610565185547, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_agent_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 0.2, "cur_lr": 0.0005, "total_loss": 1113.7708282470703, "policy_loss": -0.01906423643231392, "vf_loss": 1113.819808959961, "vf_explained_var": -7.41034746170044e-05, "kl": 0.0007578494868721031, "entropy": 2.9949986338615417, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}}, "num_steps_sampled": 50, "num_agent_steps_sampled": 300, "num_steps_trained": 50, "num_agent_steps_trained": 300}, "done": false, "episodes_total": 2, "training_iteration": 1, "trial_id": "f5f5f_00000", "experiment_id": "d79be65b63864cf3aab1c0ed35a1f497", "date": "2025-03-09_18-28-53", "timestamp": 1741541333, "time_this_iter_s": 0.4526538848876953, "time_total_s": 0.4526538848876953, "pid": 48554, "hostname": "julien-PC", "node_ip": "192.168.89.58", "config": {"num_workers": 1, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 50, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 50, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "Centralized_Critic_Model", "custom_model_config": {"env": "mpe", "env_args": {"continuous_actions": false, "max_cycles": 25, "map_name": "simple_world_comm"}, "mask_flag": false, "global_state_flag": false, "opp_action_in_cc": true, "agent_level_batch_update": false, "force_coop": false, "local_mode": false, "share_policy": "group", "evaluation_interval": 50, "framework": "torch", "num_workers": 1, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "checkpoint_freq": 20, "checkpoint_end": true, "restore_path": {"model_path": "", "params_path": ""}, "stop_iters": 9999999, "stop_timesteps": 2000000, "stop_reward": 999999, "seed": 321, "local_dir": "", "model_arch_args": {"hidden_state_size": 256, "core_arch": "mlp", "fc_layer": 2, "out_dim_fc_0": 128, "out_dim_fc_1": 64, "encode_layer": "128-256"}, "algorithm": "mappo", "space_obs": "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "space_act": "Discrete(20)", "num_agents": 6, "episode_limit": 25, "policy_mapping_info": {"simple_adversary": {"description": "one team attack, one team survive", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_crypto": {"description": "two team cooperate, one team attack", "team_prefix": ["eve_", "bob_", "alice_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_push": {"description": "one team target on landmark, one team attack", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_tag": {"description": "one team attack, one team survive", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_spread": {"description": "one team cooperate", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}, "simple_reference": {"description": "one team cooperate", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}, "simple_world_comm": {"description": "two team cooperate and attack, one team survive", "team_prefix": ["adversary_", "leadadversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_speaker_listener": {"description": "two team cooperate", "team_prefix": ["speaker_", "listener_"], "all_agents_one_policy": true, "one_agent_one_policy": true}}, "agent_name_ls": ["leadadversary_0", "adversary_0", "adversary_1", "adversary_2", "agent_0", "agent_1"]}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "mpe_simple_world_comm", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 50, "evaluation_num_episodes": 10, "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 100, "min_iter_time_s": 0, "timesteps_per_iteration": 0, "seed": 321, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_adversary_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}], "policy_leadadversary_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}], "policy_agent_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x7f88afef45e0>", "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "simple_optimizer": false, "monitor": -1, "use_critic": true, "use_gae": true, "lambda": 1.0, "kl_coeff": 0.2, "sgd_minibatch_size": 50, "shuffle_sequences": true, "num_sgd_iter": 2, "lr_schedule": null, "vf_loss_coeff": 1.0, "entropy_coeff": 0.01, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": null, "kl_target": 0.01, "vf_share_layers": -1}, "time_since_restore": 0.4526538848876953, "timesteps_since_restore": 0, "iterations_since_restore": 1, "perf": {"cpu_util_percent": 24.2, "ram_util_percent": 46.3}}
{"episode_reward_max": -7.994660195117557, "episode_reward_min": -105.16393252669111, "episode_reward_mean": -39.85553298865357, "episode_len_mean": 25.0, "episode_media": {}, "episodes_this_iter": 2, "policy_reward_min": {"policy_leadadversary_": -2.399035557624381, "policy_adversary_": -2.705072794727737, "policy_agent_": -96.35415811344997}, "policy_reward_max": {"policy_leadadversary_": 8.302786016962129, "policy_adversary_": 9.161857338996002, "policy_agent_": -0.49279858676528443}, "policy_reward_mean": {"policy_leadadversary_": 0.5214820172908364, "policy_adversary_": 0.774567809281551, "policy_agent_": -21.35035921689453}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-105.16393252669111, -38.08425587795351, -8.179283354852107, -7.994660195117557], "episode_lengths": [25, 25, 25, 25], "policy_policy_leadadversary__reward": [-2.399035557624381, 8.302786016962129, -1.9111450930988323, -1.90667729707557], "policy_policy_adversary__reward": [-1.6741618220129826, -2.2223381552451884, -1.7978662959280882, 6.195230400528265, 9.085335341319167, 9.161857338996002, -1.1497964664252314, -0.7244895816458103, -2.705072794727737, -1.9452619286739838, -0.5002713058248459, -2.428351018980954], "policy_policy_agent__reward": [-0.7163725824305123, -96.35415811344997, -3.617249732269532, -67.21221524348954, -1.1897859301531337, -0.49899348880136807, -0.7213000577969194, -0.49279858676528443]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6473819576775165, "mean_inference_ms": 2.8318407419375413, "mean_action_processing_ms": 0.11227561811455661, "mean_env_wait_ms": 3.330461401727403, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 1, "timesteps_total": 100, "timesteps_this_iter": 0, "agent_timesteps_total": 600, "timers": {"sample_time_ms": 434.394, "sample_throughput": 115.103, "load_time_ms": 0.59, "load_throughput": 84767.664, "learn_time_ms": 77.267, "learn_throughput": 647.104, "update_time_ms": 2.518}, "info": {"learner": {"policy_leadadversary_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 0.1, "cur_lr": 0.0005, "total_loss": 1.1552394032478333, "policy_loss": -0.006695239655673646, "vf_loss": 1.1918807625770569, "vf_explained_var": -0.006976664066314697, "kl": 6.103512204314043e-05, "entropy": 2.9952259063720703, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_adversary_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 0.09999999999999999, "cur_lr": 0.0005, "total_loss": 0.9293884014089903, "policy_loss": -0.009111778810620308, "vf_loss": 0.9684046854575475, "vf_explained_var": -0.0188066562016805, "kl": 0.0003505668809887415, "entropy": 2.9939517974853516, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_agent_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 0.1, "cur_lr": 0.0005, "total_loss": 0.16200217604637146, "policy_loss": -0.0026892870664596558, "vf_loss": 0.1945896614342928, "vf_explained_var": 0.019046500325202942, "kl": 0.0002835810563008856, "entropy": 2.992656946182251, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}}, "num_steps_sampled": 100, "num_agent_steps_sampled": 600, "num_steps_trained": 100, "num_agent_steps_trained": 600, "num_steps_trained_this_iter": 0}, "done": false, "episodes_total": 4, "training_iteration": 2, "trial_id": "f5f5f_00000", "experiment_id": "d79be65b63864cf3aab1c0ed35a1f497", "date": "2025-03-09_18-28-54", "timestamp": 1741541334, "time_this_iter_s": 0.42726683616638184, "time_total_s": 0.8799207210540771, "pid": 48554, "hostname": "julien-PC", "node_ip": "192.168.89.58", "config": {"num_workers": 1, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 50, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 50, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "Centralized_Critic_Model", "custom_model_config": {"env": "mpe", "env_args": {"continuous_actions": false, "max_cycles": 25, "map_name": "simple_world_comm"}, "mask_flag": false, "global_state_flag": false, "opp_action_in_cc": true, "agent_level_batch_update": false, "force_coop": false, "local_mode": false, "share_policy": "group", "evaluation_interval": 50, "framework": "torch", "num_workers": 1, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "checkpoint_freq": 20, "checkpoint_end": true, "restore_path": {"model_path": "", "params_path": ""}, "stop_iters": 9999999, "stop_timesteps": 2000000, "stop_reward": 999999, "seed": 321, "local_dir": "", "model_arch_args": {"hidden_state_size": 256, "core_arch": "mlp", "fc_layer": 2, "out_dim_fc_0": 128, "out_dim_fc_1": 64, "encode_layer": "128-256"}, "algorithm": "mappo", "space_obs": "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "space_act": "Discrete(20)", "num_agents": 6, "episode_limit": 25, "policy_mapping_info": {"simple_adversary": {"description": "one team attack, one team survive", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_crypto": {"description": "two team cooperate, one team attack", "team_prefix": ["eve_", "bob_", "alice_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_push": {"description": "one team target on landmark, one team attack", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_tag": {"description": "one team attack, one team survive", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_spread": {"description": "one team cooperate", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}, "simple_reference": {"description": "one team cooperate", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}, "simple_world_comm": {"description": "two team cooperate and attack, one team survive", "team_prefix": ["adversary_", "leadadversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_speaker_listener": {"description": "two team cooperate", "team_prefix": ["speaker_", "listener_"], "all_agents_one_policy": true, "one_agent_one_policy": true}}, "agent_name_ls": ["leadadversary_0", "adversary_0", "adversary_1", "adversary_2", "agent_0", "agent_1"]}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "mpe_simple_world_comm", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 50, "evaluation_num_episodes": 10, "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 100, "min_iter_time_s": 0, "timesteps_per_iteration": 0, "seed": 321, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_adversary_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}], "policy_leadadversary_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}], "policy_agent_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x7f88aff9ca60>", "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "simple_optimizer": false, "monitor": -1, "use_critic": true, "use_gae": true, "lambda": 1.0, "kl_coeff": 0.2, "sgd_minibatch_size": 50, "shuffle_sequences": true, "num_sgd_iter": 2, "lr_schedule": null, "vf_loss_coeff": 1.0, "entropy_coeff": 0.01, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": null, "kl_target": 0.01, "vf_share_layers": -1}, "time_since_restore": 0.8799207210540771, "timesteps_since_restore": 0, "iterations_since_restore": 2, "perf": {"cpu_util_percent": 18.5, "ram_util_percent": 46.3}}
{"episode_reward_max": 17.029253709543042, "episode_reward_min": -105.16393252669111, "episode_reward_mean": -34.22507035646618, "episode_len_mean": 25.0, "episode_media": {}, "episodes_this_iter": 2, "policy_reward_min": {"policy_leadadversary_": -4.038309224748798, "policy_adversary_": -3.094481311936268, "policy_agent_": -96.35415811344997}, "policy_reward_max": {"policy_leadadversary_": 8.302786016962129, "policy_adversary_": 9.161857338996002, "policy_agent_": 27.837397306389533}, "policy_reward_mean": {"policy_leadadversary_": -0.5241972473397079, "policy_adversary_": -0.17358723316612432, "policy_agent_": -16.590055704814052}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-105.16393252669111, -38.08425587795351, -8.179283354852107, -7.994660195117557, 17.029253709543042, -62.95754389372587], "episode_lengths": [25, 25, 25, 25, 25, 25], "policy_policy_leadadversary__reward": [-2.399035557624381, 8.302786016962129, -1.9111450930988323, -1.90667729707557, -4.038309224748798, -1.1928023284527944], "policy_policy_adversary__reward": [-1.6741618220129826, -2.2223381552451884, -1.7978662959280882, 6.195230400528265, 9.085335341319167, 9.161857338996002, -1.1497964664252314, -0.7244895816458103, -2.705072794727737, -1.9452619286739838, -0.5002713058248459, -2.428351018980954, -2.073914420541661, -2.491769450548833, -1.928789335869684, -1.6434950773450425, -1.1869343121273628, -3.094481311936268], "policy_policy_agent__reward": [-0.7163725824305123, -96.35415811344997, -3.617249732269532, -67.21221524348954, -1.1897859301531337, -0.49899348880136807, -0.7213000577969194, -0.49279858676528443, 27.837397306389533, -0.27536116513751435, -1.8093468032775135, -54.03048406058688]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6357300643908376, "mean_inference_ms": 2.8551416877531963, "mean_action_processing_ms": 0.11241475919826031, "mean_env_wait_ms": 3.3560342393392375, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 1, "timesteps_total": 150, "timesteps_this_iter": 0, "agent_timesteps_total": 900, "timers": {"sample_time_ms": 449.194, "sample_throughput": 111.311, "load_time_ms": 0.534, "load_throughput": 93567.162, "learn_time_ms": 73.902, "learn_throughput": 676.576, "update_time_ms": 2.264}, "info": {"learner": {"policy_leadadversary_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 0.05, "cur_lr": 0.0005, "total_loss": 2.9550979137420654, "policy_loss": -0.004633963815868114, "vf_loss": 2.9896775484085083, "vf_explained_var": -0.04325741529464722, "kl": 5.607679033792223e-05, "entropy": 2.9948534965515137, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_adversary_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 0.049999999999999996, "cur_lr": 0.0005, "total_loss": 1.0062826971213024, "policy_loss": -0.007980629801750183, "vf_loss": 1.0441262324651082, "vf_explained_var": -0.05092457930246989, "kl": 0.0004232275774599728, "entropy": 2.9884056647618613, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_agent_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 0.05, "cur_lr": 0.0005, "total_loss": 367.7681579589844, "policy_loss": -0.0015971958637237549, "vf_loss": 367.79967498779297, "vf_explained_var": -0.0005611330270767212, "kl": 0.00011508832393911095, "entropy": 2.98981773853302, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}}, "num_steps_sampled": 150, "num_agent_steps_sampled": 900, "num_steps_trained": 150, "num_agent_steps_trained": 900, "num_steps_trained_this_iter": 0}, "done": false, "episodes_total": 6, "training_iteration": 3, "trial_id": "f5f5f_00000", "experiment_id": "d79be65b63864cf3aab1c0ed35a1f497", "date": "2025-03-09_18-28-54", "timestamp": 1741541334, "time_this_iter_s": 0.4421970844268799, "time_total_s": 1.322117805480957, "pid": 48554, "hostname": "julien-PC", "node_ip": "192.168.89.58", "config": {"num_workers": 1, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 50, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 50, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "Centralized_Critic_Model", "custom_model_config": {"env": "mpe", "env_args": {"continuous_actions": false, "max_cycles": 25, "map_name": "simple_world_comm"}, "mask_flag": false, "global_state_flag": false, "opp_action_in_cc": true, "agent_level_batch_update": false, "force_coop": false, "local_mode": false, "share_policy": "group", "evaluation_interval": 50, "framework": "torch", "num_workers": 1, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "checkpoint_freq": 20, "checkpoint_end": true, "restore_path": {"model_path": "", "params_path": ""}, "stop_iters": 9999999, "stop_timesteps": 2000000, "stop_reward": 999999, "seed": 321, "local_dir": "", "model_arch_args": {"hidden_state_size": 256, "core_arch": "mlp", "fc_layer": 2, "out_dim_fc_0": 128, "out_dim_fc_1": 64, "encode_layer": "128-256"}, "algorithm": "mappo", "space_obs": "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "space_act": "Discrete(20)", "num_agents": 6, "episode_limit": 25, "policy_mapping_info": {"simple_adversary": {"description": "one team attack, one team survive", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_crypto": {"description": "two team cooperate, one team attack", "team_prefix": ["eve_", "bob_", "alice_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_push": {"description": "one team target on landmark, one team attack", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_tag": {"description": "one team attack, one team survive", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_spread": {"description": "one team cooperate", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}, "simple_reference": {"description": "one team cooperate", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}, "simple_world_comm": {"description": "two team cooperate and attack, one team survive", "team_prefix": ["adversary_", "leadadversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_speaker_listener": {"description": "two team cooperate", "team_prefix": ["speaker_", "listener_"], "all_agents_one_policy": true, "one_agent_one_policy": true}}, "agent_name_ls": ["leadadversary_0", "adversary_0", "adversary_1", "adversary_2", "agent_0", "agent_1"]}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "mpe_simple_world_comm", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 50, "evaluation_num_episodes": 10, "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 100, "min_iter_time_s": 0, "timesteps_per_iteration": 0, "seed": 321, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_adversary_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}], "policy_leadadversary_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}], "policy_agent_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x7f88aff00430>", "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "simple_optimizer": false, "monitor": -1, "use_critic": true, "use_gae": true, "lambda": 1.0, "kl_coeff": 0.2, "sgd_minibatch_size": 50, "shuffle_sequences": true, "num_sgd_iter": 2, "lr_schedule": null, "vf_loss_coeff": 1.0, "entropy_coeff": 0.01, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": null, "kl_target": 0.01, "vf_share_layers": -1}, "time_since_restore": 1.322117805480957, "timesteps_since_restore": 0, "iterations_since_restore": 3, "perf": {}}
{"episode_reward_max": 17.029253709543042, "episode_reward_min": -105.16393252669111, "episode_reward_mean": -29.577578155340817, "episode_len_mean": 25.0, "episode_media": {}, "episodes_this_iter": 2, "policy_reward_min": {"policy_leadadversary_": -4.038309224748798, "policy_adversary_": -3.094481311936268, "policy_agent_": -96.35415811344997}, "policy_reward_max": {"policy_leadadversary_": 8.302786016962129, "policy_adversary_": 9.22635362970437, "policy_agent_": 27.837397306389533}, "policy_reward_mean": {"policy_leadadversary_": 0.24527369593932247, "policy_adversary_": 0.7069292312066673, "policy_agent_": -15.97181977245007}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-105.16393252669111, -38.08425587795351, -8.179283354852107, -7.994660195117557, 17.029253709543042, -62.95754389372587, -21.306978110434503, -9.963224993494906], "episode_lengths": [25, 25, 25, 25, 25, 25, 25, 25], "policy_policy_leadadversary__reward": [-2.399035557624381, 8.302786016962129, -1.9111450930988323, -1.90667729707557, -4.038309224748798, -1.1928023284527944, 6.938235712341814, -1.8308626607889866], "policy_policy_adversary__reward": [-1.6741618220129826, -2.2223381552451884, -1.7978662959280882, 6.195230400528265, 9.085335341319167, 9.161857338996002, -1.1497964664252314, -0.7244895816458103, -2.705072794727737, -1.9452619286739838, -0.5002713058248459, -2.428351018980954, -2.073914420541661, -2.491769450548833, -1.928789335869684, -1.6434950773450425, -1.1869343121273628, -3.094481311936268, 9.22635362970437, 8.437625312282139, 8.558180033954198, -1.474364867288304, -2.699172611779865, -1.9577497509222836], "policy_policy_agent__reward": [-0.7163725824305123, -96.35415811344997, -3.617249732269532, -67.21221524348954, -1.1897859301531337, -0.49899348880136807, -0.7213000577969194, -0.49279858676528443, 27.837397306389533, -0.27536116513751435, -1.8093468032775135, -54.03048406058688, -42.95203037708829, -11.515342421628725, -1.0331583960521016, -0.9679167066633642]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6225128157971607, "mean_inference_ms": 2.84491585187963, "mean_action_processing_ms": 0.11156403292502719, "mean_env_wait_ms": 3.352443194514218, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 1, "timesteps_total": 200, "timesteps_this_iter": 0, "agent_timesteps_total": 1200, "timers": {"sample_time_ms": 442.522, "sample_throughput": 112.989, "load_time_ms": 0.51, "load_throughput": 98123.851, "learn_time_ms": 72.476, "learn_throughput": 689.882, "update_time_ms": 2.119}, "info": {"learner": {"policy_leadadversary_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 0.025, "cur_lr": 0.0005, "total_loss": 16.94384479522705, "policy_loss": -0.003726049158721878, "vf_loss": 16.977519989013672, "vf_explained_var": -0.01862889528274536, "kl": 1.7464292612318078e-05, "entropy": 2.995032548904419, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_adversary_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 0.024999999999999998, "cur_lr": 0.0005, "total_loss": 24.76324474811554, "policy_loss": -0.008555769299467405, "vf_loss": 24.801640053590138, "vf_explained_var": -0.02297516663869222, "kl": 0.00037216066539317677, "entropy": 2.9848137696584067, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_agent_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 0.025, "cur_lr": 0.0005, "total_loss": 140.6798720508814, "policy_loss": -0.002555757761001587, "vf_loss": 140.71229495108128, "vf_explained_var": -0.0631071925163269, "kl": 0.00012024437595592463, "entropy": 2.9884461164474487, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}}, "num_steps_sampled": 200, "num_agent_steps_sampled": 1200, "num_steps_trained": 200, "num_agent_steps_trained": 1200, "num_steps_trained_this_iter": 0}, "done": false, "episodes_total": 8, "training_iteration": 4, "trial_id": "f5f5f_00000", "experiment_id": "d79be65b63864cf3aab1c0ed35a1f497", "date": "2025-03-09_18-28-54", "timestamp": 1741541334, "time_this_iter_s": 0.39237356185913086, "time_total_s": 1.714491367340088, "pid": 48554, "hostname": "julien-PC", "node_ip": "192.168.89.58", "config": {"num_workers": 1, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 50, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 50, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "Centralized_Critic_Model", "custom_model_config": {"env": "mpe", "env_args": {"continuous_actions": false, "max_cycles": 25, "map_name": "simple_world_comm"}, "mask_flag": false, "global_state_flag": false, "opp_action_in_cc": true, "agent_level_batch_update": false, "force_coop": false, "local_mode": false, "share_policy": "group", "evaluation_interval": 50, "framework": "torch", "num_workers": 1, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "checkpoint_freq": 20, "checkpoint_end": true, "restore_path": {"model_path": "", "params_path": ""}, "stop_iters": 9999999, "stop_timesteps": 2000000, "stop_reward": 999999, "seed": 321, "local_dir": "", "model_arch_args": {"hidden_state_size": 256, "core_arch": "mlp", "fc_layer": 2, "out_dim_fc_0": 128, "out_dim_fc_1": 64, "encode_layer": "128-256"}, "algorithm": "mappo", "space_obs": "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "space_act": "Discrete(20)", "num_agents": 6, "episode_limit": 25, "policy_mapping_info": {"simple_adversary": {"description": "one team attack, one team survive", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_crypto": {"description": "two team cooperate, one team attack", "team_prefix": ["eve_", "bob_", "alice_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_push": {"description": "one team target on landmark, one team attack", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_tag": {"description": "one team attack, one team survive", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_spread": {"description": "one team cooperate", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}, "simple_reference": {"description": "one team cooperate", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}, "simple_world_comm": {"description": "two team cooperate and attack, one team survive", "team_prefix": ["adversary_", "leadadversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_speaker_listener": {"description": "two team cooperate", "team_prefix": ["speaker_", "listener_"], "all_agents_one_policy": true, "one_agent_one_policy": true}}, "agent_name_ls": ["leadadversary_0", "adversary_0", "adversary_1", "adversary_2", "agent_0", "agent_1"]}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "mpe_simple_world_comm", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 50, "evaluation_num_episodes": 10, "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 100, "min_iter_time_s": 0, "timesteps_per_iteration": 0, "seed": 321, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_adversary_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}], "policy_leadadversary_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}], "policy_agent_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x7f88aff00550>", "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "simple_optimizer": false, "monitor": -1, "use_critic": true, "use_gae": true, "lambda": 1.0, "kl_coeff": 0.2, "sgd_minibatch_size": 50, "shuffle_sequences": true, "num_sgd_iter": 2, "lr_schedule": null, "vf_loss_coeff": 1.0, "entropy_coeff": 0.01, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": null, "kl_target": 0.01, "vf_share_layers": -1}, "time_since_restore": 1.714491367340088, "timesteps_since_restore": 0, "iterations_since_restore": 4, "perf": {"cpu_util_percent": 21.6, "ram_util_percent": 46.4}}
{"episode_reward_max": 17.029253709543042, "episode_reward_min": -105.16393252669111, "episode_reward_mean": -28.00947708819257, "episode_len_mean": 25.0, "episode_media": {}, "episodes_this_iter": 2, "policy_reward_min": {"policy_leadadversary_": -4.038309224748798, "policy_adversary_": -3.094481311936268, "policy_agent_": -96.35415811344997}, "policy_reward_max": {"policy_leadadversary_": 8.940230507526055, "policy_adversary_": 9.22635362970437, "policy_agent_": 27.837397306389533}, "policy_reward_mean": {"policy_leadadversary_": 0.880586484689894, "policy_adversary_": 1.2497785943843627, "policy_agent_": -16.319699678017777}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-105.16393252669111, -38.08425587795351, -8.179283354852107, -7.994660195117557, 17.029253709543042, -62.95754389372587, -21.306978110434503, -9.963224993494906, -35.6723275216737, -7.801818117525467], "episode_lengths": [25, 25, 25, 25, 25, 25, 25, 25, 25, 25], "policy_policy_leadadversary__reward": [-2.399035557624381, 8.302786016962129, -1.9111450930988323, -1.90667729707557, -4.038309224748798, -1.1928023284527944, 6.938235712341814, -1.8308626607889866, 8.940230507526055, -2.0965552281416944], "policy_policy_adversary__reward": [-1.6741618220129826, -2.2223381552451884, -1.7978662959280882, 6.195230400528265, 9.085335341319167, 9.161857338996002, -1.1497964664252314, -0.7244895816458103, -2.705072794727737, -1.9452619286739838, -0.5002713058248459, -2.428351018980954, -2.073914420541661, -2.491769450548833, -1.928789335869684, -1.6434950773450425, -1.1869343121273628, -3.094481311936268, 9.22635362970437, 8.437625312282139, 8.558180033954198, -1.474364867288304, -2.699172611779865, -1.9577497509222836, 8.892856901464825, 8.219320737803885, 8.128349998386524, -2.4694792844144473, -0.6928708713293986, -1.5511211993405283], "policy_policy_agent__reward": [-0.7163725824305123, -96.35415811344997, -3.617249732269532, -67.21221524348954, -1.1897859301531337, -0.49899348880136807, -0.7213000577969194, -0.49279858676528443, 27.837397306389533, -0.27536116513751435, -1.8093468032775135, -54.03048406058688, -42.95203037708829, -11.515342421628725, -1.0331583960521016, -0.9679167066633642, -0.927224282567324, -68.92586138428767, -0.4035124455295982, -0.5882790887698023]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6120094788165096, "mean_inference_ms": 2.827085195775231, "mean_action_processing_ms": 0.11063708792298677, "mean_env_wait_ms": 3.34661673058093, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 1, "timesteps_total": 250, "timesteps_this_iter": 0, "agent_timesteps_total": 1500, "timers": {"sample_time_ms": 439.376, "sample_throughput": 113.798, "load_time_ms": 0.496, "load_throughput": 100766.481, "learn_time_ms": 70.904, "learn_throughput": 705.179, "update_time_ms": 2.209}, "info": {"learner": {"policy_leadadversary_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 0.0125, "cur_lr": 0.0005, "total_loss": 4.317026376724243, "policy_loss": -0.0025436210259794922, "vf_loss": 4.349509954452515, "vf_explained_var": -0.002107560634613037, "kl": 1.566297162103325e-05, "entropy": 2.9939972162246704, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_adversary_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 0.012499999999999999, "cur_lr": 0.0005, "total_loss": 3.854662428299586, "policy_loss": -0.006268113851547241, "vf_loss": 3.8907624930143356, "vf_explained_var": -0.0056146979331970215, "kl": 0.00037463121912728603, "entropy": 2.9836480617523193, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_agent_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 0.0125, "cur_lr": 0.0005, "total_loss": 317.6626796722412, "policy_loss": -0.00533737987279892, "vf_loss": 317.6978874541819, "vf_explained_var": -0.2617122232913971, "kl": 0.00010628900877707181, "entropy": 2.987911343574524, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}}, "num_steps_sampled": 250, "num_agent_steps_sampled": 1500, "num_steps_trained": 250, "num_agent_steps_trained": 1500, "num_steps_trained_this_iter": 0}, "done": false, "episodes_total": 10, "training_iteration": 5, "trial_id": "f5f5f_00000", "experiment_id": "d79be65b63864cf3aab1c0ed35a1f497", "date": "2025-03-09_18-28-55", "timestamp": 1741541335, "time_this_iter_s": 0.39510250091552734, "time_total_s": 2.1095938682556152, "pid": 48554, "hostname": "julien-PC", "node_ip": "192.168.89.58", "config": {"num_workers": 1, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 50, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 50, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "Centralized_Critic_Model", "custom_model_config": {"env": "mpe", "env_args": {"continuous_actions": false, "max_cycles": 25, "map_name": "simple_world_comm"}, "mask_flag": false, "global_state_flag": false, "opp_action_in_cc": true, "agent_level_batch_update": false, "force_coop": false, "local_mode": false, "share_policy": "group", "evaluation_interval": 50, "framework": "torch", "num_workers": 1, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "checkpoint_freq": 20, "checkpoint_end": true, "restore_path": {"model_path": "", "params_path": ""}, "stop_iters": 9999999, "stop_timesteps": 2000000, "stop_reward": 999999, "seed": 321, "local_dir": "", "model_arch_args": {"hidden_state_size": 256, "core_arch": "mlp", "fc_layer": 2, "out_dim_fc_0": 128, "out_dim_fc_1": 64, "encode_layer": "128-256"}, "algorithm": "mappo", "space_obs": "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "space_act": "Discrete(20)", "num_agents": 6, "episode_limit": 25, "policy_mapping_info": {"simple_adversary": {"description": "one team attack, one team survive", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_crypto": {"description": "two team cooperate, one team attack", "team_prefix": ["eve_", "bob_", "alice_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_push": {"description": "one team target on landmark, one team attack", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_tag": {"description": "one team attack, one team survive", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_spread": {"description": "one team cooperate", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}, "simple_reference": {"description": "one team cooperate", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}, "simple_world_comm": {"description": "two team cooperate and attack, one team survive", "team_prefix": ["adversary_", "leadadversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_speaker_listener": {"description": "two team cooperate", "team_prefix": ["speaker_", "listener_"], "all_agents_one_policy": true, "one_agent_one_policy": true}}, "agent_name_ls": ["leadadversary_0", "adversary_0", "adversary_1", "adversary_2", "agent_0", "agent_1"]}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "mpe_simple_world_comm", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 50, "evaluation_num_episodes": 10, "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 100, "min_iter_time_s": 0, "timesteps_per_iteration": 0, "seed": 321, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_adversary_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}], "policy_leadadversary_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}], "policy_agent_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x7f88aff1a790>", "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "simple_optimizer": false, "monitor": -1, "use_critic": true, "use_gae": true, "lambda": 1.0, "kl_coeff": 0.2, "sgd_minibatch_size": 50, "shuffle_sequences": true, "num_sgd_iter": 2, "lr_schedule": null, "vf_loss_coeff": 1.0, "entropy_coeff": 0.01, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": null, "kl_target": 0.01, "vf_share_layers": -1}, "time_since_restore": 2.1095938682556152, "timesteps_since_restore": 0, "iterations_since_restore": 5, "perf": {"cpu_util_percent": 15.7, "ram_util_percent": 46.4}}
{"episode_reward_max": 17.029253709543042, "episode_reward_min": -105.16393252669111, "episode_reward_mean": -23.35167410134211, "episode_len_mean": 25.0, "episode_media": {}, "episodes_this_iter": 2, "policy_reward_min": {"policy_leadadversary_": -4.038309224748798, "policy_adversary_": -3.094481311936268, "policy_agent_": -96.35415811344997}, "policy_reward_max": {"policy_leadadversary_": 9.03154180074615, "policy_adversary_": 9.22635362970437, "policy_agent_": 27.837397306389533}, "policy_reward_mean": {"policy_leadadversary_": 1.3826060399677598, "policy_adversary_": 1.502264864709372, "policy_agent_": -14.620537367718994}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-105.16393252669111, -38.08425587795351, -8.179283354852107, -7.994660195117557, 17.029253709543042, -62.95754389372587, -21.306978110434503, -9.963224993494906, -35.6723275216737, -7.801818117525467, 9.04258457925305, -9.16790291343267], "episode_lengths": [25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25], "policy_policy_leadadversary__reward": [-2.399035557624381, 8.302786016962129, -1.9111450930988323, -1.90667729707557, -4.038309224748798, -1.1928023284527944, 6.938235712341814, -1.8308626607889866, 8.940230507526055, -2.0965552281416944, 9.03154180074615, -1.2461341680319717], "policy_policy_adversary__reward": [-1.6741618220129826, -2.2223381552451884, -1.7978662959280882, 6.195230400528265, 9.085335341319167, 9.161857338996002, -1.1497964664252314, -0.7244895816458103, -2.705072794727737, -1.9452619286739838, -0.5002713058248459, -2.428351018980954, -2.073914420541661, -2.491769450548833, -1.928789335869684, -1.6434950773450425, -1.1869343121273628, -3.094481311936268, 9.22635362970437, 8.437625312282139, 8.558180033954198, -1.474364867288304, -2.699172611779865, -1.9577497509222836, 8.892856901464825, 8.219320737803885, 8.128349998386524, -2.4694792844144473, -0.6928708713293986, -1.5511211993405283, 6.420991744018833, 7.748318394913137, 8.185932035407232, -2.473275836552458, -0.5674346699607982, -2.7263543698194392], "policy_policy_agent__reward": [-0.7163725824305123, -96.35415811344997, -3.617249732269532, -67.21221524348954, -1.1897859301531337, -0.49899348880136807, -0.7213000577969194, -0.49279858676528443, 27.837397306389533, -0.27536116513751435, -1.8093468032775135, -54.03048406058688, -42.95203037708829, -11.515342421628725, -1.0331583960521016, -0.9679167066633642, -0.927224282567324, -68.92586138428767, -0.4035124455295982, -0.5882790887698023, -0.3080346699354489, -22.036164725896853, -1.0213367048557638, -1.1333671642122476]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.6030314093520861, "mean_inference_ms": 2.810003405155422, "mean_action_processing_ms": 0.10978496992676774, "mean_env_wait_ms": 3.3403662152743796, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 1, "timesteps_total": 300, "timesteps_this_iter": 0, "agent_timesteps_total": 1800, "timers": {"sample_time_ms": 437.537, "sample_throughput": 114.276, "load_time_ms": 0.531, "load_throughput": 94246.963, "learn_time_ms": 71.124, "learn_throughput": 703.002, "update_time_ms": 2.128}, "info": {"learner": {"policy_leadadversary_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 0.00625, "cur_lr": 0.0005, "total_loss": 11.681378841400146, "policy_loss": -0.0022192574013022437, "vf_loss": 11.713536262512207, "vf_explained_var": 0.008106112480163574, "kl": 1.7482439467919608e-05, "entropy": 2.9939053058624268, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_adversary_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 0.0062499999999999995, "cur_lr": 0.0005, "total_loss": 9.226399938265482, "policy_loss": -0.006322582562764485, "vf_loss": 9.262501021226248, "vf_explained_var": 0.018094619115193684, "kl": 0.0002778621851494319, "entropy": 2.9780093828837075, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_agent_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 0.00625, "cur_lr": 0.0005, "total_loss": 42.81404508650303, "policy_loss": -0.004159383475780487, "vf_loss": 42.84810637682676, "vf_explained_var": -0.010625377297401428, "kl": 6.101998271590414e-05, "entropy": 2.990154802799225, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}}, "num_steps_sampled": 300, "num_agent_steps_sampled": 1800, "num_steps_trained": 300, "num_agent_steps_trained": 1800, "num_steps_trained_this_iter": 0}, "done": false, "episodes_total": 12, "training_iteration": 6, "trial_id": "f5f5f_00000", "experiment_id": "d79be65b63864cf3aab1c0ed35a1f497", "date": "2025-03-09_18-28-55", "timestamp": 1741541335, "time_this_iter_s": 0.40271925926208496, "time_total_s": 2.5123131275177, "pid": 48554, "hostname": "julien-PC", "node_ip": "192.168.89.58", "config": {"num_workers": 1, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 50, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 50, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "Centralized_Critic_Model", "custom_model_config": {"env": "mpe", "env_args": {"continuous_actions": false, "max_cycles": 25, "map_name": "simple_world_comm"}, "mask_flag": false, "global_state_flag": false, "opp_action_in_cc": true, "agent_level_batch_update": false, "force_coop": false, "local_mode": false, "share_policy": "group", "evaluation_interval": 50, "framework": "torch", "num_workers": 1, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "checkpoint_freq": 20, "checkpoint_end": true, "restore_path": {"model_path": "", "params_path": ""}, "stop_iters": 9999999, "stop_timesteps": 2000000, "stop_reward": 999999, "seed": 321, "local_dir": "", "model_arch_args": {"hidden_state_size": 256, "core_arch": "mlp", "fc_layer": 2, "out_dim_fc_0": 128, "out_dim_fc_1": 64, "encode_layer": "128-256"}, "algorithm": "mappo", "space_obs": "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "space_act": "Discrete(20)", "num_agents": 6, "episode_limit": 25, "policy_mapping_info": {"simple_adversary": {"description": "one team attack, one team survive", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_crypto": {"description": "two team cooperate, one team attack", "team_prefix": ["eve_", "bob_", "alice_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_push": {"description": "one team target on landmark, one team attack", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_tag": {"description": "one team attack, one team survive", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_spread": {"description": "one team cooperate", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}, "simple_reference": {"description": "one team cooperate", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}, "simple_world_comm": {"description": "two team cooperate and attack, one team survive", "team_prefix": ["adversary_", "leadadversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_speaker_listener": {"description": "two team cooperate", "team_prefix": ["speaker_", "listener_"], "all_agents_one_policy": true, "one_agent_one_policy": true}}, "agent_name_ls": ["leadadversary_0", "adversary_0", "adversary_1", "adversary_2", "agent_0", "agent_1"]}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "mpe_simple_world_comm", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 50, "evaluation_num_episodes": 10, "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 100, "min_iter_time_s": 0, "timesteps_per_iteration": 0, "seed": 321, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_adversary_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}], "policy_leadadversary_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}], "policy_agent_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x7f88aff1a9d0>", "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "simple_optimizer": false, "monitor": -1, "use_critic": true, "use_gae": true, "lambda": 1.0, "kl_coeff": 0.2, "sgd_minibatch_size": 50, "shuffle_sequences": true, "num_sgd_iter": 2, "lr_schedule": null, "vf_loss_coeff": 1.0, "entropy_coeff": 0.01, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": null, "kl_target": 0.01, "vf_share_layers": -1}, "time_since_restore": 2.5123131275177, "timesteps_since_restore": 0, "iterations_since_restore": 6, "perf": {}}
{"episode_reward_max": 17.029253709543042, "episode_reward_min": -105.16393252669111, "episode_reward_mean": -27.141339915027213, "episode_len_mean": 25.0, "episode_media": {}, "episodes_this_iter": 2, "policy_reward_min": {"policy_leadadversary_": -4.388542193694188, "policy_adversary_": -3.923014083123016, "policy_agent_": -96.35415811344997}, "policy_reward_max": {"policy_leadadversary_": 9.03154180074615, "policy_adversary_": 9.22635362970437, "policy_agent_": 27.837397306389533}, "policy_reward_mean": {"policy_leadadversary_": 0.5967721428138939, "policy_adversary_": 0.9827626133272777, "policy_agent_": -15.343199948911472}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-105.16393252669111, -38.08425587795351, -8.179283354852107, -7.994660195117557, 17.029253709543042, -62.95754389372587, -21.306978110434503, -9.963224993494906, -35.6723275216737, -7.801818117525467, 9.04258457925305, -9.16790291343267, -23.11610041380425, -76.64256918047145], "episode_lengths": [25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25], "policy_policy_leadadversary__reward": [-2.399035557624381, 8.302786016962129, -1.9111450930988323, -1.90667729707557, -4.038309224748798, -1.1928023284527944, 6.938235712341814, -1.8308626607889866, 8.940230507526055, -2.0965552281416944, 9.03154180074615, -1.2461341680319717, -4.388542193694188, -3.847920286524414], "policy_policy_adversary__reward": [-1.6741618220129826, -2.2223381552451884, -1.7978662959280882, 6.195230400528265, 9.085335341319167, 9.161857338996002, -1.1497964664252314, -0.7244895816458103, -2.705072794727737, -1.9452619286739838, -0.5002713058248459, -2.428351018980954, -2.073914420541661, -2.491769450548833, -1.928789335869684, -1.6434950773450425, -1.1869343121273628, -3.094481311936268, 9.22635362970437, 8.437625312282139, 8.558180033954198, -1.474364867288304, -2.699172611779865, -1.9577497509222836, 8.892856901464825, 8.219320737803885, 8.128349998386524, -2.4694792844144473, -0.6928708713293986, -1.5511211993405283, 6.420991744018833, 7.748318394913137, 8.185932035407232, -2.473275836552458, -0.5674346699607982, -2.7263543698194392, -0.5909371354705603, -1.2690244550254388, -1.8793974687070758, -2.8351443039978297, -3.923014083123016, -2.3079879234678007], "policy_policy_agent__reward": [-0.7163725824305123, -96.35415811344997, -3.617249732269532, -67.21221524348954, -1.1897859301531337, -0.49899348880136807, -0.7213000577969194, -0.49279858676528443, 27.837397306389533, -0.27536116513751435, -1.8093468032775135, -54.03048406058688, -42.95203037708829, -11.515342421628725, -1.0331583960521016, -0.9679167066633642, -0.927224282567324, -68.92586138428767, -0.4035124455295982, -0.5882790887698023, -0.3080346699354489, -22.036164725896853, -1.0213367048557638, -1.1333671642122476, -0.6140299114418961, -14.374169249465094, -0.5039952235368317, -63.22450735982157]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.595889694310663, "mean_inference_ms": 2.795499075402586, "mean_action_processing_ms": 0.10907005284526727, "mean_env_wait_ms": 3.3341120705614786, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 1, "timesteps_total": 350, "timesteps_this_iter": 0, "agent_timesteps_total": 2100, "timers": {"sample_time_ms": 437.066, "sample_throughput": 114.399, "load_time_ms": 0.514, "load_throughput": 97315.638, "learn_time_ms": 71.744, "learn_throughput": 696.924, "update_time_ms": 2.061}, "info": {"learner": {"policy_leadadversary_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 0.003125, "cur_lr": 0.0005, "total_loss": 5.001577377319336, "policy_loss": -0.004602401461450967, "vf_loss": 5.0360801219940186, "vf_explained_var": -0.0012871921062469482, "kl": 4.5948354044700324e-05, "entropy": 2.990069627761841, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_adversary_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 0.0031249999999999997, "cur_lr": 0.0005, "total_loss": 1.726941446463267, "policy_loss": -0.006559203068415324, "vf_loss": 1.7629759162664413, "vf_explained_var": -0.08909990390141805, "kl": 0.00042601394572786927, "entropy": 2.9476671616236367, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_agent_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 0.003125, "cur_lr": 0.0005, "total_loss": 350.2781581878662, "policy_loss": -0.004552282392978668, "vf_loss": 350.3125514984131, "vf_explained_var": -0.001450270414352417, "kl": 0.00014690839644226905, "entropy": 2.9832924008369446, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}}, "num_steps_sampled": 350, "num_agent_steps_sampled": 2100, "num_steps_trained": 350, "num_agent_steps_trained": 2100, "num_steps_trained_this_iter": 0}, "done": false, "episodes_total": 14, "training_iteration": 7, "trial_id": "f5f5f_00000", "experiment_id": "d79be65b63864cf3aab1c0ed35a1f497", "date": "2025-03-09_18-28-56", "timestamp": 1741541336, "time_this_iter_s": 0.4082815647125244, "time_total_s": 2.9205946922302246, "pid": 48554, "hostname": "julien-PC", "node_ip": "192.168.89.58", "config": {"num_workers": 1, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 50, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 50, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "Centralized_Critic_Model", "custom_model_config": {"env": "mpe", "env_args": {"continuous_actions": false, "max_cycles": 25, "map_name": "simple_world_comm"}, "mask_flag": false, "global_state_flag": false, "opp_action_in_cc": true, "agent_level_batch_update": false, "force_coop": false, "local_mode": false, "share_policy": "group", "evaluation_interval": 50, "framework": "torch", "num_workers": 1, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "checkpoint_freq": 20, "checkpoint_end": true, "restore_path": {"model_path": "", "params_path": ""}, "stop_iters": 9999999, "stop_timesteps": 2000000, "stop_reward": 999999, "seed": 321, "local_dir": "", "model_arch_args": {"hidden_state_size": 256, "core_arch": "mlp", "fc_layer": 2, "out_dim_fc_0": 128, "out_dim_fc_1": 64, "encode_layer": "128-256"}, "algorithm": "mappo", "space_obs": "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "space_act": "Discrete(20)", "num_agents": 6, "episode_limit": 25, "policy_mapping_info": {"simple_adversary": {"description": "one team attack, one team survive", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_crypto": {"description": "two team cooperate, one team attack", "team_prefix": ["eve_", "bob_", "alice_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_push": {"description": "one team target on landmark, one team attack", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_tag": {"description": "one team attack, one team survive", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_spread": {"description": "one team cooperate", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}, "simple_reference": {"description": "one team cooperate", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}, "simple_world_comm": {"description": "two team cooperate and attack, one team survive", "team_prefix": ["adversary_", "leadadversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_speaker_listener": {"description": "two team cooperate", "team_prefix": ["speaker_", "listener_"], "all_agents_one_policy": true, "one_agent_one_policy": true}}, "agent_name_ls": ["leadadversary_0", "adversary_0", "adversary_1", "adversary_2", "agent_0", "agent_1"]}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "mpe_simple_world_comm", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 50, "evaluation_num_episodes": 10, "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 100, "min_iter_time_s": 0, "timesteps_per_iteration": 0, "seed": 321, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_adversary_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}], "policy_leadadversary_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}], "policy_agent_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x7f88afef4e50>", "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "simple_optimizer": false, "monitor": -1, "use_critic": true, "use_gae": true, "lambda": 1.0, "kl_coeff": 0.2, "sgd_minibatch_size": 50, "shuffle_sequences": true, "num_sgd_iter": 2, "lr_schedule": null, "vf_loss_coeff": 1.0, "entropy_coeff": 0.01, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": null, "kl_target": 0.01, "vf_share_layers": -1}, "time_since_restore": 2.9205946922302246, "timesteps_since_restore": 0, "iterations_since_restore": 7, "perf": {"cpu_util_percent": 21.9, "ram_util_percent": 46.5}}
{"episode_reward_max": 19.012759487513534, "episode_reward_min": -105.16393252669111, "episode_reward_mean": -23.700215023957604, "episode_len_mean": 25.0, "episode_media": {}, "episodes_this_iter": 2, "policy_reward_min": {"policy_leadadversary_": -4.388542193694188, "policy_adversary_": -3.923014083123016, "policy_agent_": -96.35415811344997}, "policy_reward_max": {"policy_leadadversary_": 9.03154180074615, "policy_adversary_": 9.22635362970437, "policy_agent_": 27.837397306389533}, "policy_reward_mean": {"policy_leadadversary_": 0.5568924608094787, "policy_adversary_": 0.9728309146671869, "policy_agent_": -13.587800114384324}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-105.16393252669111, -38.08425587795351, -8.179283354852107, -7.994660195117557, 17.029253709543042, -62.95754389372587, -21.306978110434503, -9.963224993494906, -35.6723275216737, -7.801818117525467, 9.04258457925305, -9.16790291343267, -23.11610041380425, -76.64256918047145, 19.012759487513534, -18.237441060454206], "episode_lengths": [25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25], "policy_policy_leadadversary__reward": [-2.399035557624381, 8.302786016962129, -1.9111450930988323, -1.90667729707557, -4.038309224748798, -1.1928023284527944, 6.938235712341814, -1.8308626607889866, 8.940230507526055, -2.0965552281416944, 9.03154180074615, -1.2461341680319717, -4.388542193694188, -3.847920286524414, 3.5200136941709386, -2.964544320613795], "policy_policy_adversary__reward": [-1.6741618220129826, -2.2223381552451884, -1.7978662959280882, 6.195230400528265, 9.085335341319167, 9.161857338996002, -1.1497964664252314, -0.7244895816458103, -2.705072794727737, -1.9452619286739838, -0.5002713058248459, -2.428351018980954, -2.073914420541661, -2.491769450548833, -1.928789335869684, -1.6434950773450425, -1.1869343121273628, -3.094481311936268, 9.22635362970437, 8.437625312282139, 8.558180033954198, -1.474364867288304, -2.699172611779865, -1.9577497509222836, 8.892856901464825, 8.219320737803885, 8.128349998386524, -2.4694792844144473, -0.6928708713293986, -1.5511211993405283, 6.420991744018833, 7.748318394913137, 8.185932035407232, -2.473275836552458, -0.5674346699607982, -2.7263543698194392, -0.5909371354705603, -1.2690244550254388, -1.8793974687070758, -2.8351443039978297, -3.923014083123016, -2.3079879234678007, 4.100268481179938, 3.027702044631114, 4.02297814215707, -2.0777870863086294, -1.9840069073615636, -1.669300530018625], "policy_policy_agent__reward": [-0.7163725824305123, -96.35415811344997, -3.617249732269532, -67.21221524348954, -1.1897859301531337, -0.49899348880136807, -0.7213000577969194, -0.49279858676528443, 27.837397306389533, -0.27536116513751435, -1.8093468032775135, -54.03048406058688, -42.95203037708829, -11.515342421628725, -1.0331583960521016, -0.9679167066633642, -0.927224282567324, -68.92586138428767, -0.4035124455295982, -0.5882790887698023, -0.3080346699354489, -22.036164725896853, -1.0213367048557638, -1.1333671642122476, -0.6140299114418961, -14.374169249465094, -0.5039952235368317, -63.22450735982157, 9.76231673232199, -5.42051960694752, -0.38719184121053235, -9.154610374941065]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.5898829764882537, "mean_inference_ms": 2.7844866078511092, "mean_action_processing_ms": 0.10846830928370456, "mean_env_wait_ms": 3.3291326957725564, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 1, "timesteps_total": 400, "timesteps_this_iter": 0, "agent_timesteps_total": 2400, "timers": {"sample_time_ms": 437.893, "sample_throughput": 114.183, "load_time_ms": 0.504, "load_throughput": 99302.847, "learn_time_ms": 71.229, "learn_throughput": 701.961, "update_time_ms": 2.044}, "info": {"learner": {"policy_leadadversary_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 0.0015625, "cur_lr": 0.0005, "total_loss": 3.969346284866333, "policy_loss": -0.0028140289150178077, "vf_loss": 4.002050518989563, "vf_explained_var": 0.01816144585609436, "kl": 3.2282390607818456e-05, "entropy": 2.9890289306640625, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_adversary_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 0.0015624999999999999, "cur_lr": 0.0005, "total_loss": 3.4387414852778115, "policy_loss": -0.006395875941962004, "vf_loss": 3.474739541610082, "vf_explained_var": 0.014252394437789917, "kl": 0.00025637826822285287, "entropy": 2.960293094317118, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_agent_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 0.0015625, "cur_lr": 0.0005, "total_loss": 30.485148429870605, "policy_loss": -0.0038210302591323853, "vf_loss": 30.51882266998291, "vf_explained_var": 0.0006709247827529907, "kl": 0.00011098488306549026, "entropy": 2.9854143261909485, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}}, "num_steps_sampled": 400, "num_agent_steps_sampled": 2400, "num_steps_trained": 400, "num_agent_steps_trained": 2400, "num_steps_trained_this_iter": 0}, "done": false, "episodes_total": 16, "training_iteration": 8, "trial_id": "f5f5f_00000", "experiment_id": "d79be65b63864cf3aab1c0ed35a1f497", "date": "2025-03-09_18-28-56", "timestamp": 1741541336, "time_this_iter_s": 0.4074704647064209, "time_total_s": 3.3280651569366455, "pid": 48554, "hostname": "julien-PC", "node_ip": "192.168.89.58", "config": {"num_workers": 1, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 50, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 50, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "Centralized_Critic_Model", "custom_model_config": {"env": "mpe", "env_args": {"continuous_actions": false, "max_cycles": 25, "map_name": "simple_world_comm"}, "mask_flag": false, "global_state_flag": false, "opp_action_in_cc": true, "agent_level_batch_update": false, "force_coop": false, "local_mode": false, "share_policy": "group", "evaluation_interval": 50, "framework": "torch", "num_workers": 1, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "checkpoint_freq": 20, "checkpoint_end": true, "restore_path": {"model_path": "", "params_path": ""}, "stop_iters": 9999999, "stop_timesteps": 2000000, "stop_reward": 999999, "seed": 321, "local_dir": "", "model_arch_args": {"hidden_state_size": 256, "core_arch": "mlp", "fc_layer": 2, "out_dim_fc_0": 128, "out_dim_fc_1": 64, "encode_layer": "128-256"}, "algorithm": "mappo", "space_obs": "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "space_act": "Discrete(20)", "num_agents": 6, "episode_limit": 25, "policy_mapping_info": {"simple_adversary": {"description": "one team attack, one team survive", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_crypto": {"description": "two team cooperate, one team attack", "team_prefix": ["eve_", "bob_", "alice_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_push": {"description": "one team target on landmark, one team attack", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_tag": {"description": "one team attack, one team survive", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_spread": {"description": "one team cooperate", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}, "simple_reference": {"description": "one team cooperate", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}, "simple_world_comm": {"description": "two team cooperate and attack, one team survive", "team_prefix": ["adversary_", "leadadversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_speaker_listener": {"description": "two team cooperate", "team_prefix": ["speaker_", "listener_"], "all_agents_one_policy": true, "one_agent_one_policy": true}}, "agent_name_ls": ["leadadversary_0", "adversary_0", "adversary_1", "adversary_2", "agent_0", "agent_1"]}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "mpe_simple_world_comm", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 50, "evaluation_num_episodes": 10, "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 100, "min_iter_time_s": 0, "timesteps_per_iteration": 0, "seed": 321, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_adversary_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}], "policy_leadadversary_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}], "policy_agent_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x7f88aff50f70>", "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "simple_optimizer": false, "monitor": -1, "use_critic": true, "use_gae": true, "lambda": 1.0, "kl_coeff": 0.2, "sgd_minibatch_size": 50, "shuffle_sequences": true, "num_sgd_iter": 2, "lr_schedule": null, "vf_loss_coeff": 1.0, "entropy_coeff": 0.01, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": null, "kl_target": 0.01, "vf_share_layers": -1}, "time_since_restore": 3.3280651569366455, "timesteps_since_restore": 0, "iterations_since_restore": 8, "perf": {"cpu_util_percent": 17.9, "ram_util_percent": 46.3}}
{"episode_reward_max": 19.012759487513534, "episode_reward_min": -105.16393252669111, "episode_reward_mean": -24.89772945622963, "episode_len_mean": 25.0, "episode_media": {}, "episodes_this_iter": 2, "policy_reward_min": {"policy_leadadversary_": -4.388542193694188, "policy_adversary_": -3.9338029974868833, "policy_agent_": -96.35415811344997}, "policy_reward_max": {"policy_leadadversary_": 9.03154180074615, "policy_adversary_": 9.22635362970437, "policy_agent_": 27.837397306389533}, "policy_reward_mean": {"policy_leadadversary_": 0.1886518776136983, "policy_adversary_": 0.6298121778442196, "policy_agent_": -13.487908933687995}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-105.16393252669111, -38.08425587795351, -8.179283354852107, -7.994660195117557, 17.029253709543042, -62.95754389372587, -21.306978110434503, -9.963224993494906, -35.6723275216737, -7.801818117525467, 9.04258457925305, -9.16790291343267, -23.11610041380425, -76.64256918047145, 19.012759487513534, -18.237441060454206, -26.428737443182527, -42.52695238562914], "episode_lengths": [25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25], "policy_policy_leadadversary__reward": [-2.399035557624381, 8.302786016962129, -1.9111450930988323, -1.90667729707557, -4.038309224748798, -1.1928023284527944, 6.938235712341814, -1.8308626607889866, 8.940230507526055, -2.0965552281416944, 9.03154180074615, -1.2461341680319717, -4.388542193694188, -3.847920286524414, 3.5200136941709386, -2.964544320613795, -2.9630602344890113, -2.5514853414160785], "policy_policy_adversary__reward": [-1.6741618220129826, -2.2223381552451884, -1.7978662959280882, 6.195230400528265, 9.085335341319167, 9.161857338996002, -1.1497964664252314, -0.7244895816458103, -2.705072794727737, -1.9452619286739838, -0.5002713058248459, -2.428351018980954, -2.073914420541661, -2.491769450548833, -1.928789335869684, -1.6434950773450425, -1.1869343121273628, -3.094481311936268, 9.22635362970437, 8.437625312282139, 8.558180033954198, -1.474364867288304, -2.699172611779865, -1.9577497509222836, 8.892856901464825, 8.219320737803885, 8.128349998386524, -2.4694792844144473, -0.6928708713293986, -1.5511211993405283, 6.420991744018833, 7.748318394913137, 8.185932035407232, -2.473275836552458, -0.5674346699607982, -2.7263543698194392, -0.5909371354705603, -1.2690244550254388, -1.8793974687070758, -2.8351443039978297, -3.923014083123016, -2.3079879234678007, 4.100268481179938, 3.027702044631114, 4.02297814215707, -2.0777870863086294, -1.9840069073615636, -1.669300530018625, -3.9338029974868833, -1.129238869347308, -1.3580459203598705, -2.250033013063289, -1.3601789265843973, -2.654726573595358], "policy_policy_agent__reward": [-0.7163725824305123, -96.35415811344997, -3.617249732269532, -67.21221524348954, -1.1897859301531337, -0.49899348880136807, -0.7213000577969194, -0.49279858676528443, 27.837397306389533, -0.27536116513751435, -1.8093468032775135, -54.03048406058688, -42.95203037708829, -11.515342421628725, -1.0331583960521016, -0.9679167066633642, -0.927224282567324, -68.92586138428767, -0.4035124455295982, -0.5882790887698023, -0.3080346699354489, -22.036164725896853, -1.0213367048557638, -1.1333671642122476, -0.6140299114418961, -14.374169249465094, -0.5039952235368317, -63.22450735982157, 9.76231673232199, -5.42051960694752, -0.38719184121053235, -9.154610374941065, -1.2188982803065158, -15.825691141192937, -32.77419517584195, -0.9363333551280546]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.5845145944458977, "mean_inference_ms": 2.772534637132862, "mean_action_processing_ms": 0.10789678443413762, "mean_env_wait_ms": 3.323139575370276, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 1, "timesteps_total": 450, "timesteps_this_iter": 0, "agent_timesteps_total": 2700, "timers": {"sample_time_ms": 435.207, "sample_throughput": 114.888, "load_time_ms": 0.497, "load_throughput": 100582.83, "learn_time_ms": 70.919, "learn_throughput": 705.034, "update_time_ms": 2.01}, "info": {"learner": {"policy_leadadversary_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 0.00078125, "cur_lr": 0.0005, "total_loss": 2.237520456314087, "policy_loss": -0.0020584070961930934, "vf_loss": 2.269461750984192, "vf_explained_var": 0.005012333393096924, "kl": 2.9360726547689175e-05, "entropy": 2.9882999658584595, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_adversary_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 0.0007812499999999999, "cur_lr": 0.0005, "total_loss": 1.6838322480519612, "policy_loss": -0.0012351299325625102, "vf_loss": 1.7146148880322774, "vf_explained_var": -0.325051486492157, "kl": 0.00023506420524610494, "entropy": 2.9547783931096396, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_agent_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 0.00078125, "cur_lr": 0.0005, "total_loss": 169.7546901702881, "policy_loss": -0.004423879086971283, "vf_loss": 169.7889289855957, "vf_explained_var": -0.0005057603120803833, "kl": 0.00011350627299111515, "entropy": 2.9819536805152893, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}}, "num_steps_sampled": 450, "num_agent_steps_sampled": 2700, "num_steps_trained": 450, "num_agent_steps_trained": 2700, "num_steps_trained_this_iter": 0}, "done": false, "episodes_total": 18, "training_iteration": 9, "trial_id": "f5f5f_00000", "experiment_id": "d79be65b63864cf3aab1c0ed35a1f497", "date": "2025-03-09_18-28-57", "timestamp": 1741541337, "time_this_iter_s": 0.3855278491973877, "time_total_s": 3.713593006134033, "pid": 48554, "hostname": "julien-PC", "node_ip": "192.168.89.58", "config": {"num_workers": 1, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 50, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 50, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "Centralized_Critic_Model", "custom_model_config": {"env": "mpe", "env_args": {"continuous_actions": false, "max_cycles": 25, "map_name": "simple_world_comm"}, "mask_flag": false, "global_state_flag": false, "opp_action_in_cc": true, "agent_level_batch_update": false, "force_coop": false, "local_mode": false, "share_policy": "group", "evaluation_interval": 50, "framework": "torch", "num_workers": 1, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "checkpoint_freq": 20, "checkpoint_end": true, "restore_path": {"model_path": "", "params_path": ""}, "stop_iters": 9999999, "stop_timesteps": 2000000, "stop_reward": 999999, "seed": 321, "local_dir": "", "model_arch_args": {"hidden_state_size": 256, "core_arch": "mlp", "fc_layer": 2, "out_dim_fc_0": 128, "out_dim_fc_1": 64, "encode_layer": "128-256"}, "algorithm": "mappo", "space_obs": "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "space_act": "Discrete(20)", "num_agents": 6, "episode_limit": 25, "policy_mapping_info": {"simple_adversary": {"description": "one team attack, one team survive", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_crypto": {"description": "two team cooperate, one team attack", "team_prefix": ["eve_", "bob_", "alice_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_push": {"description": "one team target on landmark, one team attack", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_tag": {"description": "one team attack, one team survive", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_spread": {"description": "one team cooperate", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}, "simple_reference": {"description": "one team cooperate", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}, "simple_world_comm": {"description": "two team cooperate and attack, one team survive", "team_prefix": ["adversary_", "leadadversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_speaker_listener": {"description": "two team cooperate", "team_prefix": ["speaker_", "listener_"], "all_agents_one_policy": true, "one_agent_one_policy": true}}, "agent_name_ls": ["leadadversary_0", "adversary_0", "adversary_1", "adversary_2", "agent_0", "agent_1"]}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "mpe_simple_world_comm", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 50, "evaluation_num_episodes": 10, "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 100, "min_iter_time_s": 0, "timesteps_per_iteration": 0, "seed": 321, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_adversary_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}], "policy_leadadversary_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}], "policy_agent_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x7f88aff9ca60>", "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "simple_optimizer": false, "monitor": -1, "use_critic": true, "use_gae": true, "lambda": 1.0, "kl_coeff": 0.2, "sgd_minibatch_size": 50, "shuffle_sequences": true, "num_sgd_iter": 2, "lr_schedule": null, "vf_loss_coeff": 1.0, "entropy_coeff": 0.01, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": null, "kl_target": 0.01, "vf_share_layers": -1}, "time_since_restore": 3.713593006134033, "timesteps_since_restore": 0, "iterations_since_restore": 9, "perf": {}}
{"episode_reward_max": 19.012759487513534, "episode_reward_min": -105.16393252669111, "episode_reward_mean": -26.613028674090582, "episode_len_mean": 25.0, "episode_media": {}, "episodes_this_iter": 2, "policy_reward_min": {"policy_leadadversary_": -4.388542193694188, "policy_adversary_": -3.9338029974868833, "policy_agent_": -96.35415811344997}, "policy_reward_max": {"policy_leadadversary_": 9.03154180074615, "policy_adversary_": 9.22635362970437, "policy_agent_": 27.837397306389533}, "policy_reward_mean": {"policy_leadadversary_": -0.08974788476366824, "policy_adversary_": 0.3278293995326365, "policy_agent_": -13.753384493962418}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-105.16393252669111, -38.08425587795351, -8.179283354852107, -7.994660195117557, 17.029253709543042, -62.95754389372587, -21.306978110434503, -9.963224993494906, -35.6723275216737, -7.801818117525467, 9.04258457925305, -9.16790291343267, -23.11610041380425, -76.64256918047145, 19.012759487513534, -18.237441060454206, -26.428737443182527, -42.52695238562914, -72.55115007329658, -11.5502931963818], "episode_lengths": [25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25], "policy_policy_leadadversary__reward": [-2.399035557624381, 8.302786016962129, -1.9111450930988323, -1.90667729707557, -4.038309224748798, -1.1928023284527944, 6.938235712341814, -1.8308626607889866, 8.940230507526055, -2.0965552281416944, 9.03154180074615, -1.2461341680319717, -4.388542193694188, -3.847920286524414, 3.5200136941709386, -2.964544320613795, -2.9630602344890113, -2.5514853414160785, -3.423786335468951, -1.7669051568509833], "policy_policy_adversary__reward": [-1.6741618220129826, -2.2223381552451884, -1.7978662959280882, 6.195230400528265, 9.085335341319167, 9.161857338996002, -1.1497964664252314, -0.7244895816458103, -2.705072794727737, -1.9452619286739838, -0.5002713058248459, -2.428351018980954, -2.073914420541661, -2.491769450548833, -1.928789335869684, -1.6434950773450425, -1.1869343121273628, -3.094481311936268, 9.22635362970437, 8.437625312282139, 8.558180033954198, -1.474364867288304, -2.699172611779865, -1.9577497509222836, 8.892856901464825, 8.219320737803885, 8.128349998386524, -2.4694792844144473, -0.6928708713293986, -1.5511211993405283, 6.420991744018833, 7.748318394913137, 8.185932035407232, -2.473275836552458, -0.5674346699607982, -2.7263543698194392, -0.5909371354705603, -1.2690244550254388, -1.8793974687070758, -2.8351443039978297, -3.923014083123016, -2.3079879234678007, 4.100268481179938, 3.027702044631114, 4.02297814215707, -2.0777870863086294, -1.9840069073615636, -1.669300530018625, -3.9338029974868833, -1.129238869347308, -1.3580459203598705, -2.250033013063289, -1.3601789265843973, -2.654726573595358, -1.7265988852649663, -3.7396060163471576, -1.820129784531345, -2.9201218301303506, -1.7090159550536768, -2.424621160302172], "policy_policy_agent__reward": [-0.7163725824305123, -96.35415811344997, -3.617249732269532, -67.21221524348954, -1.1897859301531337, -0.49899348880136807, -0.7213000577969194, -0.49279858676528443, 27.837397306389533, -0.27536116513751435, -1.8093468032775135, -54.03048406058688, -42.95203037708829, -11.515342421628725, -1.0331583960521016, -0.9679167066633642, -0.927224282567324, -68.92586138428767, -0.4035124455295982, -0.5882790887698023, -0.3080346699354489, -22.036164725896853, -1.0213367048557638, -1.1333671642122476, -0.6140299114418961, -14.374169249465094, -0.5039952235368317, -63.22450735982157, 9.76231673232199, -5.42051960694752, -0.38719184121053235, -9.154610374941065, -1.2188982803065158, -15.825691141192937, -32.77419517584195, -0.9363333551280546, -61.65847002433208, -0.182559027352075, -1.3441869339869565, -1.3854421600576674]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.5810464567793827, "mean_inference_ms": 2.7660251240559006, "mean_action_processing_ms": 0.10754241594332843, "mean_env_wait_ms": 3.321011793562235, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 1, "timesteps_total": 500, "timesteps_this_iter": 0, "agent_timesteps_total": 3000, "timers": {"sample_time_ms": 439.035, "sample_throughput": 113.886, "load_time_ms": 0.509, "load_throughput": 98314.753, "learn_time_ms": 72.605, "learn_throughput": 688.656, "update_time_ms": 2.0}, "info": {"learner": {"policy_leadadversary_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 0.000390625, "cur_lr": 0.0005, "total_loss": 2.060045599937439, "policy_loss": -0.0024332260061055155, "vf_loss": 2.0923683643341064, "vf_explained_var": -0.08331483602523804, "kl": 2.2511476518438656e-05, "entropy": 2.988942861557007, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_adversary_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 0.00039062499999999997, "cur_lr": 0.0005, "total_loss": 1.623358537753423, "policy_loss": -0.007441711301604907, "vf_loss": 1.6602303981781006, "vf_explained_var": -0.049086570739746094, "kl": 0.0003142756882900481, "entropy": 2.943019231160482, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_agent_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 0.000390625, "cur_lr": 0.0005, "total_loss": 304.5654937401414, "policy_loss": -0.0028681978583335876, "vf_loss": 304.5981157235801, "vf_explained_var": -0.0957111120223999, "kl": 9.555781708447952e-05, "entropy": 2.9790777564048767, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}}, "num_steps_sampled": 500, "num_agent_steps_sampled": 3000, "num_steps_trained": 500, "num_agent_steps_trained": 3000, "num_steps_trained_this_iter": 0}, "done": false, "episodes_total": 20, "training_iteration": 10, "trial_id": "f5f5f_00000", "experiment_id": "d79be65b63864cf3aab1c0ed35a1f497", "date": "2025-03-09_18-28-57", "timestamp": 1741541337, "time_this_iter_s": 0.4627358913421631, "time_total_s": 4.176328897476196, "pid": 48554, "hostname": "julien-PC", "node_ip": "192.168.89.58", "config": {"num_workers": 1, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 50, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 50, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "Centralized_Critic_Model", "custom_model_config": {"env": "mpe", "env_args": {"continuous_actions": false, "max_cycles": 25, "map_name": "simple_world_comm"}, "mask_flag": false, "global_state_flag": false, "opp_action_in_cc": true, "agent_level_batch_update": false, "force_coop": false, "local_mode": false, "share_policy": "group", "evaluation_interval": 50, "framework": "torch", "num_workers": 1, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "checkpoint_freq": 20, "checkpoint_end": true, "restore_path": {"model_path": "", "params_path": ""}, "stop_iters": 9999999, "stop_timesteps": 2000000, "stop_reward": 999999, "seed": 321, "local_dir": "", "model_arch_args": {"hidden_state_size": 256, "core_arch": "mlp", "fc_layer": 2, "out_dim_fc_0": 128, "out_dim_fc_1": 64, "encode_layer": "128-256"}, "algorithm": "mappo", "space_obs": "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "space_act": "Discrete(20)", "num_agents": 6, "episode_limit": 25, "policy_mapping_info": {"simple_adversary": {"description": "one team attack, one team survive", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_crypto": {"description": "two team cooperate, one team attack", "team_prefix": ["eve_", "bob_", "alice_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_push": {"description": "one team target on landmark, one team attack", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_tag": {"description": "one team attack, one team survive", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_spread": {"description": "one team cooperate", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}, "simple_reference": {"description": "one team cooperate", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}, "simple_world_comm": {"description": "two team cooperate and attack, one team survive", "team_prefix": ["adversary_", "leadadversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_speaker_listener": {"description": "two team cooperate", "team_prefix": ["speaker_", "listener_"], "all_agents_one_policy": true, "one_agent_one_policy": true}}, "agent_name_ls": ["leadadversary_0", "adversary_0", "adversary_1", "adversary_2", "agent_0", "agent_1"]}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "mpe_simple_world_comm", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 50, "evaluation_num_episodes": 10, "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 100, "min_iter_time_s": 0, "timesteps_per_iteration": 0, "seed": 321, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_adversary_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}], "policy_leadadversary_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}], "policy_agent_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x7f88afef41f0>", "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "simple_optimizer": false, "monitor": -1, "use_critic": true, "use_gae": true, "lambda": 1.0, "kl_coeff": 0.2, "sgd_minibatch_size": 50, "shuffle_sequences": true, "num_sgd_iter": 2, "lr_schedule": null, "vf_loss_coeff": 1.0, "entropy_coeff": 0.01, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": null, "kl_target": 0.01, "vf_share_layers": -1}, "time_since_restore": 4.176328897476196, "timesteps_since_restore": 0, "iterations_since_restore": 10, "perf": {"cpu_util_percent": 22.2, "ram_util_percent": 46.3}}
{"episode_reward_max": 19.012759487513534, "episode_reward_min": -155.78308767969756, "episode_reward_mean": -31.720912855132283, "episode_len_mean": 25.0, "episode_media": {}, "episodes_this_iter": 2, "policy_reward_min": {"policy_leadadversary_": -4.388542193694188, "policy_adversary_": -3.9338029974868833, "policy_agent_": -96.35415811344997}, "policy_reward_max": {"policy_leadadversary_": 9.03154180074615, "policy_adversary_": 9.22635362970437, "policy_agent_": 27.837397306389533}, "policy_reward_mean": {"policy_leadadversary_": -0.2668168328108837, "policy_adversary_": 0.10264191237940638, "policy_agent_": -15.881010879729814}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-105.16393252669111, -38.08425587795351, -8.179283354852107, -7.994660195117557, 17.029253709543042, -62.95754389372587, -21.306978110434503, -9.963224993494906, -35.6723275216737, -7.801818117525467, 9.04258457925305, -9.16790291343267, -23.11610041380425, -76.64256918047145, 19.012759487513534, -18.237441060454206, -26.428737443182527, -42.52695238562914, -72.55115007329658, -11.5502931963818, -9.816421651401027, -155.78308767969756], "episode_lengths": [25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25], "policy_policy_leadadversary__reward": [-2.399035557624381, 8.302786016962129, -1.9111450930988323, -1.90667729707557, -4.038309224748798, -1.1928023284527944, 6.938235712341814, -1.8308626607889866, 8.940230507526055, -2.0965552281416944, 9.03154180074615, -1.2461341680319717, -4.388542193694188, -3.847920286524414, 3.5200136941709386, -2.964544320613795, -2.9630602344890113, -2.5514853414160785, -3.423786335468951, -1.7669051568509833, -1.2819197880175521, -2.793092838548524], "policy_policy_adversary__reward": [-1.6741618220129826, -2.2223381552451884, -1.7978662959280882, 6.195230400528265, 9.085335341319167, 9.161857338996002, -1.1497964664252314, -0.7244895816458103, -2.705072794727737, -1.9452619286739838, -0.5002713058248459, -2.428351018980954, -2.073914420541661, -2.491769450548833, -1.928789335869684, -1.6434950773450425, -1.1869343121273628, -3.094481311936268, 9.22635362970437, 8.437625312282139, 8.558180033954198, -1.474364867288304, -2.699172611779865, -1.9577497509222836, 8.892856901464825, 8.219320737803885, 8.128349998386524, -2.4694792844144473, -0.6928708713293986, -1.5511211993405283, 6.420991744018833, 7.748318394913137, 8.185932035407232, -2.473275836552458, -0.5674346699607982, -2.7263543698194392, -0.5909371354705603, -1.2690244550254388, -1.8793974687070758, -2.8351443039978297, -3.923014083123016, -2.3079879234678007, 4.100268481179938, 3.027702044631114, 4.02297814215707, -2.0777870863086294, -1.9840069073615636, -1.669300530018625, -3.9338029974868833, -1.129238869347308, -1.3580459203598705, -2.250033013063289, -1.3601789265843973, -2.654726573595358, -1.7265988852649663, -3.7396060163471576, -1.820129784531345, -2.9201218301303506, -1.7090159550536768, -2.424621160302172, -2.53712801256559, -2.876435417206376, -1.4196451923447468, -1.5248898471676176, -2.768539034385942, -1.768760251247098], "policy_policy_agent__reward": [-0.7163725824305123, -96.35415811344997, -3.617249732269532, -67.21221524348954, -1.1897859301531337, -0.49899348880136807, -0.7213000577969194, -0.49279858676528443, 27.837397306389533, -0.27536116513751435, -1.8093468032775135, -54.03048406058688, -42.95203037708829, -11.515342421628725, -1.0331583960521016, -0.9679167066633642, -0.927224282567324, -68.92586138428767, -0.4035124455295982, -0.5882790887698023, -0.3080346699354489, -22.036164725896853, -1.0213367048557638, -1.1333671642122476, -0.6140299114418961, -14.374169249465094, -0.5039952235368317, -63.22450735982157, 9.76231673232199, -5.42051960694752, -0.38719184121053235, -9.154610374941065, -1.2188982803065158, -15.825691141192937, -32.77419517584195, -0.9363333551280546, -61.65847002433208, -0.182559027352075, -1.3441869339869565, -1.3854421600576674, -1.2320086747231598, -0.4692845665436012, -91.95162118944084, -54.97618451890754]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.5781398389498883, "mean_inference_ms": 2.761077764148904, "mean_action_processing_ms": 0.10724699335305261, "mean_env_wait_ms": 3.3190079323950332, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 1, "timesteps_total": 550, "timesteps_this_iter": 0, "agent_timesteps_total": 3300, "timers": {"sample_time_ms": 445.569, "sample_throughput": 112.216, "load_time_ms": 0.512, "load_throughput": 97628.23, "learn_time_ms": 71.85, "learn_throughput": 695.892, "update_time_ms": 1.958}, "info": {"learner": {"policy_leadadversary_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 0.0001953125, "cur_lr": 0.0005, "total_loss": 1.1540106534957886, "policy_loss": -0.002138477675615036, "vf_loss": 1.1860721707344055, "vf_explained_var": 0.08758214116096497, "kl": 1.1658975832773422e-05, "entropy": 2.9923126697540283, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_adversary_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 0.00019531249999999998, "cur_lr": 0.0005, "total_loss": 0.7595090468724569, "policy_loss": -0.007325252518057823, "vf_loss": 0.7963854844371477, "vf_explained_var": 0.03856534759203593, "kl": 0.0004688759018017426, "entropy": 2.9551174640655518, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_agent_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 0.0001953125, "cur_lr": 0.0005, "total_loss": 1174.0814475566149, "policy_loss": -0.006169915199279785, "vf_loss": 1174.1174533292651, "vf_explained_var": 0.07015706598758698, "kl": 0.00018879554586881397, "entropy": 2.98247754573822, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}}, "num_steps_sampled": 550, "num_agent_steps_sampled": 3300, "num_steps_trained": 550, "num_agent_steps_trained": 3300, "num_steps_trained_this_iter": 0}, "done": false, "episodes_total": 22, "training_iteration": 11, "trial_id": "f5f5f_00000", "experiment_id": "d79be65b63864cf3aab1c0ed35a1f497", "date": "2025-03-09_18-28-58", "timestamp": 1741541338, "time_this_iter_s": 0.42182397842407227, "time_total_s": 4.5981528759002686, "pid": 48554, "hostname": "julien-PC", "node_ip": "192.168.89.58", "config": {"num_workers": 1, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 50, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 50, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "Centralized_Critic_Model", "custom_model_config": {"env": "mpe", "env_args": {"continuous_actions": false, "max_cycles": 25, "map_name": "simple_world_comm"}, "mask_flag": false, "global_state_flag": false, "opp_action_in_cc": true, "agent_level_batch_update": false, "force_coop": false, "local_mode": false, "share_policy": "group", "evaluation_interval": 50, "framework": "torch", "num_workers": 1, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "checkpoint_freq": 20, "checkpoint_end": true, "restore_path": {"model_path": "", "params_path": ""}, "stop_iters": 9999999, "stop_timesteps": 2000000, "stop_reward": 999999, "seed": 321, "local_dir": "", "model_arch_args": {"hidden_state_size": 256, "core_arch": "mlp", "fc_layer": 2, "out_dim_fc_0": 128, "out_dim_fc_1": 64, "encode_layer": "128-256"}, "algorithm": "mappo", "space_obs": "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "space_act": "Discrete(20)", "num_agents": 6, "episode_limit": 25, "policy_mapping_info": {"simple_adversary": {"description": "one team attack, one team survive", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_crypto": {"description": "two team cooperate, one team attack", "team_prefix": ["eve_", "bob_", "alice_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_push": {"description": "one team target on landmark, one team attack", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_tag": {"description": "one team attack, one team survive", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_spread": {"description": "one team cooperate", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}, "simple_reference": {"description": "one team cooperate", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}, "simple_world_comm": {"description": "two team cooperate and attack, one team survive", "team_prefix": ["adversary_", "leadadversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_speaker_listener": {"description": "two team cooperate", "team_prefix": ["speaker_", "listener_"], "all_agents_one_policy": true, "one_agent_one_policy": true}}, "agent_name_ls": ["leadadversary_0", "adversary_0", "adversary_1", "adversary_2", "agent_0", "agent_1"]}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "mpe_simple_world_comm", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 50, "evaluation_num_episodes": 10, "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 100, "min_iter_time_s": 0, "timesteps_per_iteration": 0, "seed": 321, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_adversary_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}], "policy_leadadversary_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}], "policy_agent_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x7f88afef4dc0>", "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "simple_optimizer": false, "monitor": -1, "use_critic": true, "use_gae": true, "lambda": 1.0, "kl_coeff": 0.2, "sgd_minibatch_size": 50, "shuffle_sequences": true, "num_sgd_iter": 2, "lr_schedule": null, "vf_loss_coeff": 1.0, "entropy_coeff": 0.01, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": null, "kl_target": 0.01, "vf_share_layers": -1}, "time_since_restore": 4.5981528759002686, "timesteps_since_restore": 0, "iterations_since_restore": 11, "perf": {}}
{"episode_reward_max": 19.012759487513534, "episode_reward_min": -155.78308767969756, "episode_reward_mean": -31.070047049786524, "episode_len_mean": 25.0, "episode_media": {}, "episodes_this_iter": 2, "policy_reward_min": {"policy_leadadversary_": -4.388542193694188, "policy_adversary_": -3.9338029974868833, "policy_agent_": -96.35415811344997}, "policy_reward_max": {"policy_leadadversary_": 9.03154180074615, "policy_adversary_": 9.22635362970437, "policy_agent_": 27.837397306389533}, "policy_reward_mean": {"policy_leadadversary_": -0.4364191992597875, "policy_adversary_": -0.10878327134231089, "policy_agent_": -15.153639018249905}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-105.16393252669111, -38.08425587795351, -8.179283354852107, -7.994660195117557, 17.029253709543042, -62.95754389372587, -21.306978110434503, -9.963224993494906, -35.6723275216737, -7.801818117525467, 9.04258457925305, -9.16790291343267, -23.11610041380425, -76.64256918047145, 19.012759487513534, -18.237441060454206, -26.428737443182527, -42.52695238562914, -72.55115007329658, -11.5502931963818, -9.816421651401027, -155.78308767969756, -37.19573589166182, -10.625310490304468], "episode_lengths": [25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25], "policy_policy_leadadversary__reward": [-2.399035557624381, 8.302786016962129, -1.9111450930988323, -1.90667729707557, -4.038309224748798, -1.1928023284527944, 6.938235712341814, -1.8308626607889866, 8.940230507526055, -2.0965552281416944, 9.03154180074615, -1.2461341680319717, -4.388542193694188, -3.847920286524414, 3.5200136941709386, -2.964544320613795, -2.9630602344890113, -2.5514853414160785, -3.423786335468951, -1.7669051568509833, -1.2819197880175521, -2.793092838548524, -2.715631555274084, -1.888458905121377], "policy_policy_adversary__reward": [-1.6741618220129826, -2.2223381552451884, -1.7978662959280882, 6.195230400528265, 9.085335341319167, 9.161857338996002, -1.1497964664252314, -0.7244895816458103, -2.705072794727737, -1.9452619286739838, -0.5002713058248459, -2.428351018980954, -2.073914420541661, -2.491769450548833, -1.928789335869684, -1.6434950773450425, -1.1869343121273628, -3.094481311936268, 9.22635362970437, 8.437625312282139, 8.558180033954198, -1.474364867288304, -2.699172611779865, -1.9577497509222836, 8.892856901464825, 8.219320737803885, 8.128349998386524, -2.4694792844144473, -0.6928708713293986, -1.5511211993405283, 6.420991744018833, 7.748318394913137, 8.185932035407232, -2.473275836552458, -0.5674346699607982, -2.7263543698194392, -0.5909371354705603, -1.2690244550254388, -1.8793974687070758, -2.8351443039978297, -3.923014083123016, -2.3079879234678007, 4.100268481179938, 3.027702044631114, 4.02297814215707, -2.0777870863086294, -1.9840069073615636, -1.669300530018625, -3.9338029974868833, -1.129238869347308, -1.3580459203598705, -2.250033013063289, -1.3601789265843973, -2.654726573595358, -1.7265988852649663, -3.7396060163471576, -1.820129784531345, -2.9201218301303506, -1.7090159550536768, -2.424621160302172, -2.53712801256559, -2.876435417206376, -1.4196451923447468, -1.5248898471676176, -2.768539034385942, -1.768760251247098, -3.013244773720978, -3.0025095397079458, -1.7121641935407885, -1.6493536919918832, -3.9070691940185345, -1.3224203607070766], "policy_policy_agent__reward": [-0.7163725824305123, -96.35415811344997, -3.617249732269532, -67.21221524348954, -1.1897859301531337, -0.49899348880136807, -0.7213000577969194, -0.49279858676528443, 27.837397306389533, -0.27536116513751435, -1.8093468032775135, -54.03048406058688, -42.95203037708829, -11.515342421628725, -1.0331583960521016, -0.9679167066633642, -0.927224282567324, -68.92586138428767, -0.4035124455295982, -0.5882790887698023, -0.3080346699354489, -22.036164725896853, -1.0213367048557638, -1.1333671642122476, -0.6140299114418961, -14.374169249465094, -0.5039952235368317, -63.22450735982157, 9.76231673232199, -5.42051960694752, -0.38719184121053235, -9.154610374941065, -1.2188982803065158, -15.825691141192937, -32.77419517584195, -0.9363333551280546, -61.65847002433208, -0.182559027352075, -1.3441869339869565, -1.3854421600576674, -1.2320086747231598, -0.4692845665436012, -91.95162118944084, -54.97618451890754, -25.81192265544753, -0.9402631739704812, -0.7369098771347249, -1.1210984613308634]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.5755334188217162, "mean_inference_ms": 2.7564903055302055, "mean_action_processing_ms": 0.10697817004403327, "mean_env_wait_ms": 3.316838135672312, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 1, "timesteps_total": 600, "timesteps_this_iter": 0, "agent_timesteps_total": 3600, "timers": {"sample_time_ms": 443.229, "sample_throughput": 112.809, "load_time_ms": 0.507, "load_throughput": 98545.745, "learn_time_ms": 71.307, "learn_throughput": 701.194, "update_time_ms": 1.837}, "info": {"learner": {"policy_leadadversary_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 9.765625e-05, "cur_lr": 0.0005, "total_loss": 1.232650339603424, "policy_loss": -0.0013998984917975577, "vf_loss": 1.263925850391388, "vf_explained_var": 0.002066373825073242, "kl": 9.872631554053157e-06, "entropy": 2.987551212310791, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_adversary_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 9.765624999999999e-05, "cur_lr": 0.0005, "total_loss": 1.0280315379301708, "policy_loss": -0.008692316710948944, "vf_loss": 1.066215343773365, "vf_explained_var": -0.011475970347722372, "kl": 0.0008057738408773218, "entropy": 2.949159264564514, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_agent_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 9.765625e-05, "cur_lr": 0.0005, "total_loss": 40.85015232488513, "policy_loss": -0.005830489099025726, "vf_loss": 40.885631401091814, "vf_explained_var": 0.31571677327156067, "kl": 0.0001759425633600653, "entropy": 2.9647178649902344, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}}, "num_steps_sampled": 600, "num_agent_steps_sampled": 3600, "num_steps_trained": 600, "num_agent_steps_trained": 3600, "num_steps_trained_this_iter": 0}, "done": false, "episodes_total": 24, "training_iteration": 12, "trial_id": "f5f5f_00000", "experiment_id": "d79be65b63864cf3aab1c0ed35a1f497", "date": "2025-03-09_18-28-58", "timestamp": 1741541338, "time_this_iter_s": 0.3994753360748291, "time_total_s": 4.997628211975098, "pid": 48554, "hostname": "julien-PC", "node_ip": "192.168.89.58", "config": {"num_workers": 1, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 50, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 50, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "Centralized_Critic_Model", "custom_model_config": {"env": "mpe", "env_args": {"continuous_actions": false, "max_cycles": 25, "map_name": "simple_world_comm"}, "mask_flag": false, "global_state_flag": false, "opp_action_in_cc": true, "agent_level_batch_update": false, "force_coop": false, "local_mode": false, "share_policy": "group", "evaluation_interval": 50, "framework": "torch", "num_workers": 1, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "checkpoint_freq": 20, "checkpoint_end": true, "restore_path": {"model_path": "", "params_path": ""}, "stop_iters": 9999999, "stop_timesteps": 2000000, "stop_reward": 999999, "seed": 321, "local_dir": "", "model_arch_args": {"hidden_state_size": 256, "core_arch": "mlp", "fc_layer": 2, "out_dim_fc_0": 128, "out_dim_fc_1": 64, "encode_layer": "128-256"}, "algorithm": "mappo", "space_obs": "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "space_act": "Discrete(20)", "num_agents": 6, "episode_limit": 25, "policy_mapping_info": {"simple_adversary": {"description": "one team attack, one team survive", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_crypto": {"description": "two team cooperate, one team attack", "team_prefix": ["eve_", "bob_", "alice_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_push": {"description": "one team target on landmark, one team attack", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_tag": {"description": "one team attack, one team survive", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_spread": {"description": "one team cooperate", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}, "simple_reference": {"description": "one team cooperate", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}, "simple_world_comm": {"description": "two team cooperate and attack, one team survive", "team_prefix": ["adversary_", "leadadversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_speaker_listener": {"description": "two team cooperate", "team_prefix": ["speaker_", "listener_"], "all_agents_one_policy": true, "one_agent_one_policy": true}}, "agent_name_ls": ["leadadversary_0", "adversary_0", "adversary_1", "adversary_2", "agent_0", "agent_1"]}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "mpe_simple_world_comm", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 50, "evaluation_num_episodes": 10, "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 100, "min_iter_time_s": 0, "timesteps_per_iteration": 0, "seed": 321, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_adversary_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}], "policy_leadadversary_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}], "policy_agent_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x7f88aff1a550>", "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "simple_optimizer": false, "monitor": -1, "use_critic": true, "use_gae": true, "lambda": 1.0, "kl_coeff": 0.2, "sgd_minibatch_size": 50, "shuffle_sequences": true, "num_sgd_iter": 2, "lr_schedule": null, "vf_loss_coeff": 1.0, "entropy_coeff": 0.01, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": null, "kl_target": 0.01, "vf_share_layers": -1}, "time_since_restore": 4.997628211975098, "timesteps_since_restore": 0, "iterations_since_restore": 12, "perf": {"cpu_util_percent": 24.5, "ram_util_percent": 46.5}}
{"episode_reward_max": 56.41119583016825, "episode_reward_min": -155.78308767969756, "episode_reward_mean": -25.71751198023821, "episode_len_mean": 25.0, "episode_media": {}, "episodes_this_iter": 2, "policy_reward_min": {"policy_leadadversary_": -4.388542193694188, "policy_adversary_": -3.9338029974868833, "policy_agent_": -96.35415811344997}, "policy_reward_max": {"policy_leadadversary_": 19.490398201983847, "policy_adversary_": 19.519538085643973, "policy_agent_": 27.837397306389533}, "policy_reward_mean": {"policy_leadadversary_": 0.6733736068630386, "policy_adversary_": 0.9571833836952583, "policy_agent_": -14.631217869093511}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-105.16393252669111, -38.08425587795351, -8.179283354852107, -7.994660195117557, 17.029253709543042, -62.95754389372587, -21.306978110434503, -9.963224993494906, -35.6723275216737, -7.801818117525467, 9.04258457925305, -9.16790291343267, -23.11610041380425, -76.64256918047145, 19.012759487513534, -18.237441060454206, -26.428737443182527, -42.52695238562914, -72.55115007329658, -11.5502931963818, -9.816421651401027, -155.78308767969756, -37.19573589166182, -10.625310490304468, 20.61462187851487, 56.41119583016825], "episode_lengths": [25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25], "policy_policy_leadadversary__reward": [-2.399035557624381, 8.302786016962129, -1.9111450930988323, -1.90667729707557, -4.038309224748798, -1.1928023284527944, 6.938235712341814, -1.8308626607889866, 8.940230507526055, -2.0965552281416944, 9.03154180074615, -1.2461341680319717, -4.388542193694188, -3.847920286524414, 3.5200136941709386, -2.964544320613795, -2.9630602344890113, -2.5514853414160785, -3.423786335468951, -1.7669051568509833, -1.2819197880175521, -2.793092838548524, -2.715631555274084, -1.888458905121377, 8.491376358690058, 19.490398201983847], "policy_policy_adversary__reward": [-1.6741618220129826, -2.2223381552451884, -1.7978662959280882, 6.195230400528265, 9.085335341319167, 9.161857338996002, -1.1497964664252314, -0.7244895816458103, -2.705072794727737, -1.9452619286739838, -0.5002713058248459, -2.428351018980954, -2.073914420541661, -2.491769450548833, -1.928789335869684, -1.6434950773450425, -1.1869343121273628, -3.094481311936268, 9.22635362970437, 8.437625312282139, 8.558180033954198, -1.474364867288304, -2.699172611779865, -1.9577497509222836, 8.892856901464825, 8.219320737803885, 8.128349998386524, -2.4694792844144473, -0.6928708713293986, -1.5511211993405283, 6.420991744018833, 7.748318394913137, 8.185932035407232, -2.473275836552458, -0.5674346699607982, -2.7263543698194392, -0.5909371354705603, -1.2690244550254388, -1.8793974687070758, -2.8351443039978297, -3.923014083123016, -2.3079879234678007, 4.100268481179938, 3.027702044631114, 4.02297814215707, -2.0777870863086294, -1.9840069073615636, -1.669300530018625, -3.9338029974868833, -1.129238869347308, -1.3580459203598705, -2.250033013063289, -1.3601789265843973, -2.654726573595358, -1.7265988852649663, -3.7396060163471576, -1.820129784531345, -2.9201218301303506, -1.7090159550536768, -2.424621160302172, -2.53712801256559, -2.876435417206376, -1.4196451923447468, -1.5248898471676176, -2.768539034385942, -1.768760251247098, -3.013244773720978, -3.0025095397079458, -1.7121641935407885, -1.6493536919918832, -3.9070691940185345, -1.3224203607070766, 7.256290170226535, 8.009821795532776, 9.511926850083018, 19.35511620124706, 18.84000636214317, 19.519538085643973], "policy_policy_agent__reward": [-0.7163725824305123, -96.35415811344997, -3.617249732269532, -67.21221524348954, -1.1897859301531337, -0.49899348880136807, -0.7213000577969194, -0.49279858676528443, 27.837397306389533, -0.27536116513751435, -1.8093468032775135, -54.03048406058688, -42.95203037708829, -11.515342421628725, -1.0331583960521016, -0.9679167066633642, -0.927224282567324, -68.92586138428767, -0.4035124455295982, -0.5882790887698023, -0.3080346699354489, -22.036164725896853, -1.0213367048557638, -1.1333671642122476, -0.6140299114418961, -14.374169249465094, -0.5039952235368317, -63.22450735982157, 9.76231673232199, -5.42051960694752, -0.38719184121053235, -9.154610374941065, -1.2188982803065158, -15.825691141192937, -32.77419517584195, -0.9363333551280546, -61.65847002433208, -0.182559027352075, -1.3441869339869565, -1.3854421600576674, -1.2320086747231598, -0.4692845665436012, -91.95162118944084, -54.97618451890754, -25.81192265544753, -0.9402631739704812, -0.7369098771347249, -1.1210984613308634, -1.2206295166227459, -11.434163779394773, -20.24663057019898, -0.5472324506508123]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.5729906157791889, "mean_inference_ms": 2.7510391755194936, "mean_action_processing_ms": 0.10671608632327895, "mean_env_wait_ms": 3.3142380779206295, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 1, "timesteps_total": 650, "timesteps_this_iter": 0, "agent_timesteps_total": 3900, "timers": {"sample_time_ms": 436.73, "sample_throughput": 114.487, "load_time_ms": 0.509, "load_throughput": 98195.065, "learn_time_ms": 71.096, "learn_throughput": 703.279, "update_time_ms": 1.84}, "info": {"learner": {"policy_leadadversary_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 4.8828125e-05, "cur_lr": 0.0005, "total_loss": 98.53156661987305, "policy_loss": -0.002529929960147115, "vf_loss": 98.56396102905273, "vf_explained_var": 0.0016281306743621826, "kl": 1.201875642564687e-05, "entropy": 2.9860678911209106, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_adversary_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 4.8828124999999996e-05, "cur_lr": 0.0005, "total_loss": 99.64289093017578, "policy_loss": -0.002922790435453256, "vf_loss": 99.67563374837239, "vf_explained_var": -0.011847923199335733, "kl": 0.00014730529274675774, "entropy": 2.9815456867218018, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_agent_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 4.8828125e-05, "cur_lr": 0.0005, "total_loss": 50.59625053405762, "policy_loss": -0.0050486549735069275, "vf_loss": 50.6311731338501, "vf_explained_var": -0.005903899669647217, "kl": 8.274678525421741e-05, "entropy": 2.9872870445251465, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}}, "num_steps_sampled": 650, "num_agent_steps_sampled": 3900, "num_steps_trained": 650, "num_agent_steps_trained": 3900, "num_steps_trained_this_iter": 0}, "done": false, "episodes_total": 26, "training_iteration": 13, "trial_id": "f5f5f_00000", "experiment_id": "d79be65b63864cf3aab1c0ed35a1f497", "date": "2025-03-09_18-28-58", "timestamp": 1741541338, "time_this_iter_s": 0.38449907302856445, "time_total_s": 5.382127285003662, "pid": 48554, "hostname": "julien-PC", "node_ip": "192.168.89.58", "config": {"num_workers": 1, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 50, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 50, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "Centralized_Critic_Model", "custom_model_config": {"env": "mpe", "env_args": {"continuous_actions": false, "max_cycles": 25, "map_name": "simple_world_comm"}, "mask_flag": false, "global_state_flag": false, "opp_action_in_cc": true, "agent_level_batch_update": false, "force_coop": false, "local_mode": false, "share_policy": "group", "evaluation_interval": 50, "framework": "torch", "num_workers": 1, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "checkpoint_freq": 20, "checkpoint_end": true, "restore_path": {"model_path": "", "params_path": ""}, "stop_iters": 9999999, "stop_timesteps": 2000000, "stop_reward": 999999, "seed": 321, "local_dir": "", "model_arch_args": {"hidden_state_size": 256, "core_arch": "mlp", "fc_layer": 2, "out_dim_fc_0": 128, "out_dim_fc_1": 64, "encode_layer": "128-256"}, "algorithm": "mappo", "space_obs": "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "space_act": "Discrete(20)", "num_agents": 6, "episode_limit": 25, "policy_mapping_info": {"simple_adversary": {"description": "one team attack, one team survive", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_crypto": {"description": "two team cooperate, one team attack", "team_prefix": ["eve_", "bob_", "alice_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_push": {"description": "one team target on landmark, one team attack", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_tag": {"description": "one team attack, one team survive", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_spread": {"description": "one team cooperate", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}, "simple_reference": {"description": "one team cooperate", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}, "simple_world_comm": {"description": "two team cooperate and attack, one team survive", "team_prefix": ["adversary_", "leadadversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_speaker_listener": {"description": "two team cooperate", "team_prefix": ["speaker_", "listener_"], "all_agents_one_policy": true, "one_agent_one_policy": true}}, "agent_name_ls": ["leadadversary_0", "adversary_0", "adversary_1", "adversary_2", "agent_0", "agent_1"]}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "mpe_simple_world_comm", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 50, "evaluation_num_episodes": 10, "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 100, "min_iter_time_s": 0, "timesteps_per_iteration": 0, "seed": 321, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_adversary_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}], "policy_leadadversary_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}], "policy_agent_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x7f88aff00310>", "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "simple_optimizer": false, "monitor": -1, "use_critic": true, "use_gae": true, "lambda": 1.0, "kl_coeff": 0.2, "sgd_minibatch_size": 50, "shuffle_sequences": true, "num_sgd_iter": 2, "lr_schedule": null, "vf_loss_coeff": 1.0, "entropy_coeff": 0.01, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": null, "kl_target": 0.01, "vf_share_layers": -1}, "time_since_restore": 5.382127285003662, "timesteps_since_restore": 0, "iterations_since_restore": 13, "perf": {"cpu_util_percent": 16.0, "ram_util_percent": 46.5}}
{"episode_reward_max": 56.41119583016825, "episode_reward_min": -155.78308767969756, "episode_reward_mean": -24.788304381570974, "episode_len_mean": 25.0, "episode_media": {}, "episodes_this_iter": 2, "policy_reward_min": {"policy_leadadversary_": -4.388542193694188, "policy_adversary_": -3.9338029974868833, "policy_agent_": -96.35415811344997}, "policy_reward_max": {"policy_leadadversary_": 19.490398201983847, "policy_adversary_": 19.519538085643973, "policy_agent_": 27.837397306389533}, "policy_reward_mean": {"policy_leadadversary_": 0.43302875069131763, "policy_adversary_": 0.6998437085154029, "policy_agent_": -13.660432128904253}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-105.16393252669111, -38.08425587795351, -8.179283354852107, -7.994660195117557, 17.029253709543042, -62.95754389372587, -21.306978110434503, -9.963224993494906, -35.6723275216737, -7.801818117525467, 9.04258457925305, -9.16790291343267, -23.11610041380425, -76.64256918047145, 19.012759487513534, -18.237441060454206, -26.428737443182527, -42.52695238562914, -72.55115007329658, -11.5502931963818, -9.816421651401027, -155.78308767969756, -37.19573589166182, -10.625310490304468, 20.61462187851487, 56.41119583016825, -13.315507414957573, -12.10170378283629], "episode_lengths": [25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25], "policy_policy_leadadversary__reward": [-2.399035557624381, 8.302786016962129, -1.9111450930988323, -1.90667729707557, -4.038309224748798, -1.1928023284527944, 6.938235712341814, -1.8308626607889866, 8.940230507526055, -2.0965552281416944, 9.03154180074615, -1.2461341680319717, -4.388542193694188, -3.847920286524414, 3.5200136941709386, -2.964544320613795, -2.9630602344890113, -2.5514853414160785, -3.423786335468951, -1.7669051568509833, -1.2819197880175521, -2.793092838548524, -2.715631555274084, -1.888458905121377, 8.491376358690058, 19.490398201983847, -3.414607305270243, -1.9683014538118688], "policy_policy_adversary__reward": [-1.6741618220129826, -2.2223381552451884, -1.7978662959280882, 6.195230400528265, 9.085335341319167, 9.161857338996002, -1.1497964664252314, -0.7244895816458103, -2.705072794727737, -1.9452619286739838, -0.5002713058248459, -2.428351018980954, -2.073914420541661, -2.491769450548833, -1.928789335869684, -1.6434950773450425, -1.1869343121273628, -3.094481311936268, 9.22635362970437, 8.437625312282139, 8.558180033954198, -1.474364867288304, -2.699172611779865, -1.9577497509222836, 8.892856901464825, 8.219320737803885, 8.128349998386524, -2.4694792844144473, -0.6928708713293986, -1.5511211993405283, 6.420991744018833, 7.748318394913137, 8.185932035407232, -2.473275836552458, -0.5674346699607982, -2.7263543698194392, -0.5909371354705603, -1.2690244550254388, -1.8793974687070758, -2.8351443039978297, -3.923014083123016, -2.3079879234678007, 4.100268481179938, 3.027702044631114, 4.02297814215707, -2.0777870863086294, -1.9840069073615636, -1.669300530018625, -3.9338029974868833, -1.129238869347308, -1.3580459203598705, -2.250033013063289, -1.3601789265843973, -2.654726573595358, -1.7265988852649663, -3.7396060163471576, -1.820129784531345, -2.9201218301303506, -1.7090159550536768, -2.424621160302172, -2.53712801256559, -2.876435417206376, -1.4196451923447468, -1.5248898471676176, -2.768539034385942, -1.768760251247098, -3.013244773720978, -3.0025095397079458, -1.7121641935407885, -1.6493536919918832, -3.9070691940185345, -1.3224203607070766, 7.256290170226535, 8.009821795532776, 9.511926850083018, 19.35511620124706, 18.84000636214317, 19.519538085643973, -1.4999996116110483, -3.8952631049612574, -2.070937464898907, -2.546883520298444, -2.9137085640161007, -2.9466401471505344], "policy_policy_agent__reward": [-0.7163725824305123, -96.35415811344997, -3.617249732269532, -67.21221524348954, -1.1897859301531337, -0.49899348880136807, -0.7213000577969194, -0.49279858676528443, 27.837397306389533, -0.27536116513751435, -1.8093468032775135, -54.03048406058688, -42.95203037708829, -11.515342421628725, -1.0331583960521016, -0.9679167066633642, -0.927224282567324, -68.92586138428767, -0.4035124455295982, -0.5882790887698023, -0.3080346699354489, -22.036164725896853, -1.0213367048557638, -1.1333671642122476, -0.6140299114418961, -14.374169249465094, -0.5039952235368317, -63.22450735982157, 9.76231673232199, -5.42051960694752, -0.38719184121053235, -9.154610374941065, -1.2188982803065158, -15.825691141192937, -32.77419517584195, -0.9363333551280546, -61.65847002433208, -0.182559027352075, -1.3441869339869565, -1.3854421600576674, -1.2320086747231598, -0.4692845665436012, -91.95162118944084, -54.97618451890754, -25.81192265544753, -0.9402631739704812, -0.7369098771347249, -1.1210984613308634, -1.2206295166227459, -11.434163779394773, -20.24663057019898, -0.5472324506508123, -0.7910180498339603, -1.6436818783821536, -0.8500054566128976, -0.8761646409464355]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.57062926416587, "mean_inference_ms": 2.745973256208644, "mean_action_processing_ms": 0.10649206432189784, "mean_env_wait_ms": 3.311637839782304, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 1, "timesteps_total": 700, "timesteps_this_iter": 0, "agent_timesteps_total": 4200, "timers": {"sample_time_ms": 437.077, "sample_throughput": 114.396, "load_time_ms": 0.509, "load_throughput": 98291.714, "learn_time_ms": 71.049, "learn_throughput": 703.739, "update_time_ms": 1.849}, "info": {"learner": {"policy_leadadversary_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 2.44140625e-05, "cur_lr": 0.0005, "total_loss": 1.9302046298980713, "policy_loss": -0.0032387507893143486, "vf_loss": 1.9631713032722473, "vf_explained_var": 0.06449395418167114, "kl": 3.669388025517861e-05, "entropy": 2.9727907180786133, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_adversary_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 2.4414062499999998e-05, "cur_lr": 0.0005, "total_loss": 1.4154575765132904, "policy_loss": -0.007855477432409922, "vf_loss": 1.452760378519694, "vf_explained_var": 0.002133737007776896, "kl": 0.00033828441386507535, "entropy": 2.9447356859842935, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_agent_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 2.44140625e-05, "cur_lr": 0.0005, "total_loss": 1.309203177690506, "policy_loss": -0.0036101043224334717, "vf_loss": 1.342429332435131, "vf_explained_var": -0.4791221469640732, "kl": 0.0001459386110047589, "entropy": 2.9615999460220337, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}}, "num_steps_sampled": 700, "num_agent_steps_sampled": 4200, "num_steps_trained": 700, "num_agent_steps_trained": 4200, "num_steps_trained_this_iter": 0}, "done": false, "episodes_total": 28, "training_iteration": 14, "trial_id": "f5f5f_00000", "experiment_id": "d79be65b63864cf3aab1c0ed35a1f497", "date": "2025-03-09_18-28-59", "timestamp": 1741541339, "time_this_iter_s": 0.3994290828704834, "time_total_s": 5.7815563678741455, "pid": 48554, "hostname": "julien-PC", "node_ip": "192.168.89.58", "config": {"num_workers": 1, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 50, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 50, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "Centralized_Critic_Model", "custom_model_config": {"env": "mpe", "env_args": {"continuous_actions": false, "max_cycles": 25, "map_name": "simple_world_comm"}, "mask_flag": false, "global_state_flag": false, "opp_action_in_cc": true, "agent_level_batch_update": false, "force_coop": false, "local_mode": false, "share_policy": "group", "evaluation_interval": 50, "framework": "torch", "num_workers": 1, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "checkpoint_freq": 20, "checkpoint_end": true, "restore_path": {"model_path": "", "params_path": ""}, "stop_iters": 9999999, "stop_timesteps": 2000000, "stop_reward": 999999, "seed": 321, "local_dir": "", "model_arch_args": {"hidden_state_size": 256, "core_arch": "mlp", "fc_layer": 2, "out_dim_fc_0": 128, "out_dim_fc_1": 64, "encode_layer": "128-256"}, "algorithm": "mappo", "space_obs": "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "space_act": "Discrete(20)", "num_agents": 6, "episode_limit": 25, "policy_mapping_info": {"simple_adversary": {"description": "one team attack, one team survive", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_crypto": {"description": "two team cooperate, one team attack", "team_prefix": ["eve_", "bob_", "alice_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_push": {"description": "one team target on landmark, one team attack", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_tag": {"description": "one team attack, one team survive", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_spread": {"description": "one team cooperate", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}, "simple_reference": {"description": "one team cooperate", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}, "simple_world_comm": {"description": "two team cooperate and attack, one team survive", "team_prefix": ["adversary_", "leadadversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_speaker_listener": {"description": "two team cooperate", "team_prefix": ["speaker_", "listener_"], "all_agents_one_policy": true, "one_agent_one_policy": true}}, "agent_name_ls": ["leadadversary_0", "adversary_0", "adversary_1", "adversary_2", "agent_0", "agent_1"]}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "mpe_simple_world_comm", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 50, "evaluation_num_episodes": 10, "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 100, "min_iter_time_s": 0, "timesteps_per_iteration": 0, "seed": 321, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_adversary_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}], "policy_leadadversary_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}], "policy_agent_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x7f88aff00dc0>", "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "simple_optimizer": false, "monitor": -1, "use_critic": true, "use_gae": true, "lambda": 1.0, "kl_coeff": 0.2, "sgd_minibatch_size": 50, "shuffle_sequences": true, "num_sgd_iter": 2, "lr_schedule": null, "vf_loss_coeff": 1.0, "entropy_coeff": 0.01, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": null, "kl_target": 0.01, "vf_share_layers": -1}, "time_since_restore": 5.7815563678741455, "timesteps_since_restore": 0, "iterations_since_restore": 14, "perf": {}}
{"episode_reward_max": 98.42627085353728, "episode_reward_min": -155.78308767969756, "episode_reward_mean": -20.956496896227165, "episode_len_mean": 25.0, "episode_media": {}, "episodes_this_iter": 2, "policy_reward_min": {"policy_leadadversary_": -4.388542193694188, "policy_adversary_": -3.9338029974868833, "policy_agent_": -96.35415811344997}, "policy_reward_max": {"policy_leadadversary_": 33.854903602692595, "policy_adversary_": 34.36826010203567, "policy_agent_": 27.837397306389533}, "policy_reward_mean": {"policy_leadadversary_": 1.457636946625867, "policy_adversary_": 1.714572559265687, "policy_agent_": -13.778925760325048}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-105.16393252669111, -38.08425587795351, -8.179283354852107, -7.994660195117557, 17.029253709543042, -62.95754389372587, -21.306978110434503, -9.963224993494906, -35.6723275216737, -7.801818117525467, 9.04258457925305, -9.16790291343267, -23.11610041380425, -76.64256918047145, 19.012759487513534, -18.237441060454206, -26.428737443182527, -42.52695238562914, -72.55115007329658, -11.5502931963818, -9.816421651401027, -155.78308767969756, -37.19573589166182, -10.625310490304468, 20.61462187851487, 56.41119583016825, -13.315507414957573, -12.10170378283629, 98.42627085353728, -33.048655056364915], "episode_lengths": [25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25], "policy_policy_leadadversary__reward": [-2.399035557624381, 8.302786016962129, -1.9111450930988323, -1.90667729707557, -4.038309224748798, -1.1928023284527944, 6.938235712341814, -1.8308626607889866, 8.940230507526055, -2.0965552281416944, 9.03154180074615, -1.2461341680319717, -4.388542193694188, -3.847920286524414, 3.5200136941709386, -2.964544320613795, -2.9630602344890113, -2.5514853414160785, -3.423786335468951, -1.7669051568509833, -1.2819197880175521, -2.793092838548524, -2.715631555274084, -1.888458905121377, 8.491376358690058, 19.490398201983847, -3.414607305270243, -1.9683014538118688, 33.854903602692595, -2.2506002232734774], "policy_policy_adversary__reward": [-1.6741618220129826, -2.2223381552451884, -1.7978662959280882, 6.195230400528265, 9.085335341319167, 9.161857338996002, -1.1497964664252314, -0.7244895816458103, -2.705072794727737, -1.9452619286739838, -0.5002713058248459, -2.428351018980954, -2.073914420541661, -2.491769450548833, -1.928789335869684, -1.6434950773450425, -1.1869343121273628, -3.094481311936268, 9.22635362970437, 8.437625312282139, 8.558180033954198, -1.474364867288304, -2.699172611779865, -1.9577497509222836, 8.892856901464825, 8.219320737803885, 8.128349998386524, -2.4694792844144473, -0.6928708713293986, -1.5511211993405283, 6.420991744018833, 7.748318394913137, 8.185932035407232, -2.473275836552458, -0.5674346699607982, -2.7263543698194392, -0.5909371354705603, -1.2690244550254388, -1.8793974687070758, -2.8351443039978297, -3.923014083123016, -2.3079879234678007, 4.100268481179938, 3.027702044631114, 4.02297814215707, -2.0777870863086294, -1.9840069073615636, -1.669300530018625, -3.9338029974868833, -1.129238869347308, -1.3580459203598705, -2.250033013063289, -1.3601789265843973, -2.654726573595358, -1.7265988852649663, -3.7396060163471576, -1.820129784531345, -2.9201218301303506, -1.7090159550536768, -2.424621160302172, -2.53712801256559, -2.876435417206376, -1.4196451923447468, -1.5248898471676176, -2.768539034385942, -1.768760251247098, -3.013244773720978, -3.0025095397079458, -1.7121641935407885, -1.6493536919918832, -3.9070691940185345, -1.3224203607070766, 7.256290170226535, 8.009821795532776, 9.511926850083018, 19.35511620124706, 18.84000636214317, 19.519538085643973, -1.4999996116110483, -3.8952631049612574, -2.070937464898907, -2.546883520298444, -2.9137085640161007, -2.9466401471505344, 33.50823641156576, 34.36826010203567, 32.62555861085578, -1.594179646955657, -0.7648866358471844, -2.61833002303639], "policy_policy_agent__reward": [-0.7163725824305123, -96.35415811344997, -3.617249732269532, -67.21221524348954, -1.1897859301531337, -0.49899348880136807, -0.7213000577969194, -0.49279858676528443, 27.837397306389533, -0.27536116513751435, -1.8093468032775135, -54.03048406058688, -42.95203037708829, -11.515342421628725, -1.0331583960521016, -0.9679167066633642, -0.927224282567324, -68.92586138428767, -0.4035124455295982, -0.5882790887698023, -0.3080346699354489, -22.036164725896853, -1.0213367048557638, -1.1333671642122476, -0.6140299114418961, -14.374169249465094, -0.5039952235368317, -63.22450735982157, 9.76231673232199, -5.42051960694752, -0.38719184121053235, -9.154610374941065, -1.2188982803065158, -15.825691141192937, -32.77419517584195, -0.9363333551280546, -61.65847002433208, -0.182559027352075, -1.3441869339869565, -1.3854421600576674, -1.2320086747231598, -0.4692845665436012, -91.95162118944084, -54.97618451890754, -25.81192265544753, -0.9402631739704812, -0.7369098771347249, -1.1210984613308634, -1.2206295166227459, -11.434163779394773, -20.24663057019898, -0.5472324506508123, -0.7910180498339603, -1.6436818783821536, -0.8500054566128976, -0.8761646409464355, -20.63989374553544, -15.290794128077069, -1.1518478368502902, -24.66881069040191]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.5683587140557939, "mean_inference_ms": 2.740513430409311, "mean_action_processing_ms": 0.1062588156352545, "mean_env_wait_ms": 3.308812925780856, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 1, "timesteps_total": 750, "timesteps_this_iter": 0, "agent_timesteps_total": 4500, "timers": {"sample_time_ms": 435.902, "sample_throughput": 114.705, "load_time_ms": 0.506, "load_throughput": 98866.302, "learn_time_ms": 70.988, "learn_throughput": 704.348, "update_time_ms": 1.815}, "info": {"learner": {"policy_leadadversary_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 1.220703125e-05, "cur_lr": 0.0005, "total_loss": 304.9073944091797, "policy_loss": -0.0035540301166481214, "vf_loss": 304.94065856933594, "vf_explained_var": -0.007585465908050537, "kl": 5.10352494780264e-05, "entropy": 2.9694868326187134, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_adversary_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 1.2207031249999999e-05, "cur_lr": 0.0005, "total_loss": 296.6415762106578, "policy_loss": -0.006596554769203067, "vf_loss": 296.6779135465622, "vf_explained_var": -0.008326520522435507, "kl": 0.0002703764500478674, "entropy": 2.970482349395752, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_agent_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 1.220703125e-05, "cur_lr": 0.0005, "total_loss": 151.25583267211914, "policy_loss": -0.0048827119171619415, "vf_loss": 151.29044342041016, "vf_explained_var": 0.010844439268112183, "kl": 0.00019883239550733123, "entropy": 2.974142074584961, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}}, "num_steps_sampled": 750, "num_agent_steps_sampled": 4500, "num_steps_trained": 750, "num_agent_steps_trained": 4500, "num_steps_trained_this_iter": 0}, "done": false, "episodes_total": 30, "training_iteration": 15, "trial_id": "f5f5f_00000", "experiment_id": "d79be65b63864cf3aab1c0ed35a1f497", "date": "2025-03-09_18-28-59", "timestamp": 1741541339, "time_this_iter_s": 0.382737398147583, "time_total_s": 6.1642937660217285, "pid": 48554, "hostname": "julien-PC", "node_ip": "192.168.89.58", "config": {"num_workers": 1, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 50, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 50, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "Centralized_Critic_Model", "custom_model_config": {"env": "mpe", "env_args": {"continuous_actions": false, "max_cycles": 25, "map_name": "simple_world_comm"}, "mask_flag": false, "global_state_flag": false, "opp_action_in_cc": true, "agent_level_batch_update": false, "force_coop": false, "local_mode": false, "share_policy": "group", "evaluation_interval": 50, "framework": "torch", "num_workers": 1, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "checkpoint_freq": 20, "checkpoint_end": true, "restore_path": {"model_path": "", "params_path": ""}, "stop_iters": 9999999, "stop_timesteps": 2000000, "stop_reward": 999999, "seed": 321, "local_dir": "", "model_arch_args": {"hidden_state_size": 256, "core_arch": "mlp", "fc_layer": 2, "out_dim_fc_0": 128, "out_dim_fc_1": 64, "encode_layer": "128-256"}, "algorithm": "mappo", "space_obs": "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "space_act": "Discrete(20)", "num_agents": 6, "episode_limit": 25, "policy_mapping_info": {"simple_adversary": {"description": "one team attack, one team survive", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_crypto": {"description": "two team cooperate, one team attack", "team_prefix": ["eve_", "bob_", "alice_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_push": {"description": "one team target on landmark, one team attack", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_tag": {"description": "one team attack, one team survive", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_spread": {"description": "one team cooperate", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}, "simple_reference": {"description": "one team cooperate", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}, "simple_world_comm": {"description": "two team cooperate and attack, one team survive", "team_prefix": ["adversary_", "leadadversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_speaker_listener": {"description": "two team cooperate", "team_prefix": ["speaker_", "listener_"], "all_agents_one_policy": true, "one_agent_one_policy": true}}, "agent_name_ls": ["leadadversary_0", "adversary_0", "adversary_1", "adversary_2", "agent_0", "agent_1"]}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "mpe_simple_world_comm", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 50, "evaluation_num_episodes": 10, "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 100, "min_iter_time_s": 0, "timesteps_per_iteration": 0, "seed": 321, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_adversary_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}], "policy_leadadversary_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}], "policy_agent_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x7f88afef4af0>", "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "simple_optimizer": false, "monitor": -1, "use_critic": true, "use_gae": true, "lambda": 1.0, "kl_coeff": 0.2, "sgd_minibatch_size": 50, "shuffle_sequences": true, "num_sgd_iter": 2, "lr_schedule": null, "vf_loss_coeff": 1.0, "entropy_coeff": 0.01, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": null, "kl_target": 0.01, "vf_share_layers": -1}, "time_since_restore": 6.1642937660217285, "timesteps_since_restore": 0, "iterations_since_restore": 15, "perf": {"cpu_util_percent": 16.7, "ram_util_percent": 46.4}}
{"episode_reward_max": 98.42627085353728, "episode_reward_min": -155.78308767969756, "episode_reward_mean": -19.188935888086547, "episode_len_mean": 25.0, "episode_media": {}, "episodes_this_iter": 2, "policy_reward_min": {"policy_leadadversary_": -4.388542193694188, "policy_adversary_": -3.9338029974868833, "policy_agent_": -96.35415811344997}, "policy_reward_max": {"policy_leadadversary_": 33.854903602692595, "policy_adversary_": 34.36826010203567, "policy_agent_": 27.837397306389533}, "policy_reward_mean": {"policy_leadadversary_": 1.5202778217427042, "policy_adversary_": 1.8478490747947234, "policy_agent_": -13.12638046710671}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-105.16393252669111, -38.08425587795351, -8.179283354852107, -7.994660195117557, 17.029253709543042, -62.95754389372587, -21.306978110434503, -9.963224993494906, -35.6723275216737, -7.801818117525467, 9.04258457925305, -9.16790291343267, -23.11610041380425, -76.64256918047145, 19.012759487513534, -18.237441060454206, -26.428737443182527, -42.52695238562914, -72.55115007329658, -11.5502931963818, -9.816421651401027, -155.78308767969756, -37.19573589166182, -10.625310490304468, 20.61462187851487, 56.41119583016825, -13.315507414957573, -12.10170378283629, 98.42627085353728, -33.048655056364915, 7.3217210673286806, 7.327237400716829], "episode_lengths": [25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25], "policy_policy_leadadversary__reward": [-2.399035557624381, 8.302786016962129, -1.9111450930988323, -1.90667729707557, -4.038309224748798, -1.1928023284527944, 6.938235712341814, -1.8308626607889866, 8.940230507526055, -2.0965552281416944, 9.03154180074615, -1.2461341680319717, -4.388542193694188, -3.847920286524414, 3.5200136941709386, -2.964544320613795, -2.9630602344890113, -2.5514853414160785, -3.423786335468951, -1.7669051568509833, -1.2819197880175521, -2.793092838548524, -2.715631555274084, -1.888458905121377, 8.491376358690058, 19.490398201983847, -3.414607305270243, -1.9683014538118688, 33.854903602692595, -2.2506002232734774, 2.321446715633231, 2.5983351813573012], "policy_policy_adversary__reward": [-1.6741618220129826, -2.2223381552451884, -1.7978662959280882, 6.195230400528265, 9.085335341319167, 9.161857338996002, -1.1497964664252314, -0.7244895816458103, -2.705072794727737, -1.9452619286739838, -0.5002713058248459, -2.428351018980954, -2.073914420541661, -2.491769450548833, -1.928789335869684, -1.6434950773450425, -1.1869343121273628, -3.094481311936268, 9.22635362970437, 8.437625312282139, 8.558180033954198, -1.474364867288304, -2.699172611779865, -1.9577497509222836, 8.892856901464825, 8.219320737803885, 8.128349998386524, -2.4694792844144473, -0.6928708713293986, -1.5511211993405283, 6.420991744018833, 7.748318394913137, 8.185932035407232, -2.473275836552458, -0.5674346699607982, -2.7263543698194392, -0.5909371354705603, -1.2690244550254388, -1.8793974687070758, -2.8351443039978297, -3.923014083123016, -2.3079879234678007, 4.100268481179938, 3.027702044631114, 4.02297814215707, -2.0777870863086294, -1.9840069073615636, -1.669300530018625, -3.9338029974868833, -1.129238869347308, -1.3580459203598705, -2.250033013063289, -1.3601789265843973, -2.654726573595358, -1.7265988852649663, -3.7396060163471576, -1.820129784531345, -2.9201218301303506, -1.7090159550536768, -2.424621160302172, -2.53712801256559, -2.876435417206376, -1.4196451923447468, -1.5248898471676176, -2.768539034385942, -1.768760251247098, -3.013244773720978, -3.0025095397079458, -1.7121641935407885, -1.6493536919918832, -3.9070691940185345, -1.3224203607070766, 7.256290170226535, 8.009821795532776, 9.511926850083018, 19.35511620124706, 18.84000636214317, 19.519538085643973, -1.4999996116110483, -3.8952631049612574, -2.070937464898907, -2.546883520298444, -2.9137085640161007, -2.9466401471505344, 33.50823641156576, 34.36826010203567, 32.62555861085578, -1.594179646955657, -0.7648866358471844, -2.61833002303639, 3.932860003891398, 4.357704493462412, 3.6291405799201555, 3.6581468826299237, 2.9729168454181787, 4.531212041059542], "policy_policy_agent__reward": [-0.7163725824305123, -96.35415811344997, -3.617249732269532, -67.21221524348954, -1.1897859301531337, -0.49899348880136807, -0.7213000577969194, -0.49279858676528443, 27.837397306389533, -0.27536116513751435, -1.8093468032775135, -54.03048406058688, -42.95203037708829, -11.515342421628725, -1.0331583960521016, -0.9679167066633642, -0.927224282567324, -68.92586138428767, -0.4035124455295982, -0.5882790887698023, -0.3080346699354489, -22.036164725896853, -1.0213367048557638, -1.1333671642122476, -0.6140299114418961, -14.374169249465094, -0.5039952235368317, -63.22450735982157, 9.76231673232199, -5.42051960694752, -0.38719184121053235, -9.154610374941065, -1.2188982803065158, -15.825691141192937, -32.77419517584195, -0.9363333551280546, -61.65847002433208, -0.182559027352075, -1.3441869339869565, -1.3854421600576674, -1.2320086747231598, -0.4692845665436012, -91.95162118944084, -54.97618451890754, -25.81192265544753, -0.9402631739704812, -0.7369098771347249, -1.1210984613308634, -1.2206295166227459, -11.434163779394773, -20.24663057019898, -0.5472324506508123, -0.7910180498339603, -1.6436818783821536, -0.8500054566128976, -0.8761646409464355, -20.63989374553544, -15.290794128077069, -1.1518478368502902, -24.66881069040191, -5.722489091533723, -1.1969416340448025, -5.930741298443036, -0.5026322513050725]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.5662754861157682, "mean_inference_ms": 2.7353685728783725, "mean_action_processing_ms": 0.1060308315498392, "mean_env_wait_ms": 3.3060944777526213, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 1, "timesteps_total": 800, "timesteps_this_iter": 0, "agent_timesteps_total": 4800, "timers": {"sample_time_ms": 435.351, "sample_throughput": 114.85, "load_time_ms": 0.477, "load_throughput": 104836.633, "learn_time_ms": 70.333, "learn_throughput": 710.907, "update_time_ms": 1.813}, "info": {"learner": {"policy_leadadversary_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 6.103515625e-06, "cur_lr": 0.0005, "total_loss": 3.8919198513031006, "policy_loss": -0.0022619592212140827, "vf_loss": 3.9240554571151733, "vf_explained_var": -0.0016257166862487793, "kl": 2.1033124464664432e-05, "entropy": 2.987360715866089, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_adversary_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 6.1035156249999995e-06, "cur_lr": 0.0005, "total_loss": 5.498602708180745, "policy_loss": -0.006374281210203965, "vf_loss": 5.534290234247844, "vf_explained_var": 0.012178579966227213, "kl": 0.00030395268334141673, "entropy": 2.9313188393910727, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_agent_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 6.103515625e-06, "cur_lr": 0.0005, "total_loss": 4.659128427505493, "policy_loss": -0.0044134072959423065, "vf_loss": 4.693227171897888, "vf_explained_var": -0.07427892088890076, "kl": 0.00010266658765489822, "entropy": 2.968528926372528, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}}, "num_steps_sampled": 800, "num_agent_steps_sampled": 4800, "num_steps_trained": 800, "num_agent_steps_trained": 4800, "num_steps_trained_this_iter": 0}, "done": false, "episodes_total": 32, "training_iteration": 16, "trial_id": "f5f5f_00000", "experiment_id": "d79be65b63864cf3aab1c0ed35a1f497", "date": "2025-03-09_18-29-00", "timestamp": 1741541340, "time_this_iter_s": 0.3946659564971924, "time_total_s": 6.558959722518921, "pid": 48554, "hostname": "julien-PC", "node_ip": "192.168.89.58", "config": {"num_workers": 1, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 50, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 50, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "Centralized_Critic_Model", "custom_model_config": {"env": "mpe", "env_args": {"continuous_actions": false, "max_cycles": 25, "map_name": "simple_world_comm"}, "mask_flag": false, "global_state_flag": false, "opp_action_in_cc": true, "agent_level_batch_update": false, "force_coop": false, "local_mode": false, "share_policy": "group", "evaluation_interval": 50, "framework": "torch", "num_workers": 1, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "checkpoint_freq": 20, "checkpoint_end": true, "restore_path": {"model_path": "", "params_path": ""}, "stop_iters": 9999999, "stop_timesteps": 2000000, "stop_reward": 999999, "seed": 321, "local_dir": "", "model_arch_args": {"hidden_state_size": 256, "core_arch": "mlp", "fc_layer": 2, "out_dim_fc_0": 128, "out_dim_fc_1": 64, "encode_layer": "128-256"}, "algorithm": "mappo", "space_obs": "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "space_act": "Discrete(20)", "num_agents": 6, "episode_limit": 25, "policy_mapping_info": {"simple_adversary": {"description": "one team attack, one team survive", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_crypto": {"description": "two team cooperate, one team attack", "team_prefix": ["eve_", "bob_", "alice_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_push": {"description": "one team target on landmark, one team attack", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_tag": {"description": "one team attack, one team survive", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_spread": {"description": "one team cooperate", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}, "simple_reference": {"description": "one team cooperate", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}, "simple_world_comm": {"description": "two team cooperate and attack, one team survive", "team_prefix": ["adversary_", "leadadversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_speaker_listener": {"description": "two team cooperate", "team_prefix": ["speaker_", "listener_"], "all_agents_one_policy": true, "one_agent_one_policy": true}}, "agent_name_ls": ["leadadversary_0", "adversary_0", "adversary_1", "adversary_2", "agent_0", "agent_1"]}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "mpe_simple_world_comm", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 50, "evaluation_num_episodes": 10, "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 100, "min_iter_time_s": 0, "timesteps_per_iteration": 0, "seed": 321, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_adversary_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}], "policy_leadadversary_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}], "policy_agent_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x7f88aff1aee0>", "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "simple_optimizer": false, "monitor": -1, "use_critic": true, "use_gae": true, "lambda": 1.0, "kl_coeff": 0.2, "sgd_minibatch_size": 50, "shuffle_sequences": true, "num_sgd_iter": 2, "lr_schedule": null, "vf_loss_coeff": 1.0, "entropy_coeff": 0.01, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": null, "kl_target": 0.01, "vf_share_layers": -1}, "time_since_restore": 6.558959722518921, "timesteps_since_restore": 0, "iterations_since_restore": 16, "perf": {"cpu_util_percent": 16.1, "ram_util_percent": 46.4}}
{"episode_reward_max": 98.42627085353728, "episode_reward_min": -155.78308767969756, "episode_reward_mean": -18.108289771885197, "episode_len_mean": 25.0, "episode_media": {}, "episodes_this_iter": 2, "policy_reward_min": {"policy_leadadversary_": -4.388542193694188, "policy_adversary_": -4.292088229889296, "policy_agent_": -96.35415811344997}, "policy_reward_max": {"policy_leadadversary_": 33.854903602692595, "policy_adversary_": 34.36826010203567, "policy_agent_": 27.837397306389533}, "policy_reward_mean": {"policy_leadadversary_": 1.6489947569885468, "policy_adversary_": 1.8831911819598095, "policy_agent_": -12.703429037376585}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-105.16393252669111, -38.08425587795351, -8.179283354852107, -7.994660195117557, 17.029253709543042, -62.95754389372587, -21.306978110434503, -9.963224993494906, -35.6723275216737, -7.801818117525467, 9.04258457925305, -9.16790291343267, -23.11610041380425, -76.64256918047145, 19.012759487513534, -18.237441060454206, -26.428737443182527, -42.52695238562914, -72.55115007329658, -11.5502931963818, -9.816421651401027, -155.78308767969756, -37.19573589166182, -10.625310490304468, 20.61462187851487, 56.41119583016825, -13.315507414957573, -12.10170378283629, 98.42627085353728, -33.048655056364915, 7.3217210673286806, 7.327237400716829, -23.088022125278687, 21.452118299951458], "episode_lengths": [25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25], "policy_policy_leadadversary__reward": [-2.399035557624381, 8.302786016962129, -1.9111450930988323, -1.90667729707557, -4.038309224748798, -1.1928023284527944, 6.938235712341814, -1.8308626607889866, 8.940230507526055, -2.0965552281416944, 9.03154180074615, -1.2461341680319717, -4.388542193694188, -3.847920286524414, 3.5200136941709386, -2.964544320613795, -2.9630602344890113, -2.5514853414160785, -3.423786335468951, -1.7669051568509833, -1.2819197880175521, -2.793092838548524, -2.715631555274084, -1.888458905121377, 8.491376358690058, 19.490398201983847, -3.414607305270243, -1.9683014538118688, 33.854903602692595, -2.2506002232734774, 2.321446715633231, 2.5983351813573012, -1.5375524704754444, 8.954483912319503], "policy_policy_adversary__reward": [-1.6741618220129826, -2.2223381552451884, -1.7978662959280882, 6.195230400528265, 9.085335341319167, 9.161857338996002, -1.1497964664252314, -0.7244895816458103, -2.705072794727737, -1.9452619286739838, -0.5002713058248459, -2.428351018980954, -2.073914420541661, -2.491769450548833, -1.928789335869684, -1.6434950773450425, -1.1869343121273628, -3.094481311936268, 9.22635362970437, 8.437625312282139, 8.558180033954198, -1.474364867288304, -2.699172611779865, -1.9577497509222836, 8.892856901464825, 8.219320737803885, 8.128349998386524, -2.4694792844144473, -0.6928708713293986, -1.5511211993405283, 6.420991744018833, 7.748318394913137, 8.185932035407232, -2.473275836552458, -0.5674346699607982, -2.7263543698194392, -0.5909371354705603, -1.2690244550254388, -1.8793974687070758, -2.8351443039978297, -3.923014083123016, -2.3079879234678007, 4.100268481179938, 3.027702044631114, 4.02297814215707, -2.0777870863086294, -1.9840069073615636, -1.669300530018625, -3.9338029974868833, -1.129238869347308, -1.3580459203598705, -2.250033013063289, -1.3601789265843973, -2.654726573595358, -1.7265988852649663, -3.7396060163471576, -1.820129784531345, -2.9201218301303506, -1.7090159550536768, -2.424621160302172, -2.53712801256559, -2.876435417206376, -1.4196451923447468, -1.5248898471676176, -2.768539034385942, -1.768760251247098, -3.013244773720978, -3.0025095397079458, -1.7121641935407885, -1.6493536919918832, -3.9070691940185345, -1.3224203607070766, 7.256290170226535, 8.009821795532776, 9.511926850083018, 19.35511620124706, 18.84000636214317, 19.519538085643973, -1.4999996116110483, -3.8952631049612574, -2.070937464898907, -2.546883520298444, -2.9137085640161007, -2.9466401471505344, 33.50823641156576, 34.36826010203567, 32.62555861085578, -1.594179646955657, -0.7648866358471844, -2.61833002303639, 3.932860003891398, 4.357704493462412, 3.6291405799201555, 3.6581468826299237, 2.9729168454181787, 4.531212041059542, -1.4324294123408556, -3.5804977490861387, -4.292088229889296, 7.364570933829991, 8.207346016991234, 8.425087820102204], "policy_policy_agent__reward": [-0.7163725824305123, -96.35415811344997, -3.617249732269532, -67.21221524348954, -1.1897859301531337, -0.49899348880136807, -0.7213000577969194, -0.49279858676528443, 27.837397306389533, -0.27536116513751435, -1.8093468032775135, -54.03048406058688, -42.95203037708829, -11.515342421628725, -1.0331583960521016, -0.9679167066633642, -0.927224282567324, -68.92586138428767, -0.4035124455295982, -0.5882790887698023, -0.3080346699354489, -22.036164725896853, -1.0213367048557638, -1.1333671642122476, -0.6140299114418961, -14.374169249465094, -0.5039952235368317, -63.22450735982157, 9.76231673232199, -5.42051960694752, -0.38719184121053235, -9.154610374941065, -1.2188982803065158, -15.825691141192937, -32.77419517584195, -0.9363333551280546, -61.65847002433208, -0.182559027352075, -1.3441869339869565, -1.3854421600576674, -1.2320086747231598, -0.4692845665436012, -91.95162118944084, -54.97618451890754, -25.81192265544753, -0.9402631739704812, -0.7369098771347249, -1.1210984613308634, -1.2206295166227459, -11.434163779394773, -20.24663057019898, -0.5472324506508123, -0.7910180498339603, -1.6436818783821536, -0.8500054566128976, -0.8761646409464355, -20.63989374553544, -15.290794128077069, -1.1518478368502902, -24.66881069040191, -5.722489091533723, -1.1969416340448025, -5.930741298443036, -0.5026322513050725, -11.882773789857106, -0.36268047362983874, -1.1761058559306894, -10.323264527360775]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.5642850619887781, "mean_inference_ms": 2.730218681930855, "mean_action_processing_ms": 0.10581109430654634, "mean_env_wait_ms": 3.3034321188709583, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 1, "timesteps_total": 850, "timesteps_this_iter": 0, "agent_timesteps_total": 5100, "timers": {"sample_time_ms": 433.629, "sample_throughput": 115.306, "load_time_ms": 0.477, "load_throughput": 104758.08, "learn_time_ms": 69.36, "learn_throughput": 720.872, "update_time_ms": 1.82}, "info": {"learner": {"policy_leadadversary_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 3.0517578125e-06, "cur_lr": 0.0005, "total_loss": 10.061887264251709, "policy_loss": -0.0029067266266791236, "vf_loss": 10.09465742111206, "vf_explained_var": 0.015646755695343018, "kl": 1.4283955747915189e-05, "entropy": 2.9862643480300903, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_adversary_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 3.0517578124999997e-06, "cur_lr": 0.0005, "total_loss": 9.229440410931906, "policy_loss": -0.003889515995979309, "vf_loss": 9.262567400932312, "vf_explained_var": 0.030276834964752197, "kl": 0.00045996522833346987, "entropy": 2.9237300952275596, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_agent_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 3.0517578125e-06, "cur_lr": 0.0005, "total_loss": 11.796238899230957, "policy_loss": -0.002035185694694519, "vf_loss": 11.827844619750977, "vf_explained_var": 0.029310643672943115, "kl": 0.00011107668150023287, "entropy": 2.9569918513298035, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}}, "num_steps_sampled": 850, "num_agent_steps_sampled": 5100, "num_steps_trained": 850, "num_agent_steps_trained": 5100, "num_steps_trained_this_iter": 0}, "done": false, "episodes_total": 34, "training_iteration": 17, "trial_id": "f5f5f_00000", "experiment_id": "d79be65b63864cf3aab1c0ed35a1f497", "date": "2025-03-09_18-29-00", "timestamp": 1741541340, "time_this_iter_s": 0.38854122161865234, "time_total_s": 6.947500944137573, "pid": 48554, "hostname": "julien-PC", "node_ip": "192.168.89.58", "config": {"num_workers": 1, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 50, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 50, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "Centralized_Critic_Model", "custom_model_config": {"env": "mpe", "env_args": {"continuous_actions": false, "max_cycles": 25, "map_name": "simple_world_comm"}, "mask_flag": false, "global_state_flag": false, "opp_action_in_cc": true, "agent_level_batch_update": false, "force_coop": false, "local_mode": false, "share_policy": "group", "evaluation_interval": 50, "framework": "torch", "num_workers": 1, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "checkpoint_freq": 20, "checkpoint_end": true, "restore_path": {"model_path": "", "params_path": ""}, "stop_iters": 9999999, "stop_timesteps": 2000000, "stop_reward": 999999, "seed": 321, "local_dir": "", "model_arch_args": {"hidden_state_size": 256, "core_arch": "mlp", "fc_layer": 2, "out_dim_fc_0": 128, "out_dim_fc_1": 64, "encode_layer": "128-256"}, "algorithm": "mappo", "space_obs": "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "space_act": "Discrete(20)", "num_agents": 6, "episode_limit": 25, "policy_mapping_info": {"simple_adversary": {"description": "one team attack, one team survive", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_crypto": {"description": "two team cooperate, one team attack", "team_prefix": ["eve_", "bob_", "alice_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_push": {"description": "one team target on landmark, one team attack", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_tag": {"description": "one team attack, one team survive", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_spread": {"description": "one team cooperate", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}, "simple_reference": {"description": "one team cooperate", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}, "simple_world_comm": {"description": "two team cooperate and attack, one team survive", "team_prefix": ["adversary_", "leadadversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_speaker_listener": {"description": "two team cooperate", "team_prefix": ["speaker_", "listener_"], "all_agents_one_policy": true, "one_agent_one_policy": true}}, "agent_name_ls": ["leadadversary_0", "adversary_0", "adversary_1", "adversary_2", "agent_0", "agent_1"]}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "mpe_simple_world_comm", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 50, "evaluation_num_episodes": 10, "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 100, "min_iter_time_s": 0, "timesteps_per_iteration": 0, "seed": 321, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_adversary_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}], "policy_leadadversary_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}], "policy_agent_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x7f88aff1ad30>", "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "simple_optimizer": false, "monitor": -1, "use_critic": true, "use_gae": true, "lambda": 1.0, "kl_coeff": 0.2, "sgd_minibatch_size": 50, "shuffle_sequences": true, "num_sgd_iter": 2, "lr_schedule": null, "vf_loss_coeff": 1.0, "entropy_coeff": 0.01, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": null, "kl_target": 0.01, "vf_share_layers": -1}, "time_since_restore": 6.947500944137573, "timesteps_since_restore": 0, "iterations_since_restore": 17, "perf": {}}
{"episode_reward_max": 98.42627085353728, "episode_reward_min": -155.78308767969756, "episode_reward_mean": -19.082352339618375, "episode_len_mean": 25.0, "episode_media": {}, "episodes_this_iter": 2, "policy_reward_min": {"policy_leadadversary_": -4.542143749578077, "policy_adversary_": -4.292088229889296, "policy_agent_": -96.35415811344997}, "policy_reward_max": {"policy_leadadversary_": 33.854903602692595, "policy_adversary_": 34.36826010203567, "policy_agent_": 27.837397306389533}, "policy_reward_mean": {"policy_leadadversary_": 1.380130875673097, "policy_adversary_": 1.6888737840142893, "policy_agent_": -12.764552283667168}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-105.16393252669111, -38.08425587795351, -8.179283354852107, -7.994660195117557, 17.029253709543042, -62.95754389372587, -21.306978110434503, -9.963224993494906, -35.6723275216737, -7.801818117525467, 9.04258457925305, -9.16790291343267, -23.11610041380425, -76.64256918047145, 19.012759487513534, -18.237441060454206, -26.428737443182527, -42.52695238562914, -72.55115007329658, -11.5502931963818, -9.816421651401027, -155.78308767969756, -37.19573589166182, -10.625310490304468, 20.61462187851487, 56.41119583016825, -13.315507414957573, -12.10170378283629, 98.42627085353728, -33.048655056364915, 7.3217210673286806, 7.327237400716829, -23.088022125278687, 21.452118299951458, -34.22571107500617, -37.05712090715862], "episode_lengths": [25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25], "policy_policy_leadadversary__reward": [-2.399035557624381, 8.302786016962129, -1.9111450930988323, -1.90667729707557, -4.038309224748798, -1.1928023284527944, 6.938235712341814, -1.8308626607889866, 8.940230507526055, -2.0965552281416944, 9.03154180074615, -1.2461341680319717, -4.388542193694188, -3.847920286524414, 3.5200136941709386, -2.964544320613795, -2.9630602344890113, -2.5514853414160785, -3.423786335468951, -1.7669051568509833, -1.2819197880175521, -2.793092838548524, -2.715631555274084, -1.888458905121377, 8.491376358690058, 19.490398201983847, -3.414607305270243, -1.9683014538118688, 33.854903602692595, -2.2506002232734774, 2.321446715633231, 2.5983351813573012, -1.5375524704754444, 8.954483912319503, -1.838966463801028, -4.542143749578077], "policy_policy_adversary__reward": [-1.6741618220129826, -2.2223381552451884, -1.7978662959280882, 6.195230400528265, 9.085335341319167, 9.161857338996002, -1.1497964664252314, -0.7244895816458103, -2.705072794727737, -1.9452619286739838, -0.5002713058248459, -2.428351018980954, -2.073914420541661, -2.491769450548833, -1.928789335869684, -1.6434950773450425, -1.1869343121273628, -3.094481311936268, 9.22635362970437, 8.437625312282139, 8.558180033954198, -1.474364867288304, -2.699172611779865, -1.9577497509222836, 8.892856901464825, 8.219320737803885, 8.128349998386524, -2.4694792844144473, -0.6928708713293986, -1.5511211993405283, 6.420991744018833, 7.748318394913137, 8.185932035407232, -2.473275836552458, -0.5674346699607982, -2.7263543698194392, -0.5909371354705603, -1.2690244550254388, -1.8793974687070758, -2.8351443039978297, -3.923014083123016, -2.3079879234678007, 4.100268481179938, 3.027702044631114, 4.02297814215707, -2.0777870863086294, -1.9840069073615636, -1.669300530018625, -3.9338029974868833, -1.129238869347308, -1.3580459203598705, -2.250033013063289, -1.3601789265843973, -2.654726573595358, -1.7265988852649663, -3.7396060163471576, -1.820129784531345, -2.9201218301303506, -1.7090159550536768, -2.424621160302172, -2.53712801256559, -2.876435417206376, -1.4196451923447468, -1.5248898471676176, -2.768539034385942, -1.768760251247098, -3.013244773720978, -3.0025095397079458, -1.7121641935407885, -1.6493536919918832, -3.9070691940185345, -1.3224203607070766, 7.256290170226535, 8.009821795532776, 9.511926850083018, 19.35511620124706, 18.84000636214317, 19.519538085643973, -1.4999996116110483, -3.8952631049612574, -2.070937464898907, -2.546883520298444, -2.9137085640161007, -2.9466401471505344, 33.50823641156576, 34.36826010203567, 32.62555861085578, -1.594179646955657, -0.7648866358471844, -2.61833002303639, 3.932860003891398, 4.357704493462412, 3.6291405799201555, 3.6581468826299237, 2.9729168454181787, 4.531212041059542, -1.4324294123408556, -3.5804977490861387, -4.292088229889296, 7.364570933829991, 8.207346016991234, 8.425087820102204, -1.4899061486169511, -0.888369164140079, -2.1729673605733337, -0.9880730925424175, -0.8979613232266417, -3.2498547972579335], "policy_policy_agent__reward": [-0.7163725824305123, -96.35415811344997, -3.617249732269532, -67.21221524348954, -1.1897859301531337, -0.49899348880136807, -0.7213000577969194, -0.49279858676528443, 27.837397306389533, -0.27536116513751435, -1.8093468032775135, -54.03048406058688, -42.95203037708829, -11.515342421628725, -1.0331583960521016, -0.9679167066633642, -0.927224282567324, -68.92586138428767, -0.4035124455295982, -0.5882790887698023, -0.3080346699354489, -22.036164725896853, -1.0213367048557638, -1.1333671642122476, -0.6140299114418961, -14.374169249465094, -0.5039952235368317, -63.22450735982157, 9.76231673232199, -5.42051960694752, -0.38719184121053235, -9.154610374941065, -1.2188982803065158, -15.825691141192937, -32.77419517584195, -0.9363333551280546, -61.65847002433208, -0.182559027352075, -1.3441869339869565, -1.3854421600576674, -1.2320086747231598, -0.4692845665436012, -91.95162118944084, -54.97618451890754, -25.81192265544753, -0.9402631739704812, -0.7369098771347249, -1.1210984613308634, -1.2206295166227459, -11.434163779394773, -20.24663057019898, -0.5472324506508123, -0.7910180498339603, -1.6436818783821536, -0.8500054566128976, -0.8761646409464355, -20.63989374553544, -15.290794128077069, -1.1518478368502902, -24.66881069040191, -5.722489091533723, -1.1969416340448025, -5.930741298443036, -0.5026322513050725, -11.882773789857106, -0.36268047362983874, -1.1761058559306894, -10.323264527360775, -27.075190531297906, -0.7603114065768609, -0.6153024430104596, -26.76378550154309]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.5623944493101521, "mean_inference_ms": 2.725049920733684, "mean_action_processing_ms": 0.10559845647643666, "mean_env_wait_ms": 3.3008788394515642, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 1, "timesteps_total": 900, "timesteps_this_iter": 0, "agent_timesteps_total": 5400, "timers": {"sample_time_ms": 430.991, "sample_throughput": 116.012, "load_time_ms": 0.475, "load_throughput": 105194.222, "learn_time_ms": 69.44, "learn_throughput": 720.043, "update_time_ms": 1.812}, "info": {"learner": {"policy_leadadversary_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 1.52587890625e-06, "cur_lr": 0.0005, "total_loss": 3.352946400642395, "policy_loss": -0.0020221126545223456, "vf_loss": 3.3846452236175537, "vf_explained_var": 0.04155343770980835, "kl": 3.1343781252912706e-05, "entropy": 2.9676501750946045, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_adversary_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 1.5258789062499999e-06, "cur_lr": 0.0005, "total_loss": 1.8852740327517192, "policy_loss": -0.004943275203307469, "vf_loss": 1.9197866916656494, "vf_explained_var": -0.0749307672182719, "kl": 0.00020936277294506903, "entropy": 2.9569350481033325, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_agent_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 1.52587890625e-06, "cur_lr": 0.0005, "total_loss": 107.37012100219727, "policy_loss": -0.004616595804691315, "vf_loss": 107.4042329788208, "vf_explained_var": 0.004619896411895752, "kl": 0.00017183436178241918, "entropy": 2.949575662612915, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}}, "num_steps_sampled": 900, "num_agent_steps_sampled": 5400, "num_steps_trained": 900, "num_agent_steps_trained": 5400, "num_steps_trained_this_iter": 0}, "done": false, "episodes_total": 36, "training_iteration": 18, "trial_id": "f5f5f_00000", "experiment_id": "d79be65b63864cf3aab1c0ed35a1f497", "date": "2025-03-09_18-29-01", "timestamp": 1741541341, "time_this_iter_s": 0.3912181854248047, "time_total_s": 7.338719129562378, "pid": 48554, "hostname": "julien-PC", "node_ip": "192.168.89.58", "config": {"num_workers": 1, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 50, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 50, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "Centralized_Critic_Model", "custom_model_config": {"env": "mpe", "env_args": {"continuous_actions": false, "max_cycles": 25, "map_name": "simple_world_comm"}, "mask_flag": false, "global_state_flag": false, "opp_action_in_cc": true, "agent_level_batch_update": false, "force_coop": false, "local_mode": false, "share_policy": "group", "evaluation_interval": 50, "framework": "torch", "num_workers": 1, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "checkpoint_freq": 20, "checkpoint_end": true, "restore_path": {"model_path": "", "params_path": ""}, "stop_iters": 9999999, "stop_timesteps": 2000000, "stop_reward": 999999, "seed": 321, "local_dir": "", "model_arch_args": {"hidden_state_size": 256, "core_arch": "mlp", "fc_layer": 2, "out_dim_fc_0": 128, "out_dim_fc_1": 64, "encode_layer": "128-256"}, "algorithm": "mappo", "space_obs": "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "space_act": "Discrete(20)", "num_agents": 6, "episode_limit": 25, "policy_mapping_info": {"simple_adversary": {"description": "one team attack, one team survive", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_crypto": {"description": "two team cooperate, one team attack", "team_prefix": ["eve_", "bob_", "alice_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_push": {"description": "one team target on landmark, one team attack", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_tag": {"description": "one team attack, one team survive", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_spread": {"description": "one team cooperate", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}, "simple_reference": {"description": "one team cooperate", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}, "simple_world_comm": {"description": "two team cooperate and attack, one team survive", "team_prefix": ["adversary_", "leadadversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_speaker_listener": {"description": "two team cooperate", "team_prefix": ["speaker_", "listener_"], "all_agents_one_policy": true, "one_agent_one_policy": true}}, "agent_name_ls": ["leadadversary_0", "adversary_0", "adversary_1", "adversary_2", "agent_0", "agent_1"]}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "mpe_simple_world_comm", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 50, "evaluation_num_episodes": 10, "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 100, "min_iter_time_s": 0, "timesteps_per_iteration": 0, "seed": 321, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_adversary_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}], "policy_leadadversary_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}], "policy_agent_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x7f88aff00310>", "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "simple_optimizer": false, "monitor": -1, "use_critic": true, "use_gae": true, "lambda": 1.0, "kl_coeff": 0.2, "sgd_minibatch_size": 50, "shuffle_sequences": true, "num_sgd_iter": 2, "lr_schedule": null, "vf_loss_coeff": 1.0, "entropy_coeff": 0.01, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": null, "kl_target": 0.01, "vf_share_layers": -1}, "time_since_restore": 7.338719129562378, "timesteps_since_restore": 0, "iterations_since_restore": 18, "perf": {"cpu_util_percent": 16.1, "ram_util_percent": 46.4}}
{"episode_reward_max": 98.42627085353728, "episode_reward_min": -155.78308767969756, "episode_reward_mean": -18.086680065700417, "episode_len_mean": 25.0, "episode_media": {}, "episodes_this_iter": 2, "policy_reward_min": {"policy_leadadversary_": -4.542143749578077, "policy_adversary_": -4.292088229889296, "policy_agent_": -96.35415811344997}, "policy_reward_max": {"policy_leadadversary_": 33.854903602692595, "policy_adversary_": 34.36826010203567, "policy_agent_": 27.837397306389533}, "policy_reward_mean": {"policy_leadadversary_": 1.2060709858061873, "policy_adversary_": 1.52851764623702, "policy_agent_": -11.93915199510883}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-105.16393252669111, -38.08425587795351, -8.179283354852107, -7.994660195117557, 17.029253709543042, -62.95754389372587, -21.306978110434503, -9.963224993494906, -35.6723275216737, -7.801818117525467, 9.04258457925305, -9.16790291343267, -23.11610041380425, -76.64256918047145, 19.012759487513534, -18.237441060454206, -26.428737443182527, -42.52695238562914, -72.55115007329658, -11.5502931963818, -9.816421651401027, -155.78308767969756, -37.19573589166182, -10.625310490304468, 20.61462187851487, 56.41119583016825, -13.315507414957573, -12.10170378283629, 98.42627085353728, -33.048655056364915, 7.3217210673286806, 7.327237400716829, -23.088022125278687, 21.452118299951458, -34.22571107500617, -37.05712090715862, -7.758585683990852, 7.429427413636523], "episode_lengths": [25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25], "policy_policy_leadadversary__reward": [-2.399035557624381, 8.302786016962129, -1.9111450930988323, -1.90667729707557, -4.038309224748798, -1.1928023284527944, 6.938235712341814, -1.8308626607889866, 8.940230507526055, -2.0965552281416944, 9.03154180074615, -1.2461341680319717, -4.388542193694188, -3.847920286524414, 3.5200136941709386, -2.964544320613795, -2.9630602344890113, -2.5514853414160785, -3.423786335468951, -1.7669051568509833, -1.2819197880175521, -2.793092838548524, -2.715631555274084, -1.888458905121377, 8.491376358690058, 19.490398201983847, -3.414607305270243, -1.9683014538118688, 33.854903602692595, -2.2506002232734774, 2.321446715633231, 2.5983351813573012, -1.5375524704754444, 8.954483912319503, -1.838966463801028, -4.542143749578077, -1.887097192016523, -1.9669168715798406], "policy_policy_adversary__reward": [-1.6741618220129826, -2.2223381552451884, -1.7978662959280882, 6.195230400528265, 9.085335341319167, 9.161857338996002, -1.1497964664252314, -0.7244895816458103, -2.705072794727737, -1.9452619286739838, -0.5002713058248459, -2.428351018980954, -2.073914420541661, -2.491769450548833, -1.928789335869684, -1.6434950773450425, -1.1869343121273628, -3.094481311936268, 9.22635362970437, 8.437625312282139, 8.558180033954198, -1.474364867288304, -2.699172611779865, -1.9577497509222836, 8.892856901464825, 8.219320737803885, 8.128349998386524, -2.4694792844144473, -0.6928708713293986, -1.5511211993405283, 6.420991744018833, 7.748318394913137, 8.185932035407232, -2.473275836552458, -0.5674346699607982, -2.7263543698194392, -0.5909371354705603, -1.2690244550254388, -1.8793974687070758, -2.8351443039978297, -3.923014083123016, -2.3079879234678007, 4.100268481179938, 3.027702044631114, 4.02297814215707, -2.0777870863086294, -1.9840069073615636, -1.669300530018625, -3.9338029974868833, -1.129238869347308, -1.3580459203598705, -2.250033013063289, -1.3601789265843973, -2.654726573595358, -1.7265988852649663, -3.7396060163471576, -1.820129784531345, -2.9201218301303506, -1.7090159550536768, -2.424621160302172, -2.53712801256559, -2.876435417206376, -1.4196451923447468, -1.5248898471676176, -2.768539034385942, -1.768760251247098, -3.013244773720978, -3.0025095397079458, -1.7121641935407885, -1.6493536919918832, -3.9070691940185345, -1.3224203607070766, 7.256290170226535, 8.009821795532776, 9.511926850083018, 19.35511620124706, 18.84000636214317, 19.519538085643973, -1.4999996116110483, -3.8952631049612574, -2.070937464898907, -2.546883520298444, -2.9137085640161007, -2.9466401471505344, 33.50823641156576, 34.36826010203567, 32.62555861085578, -1.594179646955657, -0.7648866358471844, -2.61833002303639, 3.932860003891398, 4.357704493462412, 3.6291405799201555, 3.6581468826299237, 2.9729168454181787, 4.531212041059542, -1.4324294123408556, -3.5804977490861387, -4.292088229889296, 7.364570933829991, 8.207346016991234, 8.425087820102204, -1.4899061486169511, -0.888369164140079, -2.1729673605733337, -0.9880730925424175, -0.8979613232266417, -3.2498547972579335, -1.4440590307296355, -0.9465905128076592, -1.6162837096317748, -1.481425948122367, -1.476298099855236, -1.1826997013762748], "policy_policy_agent__reward": [-0.7163725824305123, -96.35415811344997, -3.617249732269532, -67.21221524348954, -1.1897859301531337, -0.49899348880136807, -0.7213000577969194, -0.49279858676528443, 27.837397306389533, -0.27536116513751435, -1.8093468032775135, -54.03048406058688, -42.95203037708829, -11.515342421628725, -1.0331583960521016, -0.9679167066633642, -0.927224282567324, -68.92586138428767, -0.4035124455295982, -0.5882790887698023, -0.3080346699354489, -22.036164725896853, -1.0213367048557638, -1.1333671642122476, -0.6140299114418961, -14.374169249465094, -0.5039952235368317, -63.22450735982157, 9.76231673232199, -5.42051960694752, -0.38719184121053235, -9.154610374941065, -1.2188982803065158, -15.825691141192937, -32.77419517584195, -0.9363333551280546, -61.65847002433208, -0.182559027352075, -1.3441869339869565, -1.3854421600576674, -1.2320086747231598, -0.4692845665436012, -91.95162118944084, -54.97618451890754, -25.81192265544753, -0.9402631739704812, -0.7369098771347249, -1.1210984613308634, -1.2206295166227459, -11.434163779394773, -20.24663057019898, -0.5472324506508123, -0.7910180498339603, -1.6436818783821536, -0.8500054566128976, -0.8761646409464355, -20.63989374553544, -15.290794128077069, -1.1518478368502902, -24.66881069040191, -5.722489091533723, -1.1969416340448025, -5.930741298443036, -0.5026322513050725, -11.882773789857106, -0.36268047362983874, -1.1761058559306894, -10.323264527360775, -27.075190531297906, -0.7603114065768609, -0.6153024430104596, -26.76378550154309, -0.8350650603929229, -1.029490178412334, -4.33587304506435, 17.872641079634594]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.560650803457767, "mean_inference_ms": 2.7204085489536105, "mean_action_processing_ms": 0.10540230987160817, "mean_env_wait_ms": 3.2986043594164367, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 1, "timesteps_total": 950, "timesteps_this_iter": 0, "agent_timesteps_total": 5700, "timers": {"sample_time_ms": 433.181, "sample_throughput": 115.425, "load_time_ms": 0.475, "load_throughput": 105342.174, "learn_time_ms": 69.47, "learn_throughput": 719.739, "update_time_ms": 1.807}, "info": {"learner": {"policy_leadadversary_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 7.62939453125e-07, "cur_lr": 0.0005, "total_loss": 1.1808590292930603, "policy_loss": -0.001089174770750212, "vf_loss": 1.2116912603378296, "vf_explained_var": -0.02807241678237915, "kl": 1.8697755073904254e-05, "entropy": 2.974298119544983, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_adversary_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 7.629394531249999e-07, "cur_lr": 0.0005, "total_loss": 1.3542156716187794, "policy_loss": -0.0027662403881549835, "vf_loss": 1.3865180611610413, "vf_explained_var": -0.45886747042338055, "kl": 0.0001911548523774916, "entropy": 2.9536176522572837, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_agent_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 7.62939453125e-07, "cur_lr": 0.0005, "total_loss": 51.101581767201424, "policy_loss": -0.0036170408129692078, "vf_loss": 51.1348257958889, "vf_explained_var": -0.1275433897972107, "kl": 0.00012709728696447264, "entropy": 2.9623921513557434, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}}, "num_steps_sampled": 950, "num_agent_steps_sampled": 5700, "num_steps_trained": 950, "num_agent_steps_trained": 5700, "num_steps_trained_this_iter": 0}, "done": false, "episodes_total": 38, "training_iteration": 19, "trial_id": "f5f5f_00000", "experiment_id": "d79be65b63864cf3aab1c0ed35a1f497", "date": "2025-03-09_18-29-01", "timestamp": 1741541341, "time_this_iter_s": 0.4042656421661377, "time_total_s": 7.742984771728516, "pid": 48554, "hostname": "julien-PC", "node_ip": "192.168.89.58", "config": {"num_workers": 1, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 50, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 50, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "Centralized_Critic_Model", "custom_model_config": {"env": "mpe", "env_args": {"continuous_actions": false, "max_cycles": 25, "map_name": "simple_world_comm"}, "mask_flag": false, "global_state_flag": false, "opp_action_in_cc": true, "agent_level_batch_update": false, "force_coop": false, "local_mode": false, "share_policy": "group", "evaluation_interval": 50, "framework": "torch", "num_workers": 1, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "checkpoint_freq": 20, "checkpoint_end": true, "restore_path": {"model_path": "", "params_path": ""}, "stop_iters": 9999999, "stop_timesteps": 2000000, "stop_reward": 999999, "seed": 321, "local_dir": "", "model_arch_args": {"hidden_state_size": 256, "core_arch": "mlp", "fc_layer": 2, "out_dim_fc_0": 128, "out_dim_fc_1": 64, "encode_layer": "128-256"}, "algorithm": "mappo", "space_obs": "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "space_act": "Discrete(20)", "num_agents": 6, "episode_limit": 25, "policy_mapping_info": {"simple_adversary": {"description": "one team attack, one team survive", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_crypto": {"description": "two team cooperate, one team attack", "team_prefix": ["eve_", "bob_", "alice_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_push": {"description": "one team target on landmark, one team attack", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_tag": {"description": "one team attack, one team survive", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_spread": {"description": "one team cooperate", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}, "simple_reference": {"description": "one team cooperate", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}, "simple_world_comm": {"description": "two team cooperate and attack, one team survive", "team_prefix": ["adversary_", "leadadversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_speaker_listener": {"description": "two team cooperate", "team_prefix": ["speaker_", "listener_"], "all_agents_one_policy": true, "one_agent_one_policy": true}}, "agent_name_ls": ["leadadversary_0", "adversary_0", "adversary_1", "adversary_2", "agent_0", "agent_1"]}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "mpe_simple_world_comm", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 50, "evaluation_num_episodes": 10, "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 100, "min_iter_time_s": 0, "timesteps_per_iteration": 0, "seed": 321, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_adversary_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}], "policy_leadadversary_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}], "policy_agent_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x7f88afdd70d0>", "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "simple_optimizer": false, "monitor": -1, "use_critic": true, "use_gae": true, "lambda": 1.0, "kl_coeff": 0.2, "sgd_minibatch_size": 50, "shuffle_sequences": true, "num_sgd_iter": 2, "lr_schedule": null, "vf_loss_coeff": 1.0, "entropy_coeff": 0.01, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": null, "kl_target": 0.01, "vf_share_layers": -1}, "time_since_restore": 7.742984771728516, "timesteps_since_restore": 0, "iterations_since_restore": 19, "perf": {}}
{"episode_reward_max": 98.42627085353728, "episode_reward_min": -155.78308767969756, "episode_reward_mean": -20.616583245444765, "episode_len_mean": 25.0, "episode_media": {}, "episodes_this_iter": 2, "policy_reward_min": {"policy_leadadversary_": -4.542143749578077, "policy_adversary_": -4.292088229889296, "policy_agent_": -96.35415811344997}, "policy_reward_max": {"policy_leadadversary_": 33.854903602692595, "policy_adversary_": 34.36826010203567, "policy_agent_": 27.837397306389533}, "policy_reward_mean": {"policy_leadadversary_": 0.9538234743455402, "policy_adversary_": 1.3646883190137529, "policy_agent_": -12.83223583841578}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-105.16393252669111, -38.08425587795351, -8.179283354852107, -7.994660195117557, 17.029253709543042, -62.95754389372587, -21.306978110434503, -9.963224993494906, -35.6723275216737, -7.801818117525467, 9.04258457925305, -9.16790291343267, -23.11610041380425, -76.64256918047145, 19.012759487513534, -18.237441060454206, -26.428737443182527, -42.52695238562914, -72.55115007329658, -11.5502931963818, -9.816421651401027, -155.78308767969756, -37.19573589166182, -10.625310490304468, 20.61462187851487, 56.41119583016825, -13.315507414957573, -12.10170378283629, 98.42627085353728, -33.048655056364915, 7.3217210673286806, 7.327237400716829, -23.088022125278687, 21.452118299951458, -34.22571107500617, -37.05712090715862, -7.758585683990852, 7.429427413636523, -64.55081708876132, -72.81867023241337], "episode_lengths": [25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25], "policy_policy_leadadversary__reward": [-2.399035557624381, 8.302786016962129, -1.9111450930988323, -1.90667729707557, -4.038309224748798, -1.1928023284527944, 6.938235712341814, -1.8308626607889866, 8.940230507526055, -2.0965552281416944, 9.03154180074615, -1.2461341680319717, -4.388542193694188, -3.847920286524414, 3.5200136941709386, -2.964544320613795, -2.9630602344890113, -2.5514853414160785, -3.423786335468951, -1.7669051568509833, -1.2819197880175521, -2.793092838548524, -2.715631555274084, -1.888458905121377, 8.491376358690058, 19.490398201983847, -3.414607305270243, -1.9683014538118688, 33.854903602692595, -2.2506002232734774, 2.321446715633231, 2.5983351813573012, -1.5375524704754444, 8.954483912319503, -1.838966463801028, -4.542143749578077, -1.887097192016523, -1.9669168715798406, -4.259228446259269, -3.418530040554258], "policy_policy_adversary__reward": [-1.6741618220129826, -2.2223381552451884, -1.7978662959280882, 6.195230400528265, 9.085335341319167, 9.161857338996002, -1.1497964664252314, -0.7244895816458103, -2.705072794727737, -1.9452619286739838, -0.5002713058248459, -2.428351018980954, -2.073914420541661, -2.491769450548833, -1.928789335869684, -1.6434950773450425, -1.1869343121273628, -3.094481311936268, 9.22635362970437, 8.437625312282139, 8.558180033954198, -1.474364867288304, -2.699172611779865, -1.9577497509222836, 8.892856901464825, 8.219320737803885, 8.128349998386524, -2.4694792844144473, -0.6928708713293986, -1.5511211993405283, 6.420991744018833, 7.748318394913137, 8.185932035407232, -2.473275836552458, -0.5674346699607982, -2.7263543698194392, -0.5909371354705603, -1.2690244550254388, -1.8793974687070758, -2.8351443039978297, -3.923014083123016, -2.3079879234678007, 4.100268481179938, 3.027702044631114, 4.02297814215707, -2.0777870863086294, -1.9840069073615636, -1.669300530018625, -3.9338029974868833, -1.129238869347308, -1.3580459203598705, -2.250033013063289, -1.3601789265843973, -2.654726573595358, -1.7265988852649663, -3.7396060163471576, -1.820129784531345, -2.9201218301303506, -1.7090159550536768, -2.424621160302172, -2.53712801256559, -2.876435417206376, -1.4196451923447468, -1.5248898471676176, -2.768539034385942, -1.768760251247098, -3.013244773720978, -3.0025095397079458, -1.7121641935407885, -1.6493536919918832, -3.9070691940185345, -1.3224203607070766, 7.256290170226535, 8.009821795532776, 9.511926850083018, 19.35511620124706, 18.84000636214317, 19.519538085643973, -1.4999996116110483, -3.8952631049612574, -2.070937464898907, -2.546883520298444, -2.9137085640161007, -2.9466401471505344, 33.50823641156576, 34.36826010203567, 32.62555861085578, -1.594179646955657, -0.7648866358471844, -2.61833002303639, 3.932860003891398, 4.357704493462412, 3.6291405799201555, 3.6581468826299237, 2.9729168454181787, 4.531212041059542, -1.4324294123408556, -3.5804977490861387, -4.292088229889296, 7.364570933829991, 8.207346016991234, 8.425087820102204, -1.4899061486169511, -0.888369164140079, -2.1729673605733337, -0.9880730925424175, -0.8979613232266417, -3.2498547972579335, -1.4440590307296355, -0.9465905128076592, -1.6162837096317748, -1.481425948122367, -1.476298099855236, -1.1826997013762748, -3.7611817019384506, -0.6523852938800511, -1.0971635214558346, -1.0180042767264177, -1.6408081573998603, -2.3188704379693], "policy_policy_agent__reward": [-0.7163725824305123, -96.35415811344997, -3.617249732269532, -67.21221524348954, -1.1897859301531337, -0.49899348880136807, -0.7213000577969194, -0.49279858676528443, 27.837397306389533, -0.27536116513751435, -1.8093468032775135, -54.03048406058688, -42.95203037708829, -11.515342421628725, -1.0331583960521016, -0.9679167066633642, -0.927224282567324, -68.92586138428767, -0.4035124455295982, -0.5882790887698023, -0.3080346699354489, -22.036164725896853, -1.0213367048557638, -1.1333671642122476, -0.6140299114418961, -14.374169249465094, -0.5039952235368317, -63.22450735982157, 9.76231673232199, -5.42051960694752, -0.38719184121053235, -9.154610374941065, -1.2188982803065158, -15.825691141192937, -32.77419517584195, -0.9363333551280546, -61.65847002433208, -0.182559027352075, -1.3441869339869565, -1.3854421600576674, -1.2320086747231598, -0.4692845665436012, -91.95162118944084, -54.97618451890754, -25.81192265544753, -0.9402631739704812, -0.7369098771347249, -1.1210984613308634, -1.2206295166227459, -11.434163779394773, -20.24663057019898, -0.5472324506508123, -0.7910180498339603, -1.6436818783821536, -0.8500054566128976, -0.8761646409464355, -20.63989374553544, -15.290794128077069, -1.1518478368502902, -24.66881069040191, -5.722489091533723, -1.1969416340448025, -5.930741298443036, -0.5026322513050725, -11.882773789857106, -0.36268047362983874, -1.1761058559306894, -10.323264527360775, -27.075190531297906, -0.7603114065768609, -0.6153024430104596, -26.76378550154309, -0.8350650603929229, -1.029490178412334, -4.33587304506435, 17.872641079634594, -13.677754358552278, -41.103103766675446, 2.9893260231465204, -67.41178334291008]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.5591340891681323, "mean_inference_ms": 2.7169205675640526, "mean_action_processing_ms": 0.10526052799661864, "mean_env_wait_ms": 3.297235431763574, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 1, "timesteps_total": 1000, "timesteps_this_iter": 0, "agent_timesteps_total": 6000, "timers": {"sample_time_ms": 432.385, "sample_throughput": 115.638, "load_time_ms": 0.456, "load_throughput": 109741.078, "learn_time_ms": 68.809, "learn_throughput": 726.647, "update_time_ms": 1.806}, "info": {"learner": {"policy_leadadversary_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 3.814697265625e-07, "cur_lr": 0.0005, "total_loss": 4.356520175933838, "policy_loss": -0.002716701133176791, "vf_loss": 4.389031410217285, "vf_explained_var": -0.0610196590423584, "kl": 2.1291481422158753e-05, "entropy": 2.9794464111328125, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_adversary_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 3.8146972656249997e-07, "cur_lr": 0.0005, "total_loss": 2.0251336892445884, "policy_loss": -0.007986067483822504, "vf_loss": 2.0624888141949973, "vf_explained_var": 0.14916045467058817, "kl": 0.0003052585495257887, "entropy": 2.9369062185287476, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_agent_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 3.814697265625e-07, "cur_lr": 0.0005, "total_loss": 473.21958923339844, "policy_loss": -0.005257882177829742, "vf_loss": 473.2543182373047, "vf_explained_var": 0.0023125261068344116, "kl": 0.0002024814239825895, "entropy": 2.9467825889587402, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}}, "num_steps_sampled": 1000, "num_agent_steps_sampled": 6000, "num_steps_trained": 1000, "num_agent_steps_trained": 6000, "num_steps_trained_this_iter": 0}, "done": false, "episodes_total": 40, "training_iteration": 20, "trial_id": "f5f5f_00000", "experiment_id": "d79be65b63864cf3aab1c0ed35a1f497", "date": "2025-03-09_18-29-01", "timestamp": 1741541341, "time_this_iter_s": 0.44835972785949707, "time_total_s": 8.191344499588013, "pid": 48554, "hostname": "julien-PC", "node_ip": "192.168.89.58", "config": {"num_workers": 1, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 50, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 50, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "Centralized_Critic_Model", "custom_model_config": {"env": "mpe", "env_args": {"continuous_actions": false, "max_cycles": 25, "map_name": "simple_world_comm"}, "mask_flag": false, "global_state_flag": false, "opp_action_in_cc": true, "agent_level_batch_update": false, "force_coop": false, "local_mode": false, "share_policy": "group", "evaluation_interval": 50, "framework": "torch", "num_workers": 1, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "checkpoint_freq": 20, "checkpoint_end": true, "restore_path": {"model_path": "", "params_path": ""}, "stop_iters": 9999999, "stop_timesteps": 2000000, "stop_reward": 999999, "seed": 321, "local_dir": "", "model_arch_args": {"hidden_state_size": 256, "core_arch": "mlp", "fc_layer": 2, "out_dim_fc_0": 128, "out_dim_fc_1": 64, "encode_layer": "128-256"}, "algorithm": "mappo", "space_obs": "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "space_act": "Discrete(20)", "num_agents": 6, "episode_limit": 25, "policy_mapping_info": {"simple_adversary": {"description": "one team attack, one team survive", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_crypto": {"description": "two team cooperate, one team attack", "team_prefix": ["eve_", "bob_", "alice_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_push": {"description": "one team target on landmark, one team attack", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_tag": {"description": "one team attack, one team survive", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_spread": {"description": "one team cooperate", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}, "simple_reference": {"description": "one team cooperate", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}, "simple_world_comm": {"description": "two team cooperate and attack, one team survive", "team_prefix": ["adversary_", "leadadversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_speaker_listener": {"description": "two team cooperate", "team_prefix": ["speaker_", "listener_"], "all_agents_one_policy": true, "one_agent_one_policy": true}}, "agent_name_ls": ["leadadversary_0", "adversary_0", "adversary_1", "adversary_2", "agent_0", "agent_1"]}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "mpe_simple_world_comm", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 50, "evaluation_num_episodes": 10, "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 100, "min_iter_time_s": 0, "timesteps_per_iteration": 0, "seed": 321, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_adversary_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}], "policy_leadadversary_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}], "policy_agent_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x7f88afef4dc0>", "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "simple_optimizer": false, "monitor": -1, "use_critic": true, "use_gae": true, "lambda": 1.0, "kl_coeff": 0.2, "sgd_minibatch_size": 50, "shuffle_sequences": true, "num_sgd_iter": 2, "lr_schedule": null, "vf_loss_coeff": 1.0, "entropy_coeff": 0.01, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": null, "kl_target": 0.01, "vf_share_layers": -1}, "time_since_restore": 8.191344499588013, "timesteps_since_restore": 0, "iterations_since_restore": 20, "perf": {"cpu_util_percent": 20.9, "ram_util_percent": 46.5}}
{"episode_reward_max": 98.42627085353728, "episode_reward_min": -155.78308767969756, "episode_reward_mean": -20.940769694402935, "episode_len_mean": 25.0, "episode_media": {}, "episodes_this_iter": 2, "policy_reward_min": {"policy_leadadversary_": -4.542143749578077, "policy_adversary_": -4.292088229889296, "policy_agent_": -96.35415811344997}, "policy_reward_max": {"policy_leadadversary_": 33.854903602692595, "policy_adversary_": 34.36826010203567, "policy_agent_": 27.837397306389533}, "policy_reward_mean": {"policy_leadadversary_": 0.950740581137284, "policy_adversary_": 1.2975083791298847, "policy_agent_": -12.892017706464937}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-105.16393252669111, -38.08425587795351, -8.179283354852107, -7.994660195117557, 17.029253709543042, -62.95754389372587, -21.306978110434503, -9.963224993494906, -35.6723275216737, -7.801818117525467, 9.04258457925305, -9.16790291343267, -23.11610041380425, -76.64256918047145, 19.012759487513534, -18.237441060454206, -26.428737443182527, -42.52695238562914, -72.55115007329658, -11.5502931963818, -9.816421651401027, -155.78308767969756, -37.19573589166182, -10.625310490304468, 20.61462187851487, 56.41119583016825, -13.315507414957573, -12.10170378283629, 98.42627085353728, -33.048655056364915, 7.3217210673286806, 7.327237400716829, -23.088022125278687, 21.452118299951458, -34.22571107500617, -37.05712090715862, -7.758585683990852, 7.429427413636523, -64.55081708876132, -72.81867023241337, -57.907825000034876, 3.058827652902239], "episode_lengths": [25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25], "policy_policy_leadadversary__reward": [-2.399035557624381, 8.302786016962129, -1.9111450930988323, -1.90667729707557, -4.038309224748798, -1.1928023284527944, 6.938235712341814, -1.8308626607889866, 8.940230507526055, -2.0965552281416944, 9.03154180074615, -1.2461341680319717, -4.388542193694188, -3.847920286524414, 3.5200136941709386, -2.964544320613795, -2.9630602344890113, -2.5514853414160785, -3.423786335468951, -1.7669051568509833, -1.2819197880175521, -2.793092838548524, -2.715631555274084, -1.888458905121377, 8.491376358690058, 19.490398201983847, -3.414607305270243, -1.9683014538118688, 33.854903602692595, -2.2506002232734774, 2.321446715633231, 2.5983351813573012, -1.5375524704754444, 8.954483912319503, -1.838966463801028, -4.542143749578077, -1.887097192016523, -1.9669168715798406, -4.259228446259269, -3.418530040554258, -1.6405950365018764, 3.4187604704462027], "policy_policy_adversary__reward": [-1.6741618220129826, -2.2223381552451884, -1.7978662959280882, 6.195230400528265, 9.085335341319167, 9.161857338996002, -1.1497964664252314, -0.7244895816458103, -2.705072794727737, -1.9452619286739838, -0.5002713058248459, -2.428351018980954, -2.073914420541661, -2.491769450548833, -1.928789335869684, -1.6434950773450425, -1.1869343121273628, -3.094481311936268, 9.22635362970437, 8.437625312282139, 8.558180033954198, -1.474364867288304, -2.699172611779865, -1.9577497509222836, 8.892856901464825, 8.219320737803885, 8.128349998386524, -2.4694792844144473, -0.6928708713293986, -1.5511211993405283, 6.420991744018833, 7.748318394913137, 8.185932035407232, -2.473275836552458, -0.5674346699607982, -2.7263543698194392, -0.5909371354705603, -1.2690244550254388, -1.8793974687070758, -2.8351443039978297, -3.923014083123016, -2.3079879234678007, 4.100268481179938, 3.027702044631114, 4.02297814215707, -2.0777870863086294, -1.9840069073615636, -1.669300530018625, -3.9338029974868833, -1.129238869347308, -1.3580459203598705, -2.250033013063289, -1.3601789265843973, -2.654726573595358, -1.7265988852649663, -3.7396060163471576, -1.820129784531345, -2.9201218301303506, -1.7090159550536768, -2.424621160302172, -2.53712801256559, -2.876435417206376, -1.4196451923447468, -1.5248898471676176, -2.768539034385942, -1.768760251247098, -3.013244773720978, -3.0025095397079458, -1.7121641935407885, -1.6493536919918832, -3.9070691940185345, -1.3224203607070766, 7.256290170226535, 8.009821795532776, 9.511926850083018, 19.35511620124706, 18.84000636214317, 19.519538085643973, -1.4999996116110483, -3.8952631049612574, -2.070937464898907, -2.546883520298444, -2.9137085640161007, -2.9466401471505344, 33.50823641156576, 34.36826010203567, 32.62555861085578, -1.594179646955657, -0.7648866358471844, -2.61833002303639, 3.932860003891398, 4.357704493462412, 3.6291405799201555, 3.6581468826299237, 2.9729168454181787, 4.531212041059542, -1.4324294123408556, -3.5804977490861387, -4.292088229889296, 7.364570933829991, 8.207346016991234, 8.425087820102204, -1.4899061486169511, -0.888369164140079, -2.1729673605733337, -0.9880730925424175, -0.8979613232266417, -3.2498547972579335, -1.4440590307296355, -0.9465905128076592, -1.6162837096317748, -1.481425948122367, -1.476298099855236, -1.1826997013762748, -3.7611817019384506, -0.6523852938800511, -1.0971635214558346, -1.0180042767264177, -1.6408081573998603, -2.3188704379693, -1.7534750970959454, -2.0578000943715407, -3.478190868351961, 2.197108121194587, 4.007253584005375, 0.8085618433346088], "policy_policy_agent__reward": [-0.7163725824305123, -96.35415811344997, -3.617249732269532, -67.21221524348954, -1.1897859301531337, -0.49899348880136807, -0.7213000577969194, -0.49279858676528443, 27.837397306389533, -0.27536116513751435, -1.8093468032775135, -54.03048406058688, -42.95203037708829, -11.515342421628725, -1.0331583960521016, -0.9679167066633642, -0.927224282567324, -68.92586138428767, -0.4035124455295982, -0.5882790887698023, -0.3080346699354489, -22.036164725896853, -1.0213367048557638, -1.1333671642122476, -0.6140299114418961, -14.374169249465094, -0.5039952235368317, -63.22450735982157, 9.76231673232199, -5.42051960694752, -0.38719184121053235, -9.154610374941065, -1.2188982803065158, -15.825691141192937, -32.77419517584195, -0.9363333551280546, -61.65847002433208, -0.182559027352075, -1.3441869339869565, -1.3854421600576674, -1.2320086747231598, -0.4692845665436012, -91.95162118944084, -54.97618451890754, -25.81192265544753, -0.9402631739704812, -0.7369098771347249, -1.1210984613308634, -1.2206295166227459, -11.434163779394773, -20.24663057019898, -0.5472324506508123, -0.7910180498339603, -1.6436818783821536, -0.8500054566128976, -0.8761646409464355, -20.63989374553544, -15.290794128077069, -1.1518478368502902, -24.66881069040191, -5.722489091533723, -1.1969416340448025, -5.930741298443036, -0.5026322513050725, -11.882773789857106, -0.36268047362983874, -1.1761058559306894, -10.323264527360775, -27.075190531297906, -0.7603114065768609, -0.6153024430104596, -26.76378550154309, -0.8350650603929229, -1.029490178412334, -4.33587304506435, 17.872641079634594, -13.677754358552278, -41.103103766675446, 2.9893260231465204, -67.41178334291008, -48.50968190580077, -0.4680819979127372, -5.722255746040154, -1.6506006200383874]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.558147041625708, "mean_inference_ms": 2.7162535990131316, "mean_action_processing_ms": 0.10521601281464099, "mean_env_wait_ms": 3.298065778329385, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 1, "timesteps_total": 1050, "timesteps_this_iter": 0, "agent_timesteps_total": 6300, "timers": {"sample_time_ms": 444.848, "sample_throughput": 112.398, "load_time_ms": 0.422, "load_throughput": 118382.839, "learn_time_ms": 68.426, "learn_throughput": 730.714, "update_time_ms": 1.841}, "info": {"learner": {"policy_leadadversary_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 1.9073486328125e-07, "cur_lr": 0.0005, "total_loss": 7.209531307220459, "policy_loss": -0.0019759553112086436, "vf_loss": 7.24119234085083, "vf_explained_var": -0.096771240234375, "kl": 1.419591041042223e-05, "entropy": 2.968549966812134, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_adversary_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 1.9073486328124998e-07, "cur_lr": 0.0005, "total_loss": 6.344310025374095, "policy_loss": -0.006934848924477895, "vf_loss": 6.380472819010417, "vf_explained_var": -0.07117842634518941, "kl": 0.0005552599181779527, "entropy": 2.922829786936442, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_agent_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 1.9073486328125e-07, "cur_lr": 0.0005, "total_loss": 260.4011116027832, "policy_loss": -0.004792660474777222, "vf_loss": 260.43544816970825, "vf_explained_var": -0.010602056980133057, "kl": 0.00011242400915589812, "entropy": 2.95248144865036, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}}, "num_steps_sampled": 1050, "num_agent_steps_sampled": 6300, "num_steps_trained": 1050, "num_agent_steps_trained": 6300, "num_steps_trained_this_iter": 0}, "done": false, "episodes_total": 42, "training_iteration": 21, "trial_id": "f5f5f_00000", "experiment_id": "d79be65b63864cf3aab1c0ed35a1f497", "date": "2025-03-09_18-29-02", "timestamp": 1741541342, "time_this_iter_s": 0.5224483013153076, "time_total_s": 8.71379280090332, "pid": 48554, "hostname": "julien-PC", "node_ip": "192.168.89.58", "config": {"num_workers": 1, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 50, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 50, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "Centralized_Critic_Model", "custom_model_config": {"env": "mpe", "env_args": {"continuous_actions": false, "max_cycles": 25, "map_name": "simple_world_comm"}, "mask_flag": false, "global_state_flag": false, "opp_action_in_cc": true, "agent_level_batch_update": false, "force_coop": false, "local_mode": false, "share_policy": "group", "evaluation_interval": 50, "framework": "torch", "num_workers": 1, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "checkpoint_freq": 20, "checkpoint_end": true, "restore_path": {"model_path": "", "params_path": ""}, "stop_iters": 9999999, "stop_timesteps": 2000000, "stop_reward": 999999, "seed": 321, "local_dir": "", "model_arch_args": {"hidden_state_size": 256, "core_arch": "mlp", "fc_layer": 2, "out_dim_fc_0": 128, "out_dim_fc_1": 64, "encode_layer": "128-256"}, "algorithm": "mappo", "space_obs": "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "space_act": "Discrete(20)", "num_agents": 6, "episode_limit": 25, "policy_mapping_info": {"simple_adversary": {"description": "one team attack, one team survive", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_crypto": {"description": "two team cooperate, one team attack", "team_prefix": ["eve_", "bob_", "alice_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_push": {"description": "one team target on landmark, one team attack", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_tag": {"description": "one team attack, one team survive", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_spread": {"description": "one team cooperate", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}, "simple_reference": {"description": "one team cooperate", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}, "simple_world_comm": {"description": "two team cooperate and attack, one team survive", "team_prefix": ["adversary_", "leadadversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_speaker_listener": {"description": "two team cooperate", "team_prefix": ["speaker_", "listener_"], "all_agents_one_policy": true, "one_agent_one_policy": true}}, "agent_name_ls": ["leadadversary_0", "adversary_0", "adversary_1", "adversary_2", "agent_0", "agent_1"]}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "mpe_simple_world_comm", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 50, "evaluation_num_episodes": 10, "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 100, "min_iter_time_s": 0, "timesteps_per_iteration": 0, "seed": 321, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_adversary_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}], "policy_leadadversary_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}], "policy_agent_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x7f88afdd7820>", "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "simple_optimizer": false, "monitor": -1, "use_critic": true, "use_gae": true, "lambda": 1.0, "kl_coeff": 0.2, "sgd_minibatch_size": 50, "shuffle_sequences": true, "num_sgd_iter": 2, "lr_schedule": null, "vf_loss_coeff": 1.0, "entropy_coeff": 0.01, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": null, "kl_target": 0.01, "vf_share_layers": -1}, "time_since_restore": 8.71379280090332, "timesteps_since_restore": 0, "iterations_since_restore": 21, "perf": {"cpu_util_percent": 38.2, "ram_util_percent": 46.5}}
{"episode_reward_max": 98.42627085353728, "episode_reward_min": -155.78308767969756, "episode_reward_mean": -20.07158139847163, "episode_len_mean": 25.0, "episode_media": {}, "episodes_this_iter": 2, "policy_reward_min": {"policy_leadadversary_": -4.542143749578077, "policy_adversary_": -4.292088229889296, "policy_agent_": -96.35415811344997}, "policy_reward_max": {"policy_leadadversary_": 33.854903602692595, "policy_adversary_": 34.36826010203567, "policy_agent_": 27.837397306389533}, "policy_reward_mean": {"policy_leadadversary_": 0.9279322356608296, "policy_adversary_": 1.2701998691962801, "policy_agent_": -12.405056620860648}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-105.16393252669111, -38.08425587795351, -8.179283354852107, -7.994660195117557, 17.029253709543042, -62.95754389372587, -21.306978110434503, -9.963224993494906, -35.6723275216737, -7.801818117525467, 9.04258457925305, -9.16790291343267, -23.11610041380425, -76.64256918047145, 19.012759487513534, -18.237441060454206, -26.428737443182527, -42.52695238562914, -72.55115007329658, -11.5502931963818, -9.816421651401027, -155.78308767969756, -37.19573589166182, -10.625310490304468, 20.61462187851487, 56.41119583016825, -13.315507414957573, -12.10170378283629, 98.42627085353728, -33.048655056364915, 7.3217210673286806, 7.327237400716829, -23.088022125278687, 21.452118299951458, -34.22571107500617, -37.05712090715862, -7.758585683990852, 7.429427413636523, -64.55081708876132, -72.81867023241337, -57.907825000034876, 3.058827652902239, -9.394727393314803, 5.757473025486409], "episode_lengths": [25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25], "policy_policy_leadadversary__reward": [-2.399035557624381, 8.302786016962129, -1.9111450930988323, -1.90667729707557, -4.038309224748798, -1.1928023284527944, 6.938235712341814, -1.8308626607889866, 8.940230507526055, -2.0965552281416944, 9.03154180074615, -1.2461341680319717, -4.388542193694188, -3.847920286524414, 3.5200136941709386, -2.964544320613795, -2.9630602344890113, -2.5514853414160785, -3.423786335468951, -1.7669051568509833, -1.2819197880175521, -2.793092838548524, -2.715631555274084, -1.888458905121377, 8.491376358690058, 19.490398201983847, -3.414607305270243, -1.9683014538118688, 33.854903602692595, -2.2506002232734774, 2.321446715633231, 2.5983351813573012, -1.5375524704754444, 8.954483912319503, -1.838966463801028, -4.542143749578077, -1.887097192016523, -1.9669168715798406, -4.259228446259269, -3.418530040554258, -1.6405950365018764, 3.4187604704462027, -2.0529470784989825, 2.950861039809551], "policy_policy_adversary__reward": [-1.6741618220129826, -2.2223381552451884, -1.7978662959280882, 6.195230400528265, 9.085335341319167, 9.161857338996002, -1.1497964664252314, -0.7244895816458103, -2.705072794727737, -1.9452619286739838, -0.5002713058248459, -2.428351018980954, -2.073914420541661, -2.491769450548833, -1.928789335869684, -1.6434950773450425, -1.1869343121273628, -3.094481311936268, 9.22635362970437, 8.437625312282139, 8.558180033954198, -1.474364867288304, -2.699172611779865, -1.9577497509222836, 8.892856901464825, 8.219320737803885, 8.128349998386524, -2.4694792844144473, -0.6928708713293986, -1.5511211993405283, 6.420991744018833, 7.748318394913137, 8.185932035407232, -2.473275836552458, -0.5674346699607982, -2.7263543698194392, -0.5909371354705603, -1.2690244550254388, -1.8793974687070758, -2.8351443039978297, -3.923014083123016, -2.3079879234678007, 4.100268481179938, 3.027702044631114, 4.02297814215707, -2.0777870863086294, -1.9840069073615636, -1.669300530018625, -3.9338029974868833, -1.129238869347308, -1.3580459203598705, -2.250033013063289, -1.3601789265843973, -2.654726573595358, -1.7265988852649663, -3.7396060163471576, -1.820129784531345, -2.9201218301303506, -1.7090159550536768, -2.424621160302172, -2.53712801256559, -2.876435417206376, -1.4196451923447468, -1.5248898471676176, -2.768539034385942, -1.768760251247098, -3.013244773720978, -3.0025095397079458, -1.7121641935407885, -1.6493536919918832, -3.9070691940185345, -1.3224203607070766, 7.256290170226535, 8.009821795532776, 9.511926850083018, 19.35511620124706, 18.84000636214317, 19.519538085643973, -1.4999996116110483, -3.8952631049612574, -2.070937464898907, -2.546883520298444, -2.9137085640161007, -2.9466401471505344, 33.50823641156576, 34.36826010203567, 32.62555861085578, -1.594179646955657, -0.7648866358471844, -2.61833002303639, 3.932860003891398, 4.357704493462412, 3.6291405799201555, 3.6581468826299237, 2.9729168454181787, 4.531212041059542, -1.4324294123408556, -3.5804977490861387, -4.292088229889296, 7.364570933829991, 8.207346016991234, 8.425087820102204, -1.4899061486169511, -0.888369164140079, -2.1729673605733337, -0.9880730925424175, -0.8979613232266417, -3.2498547972579335, -1.4440590307296355, -0.9465905128076592, -1.6162837096317748, -1.481425948122367, -1.476298099855236, -1.1826997013762748, -3.7611817019384506, -0.6523852938800511, -1.0971635214558346, -1.0180042767264177, -1.6408081573998603, -2.3188704379693, -1.7534750970959454, -2.0578000943715407, -3.478190868351961, 2.197108121194587, 4.007253584005375, 0.8085618433346088, -2.7249530991388955, -0.7346005542861018, -2.6911144456741787, 4.318448499420746, 3.402533710413069, 2.6100128528088105], "policy_policy_agent__reward": [-0.7163725824305123, -96.35415811344997, -3.617249732269532, -67.21221524348954, -1.1897859301531337, -0.49899348880136807, -0.7213000577969194, -0.49279858676528443, 27.837397306389533, -0.27536116513751435, -1.8093468032775135, -54.03048406058688, -42.95203037708829, -11.515342421628725, -1.0331583960521016, -0.9679167066633642, -0.927224282567324, -68.92586138428767, -0.4035124455295982, -0.5882790887698023, -0.3080346699354489, -22.036164725896853, -1.0213367048557638, -1.1333671642122476, -0.6140299114418961, -14.374169249465094, -0.5039952235368317, -63.22450735982157, 9.76231673232199, -5.42051960694752, -0.38719184121053235, -9.154610374941065, -1.2188982803065158, -15.825691141192937, -32.77419517584195, -0.9363333551280546, -61.65847002433208, -0.182559027352075, -1.3441869339869565, -1.3854421600576674, -1.2320086747231598, -0.4692845665436012, -91.95162118944084, -54.97618451890754, -25.81192265544753, -0.9402631739704812, -0.7369098771347249, -1.1210984613308634, -1.2206295166227459, -11.434163779394773, -20.24663057019898, -0.5472324506508123, -0.7910180498339603, -1.6436818783821536, -0.8500054566128976, -0.8761646409464355, -20.63989374553544, -15.290794128077069, -1.1518478368502902, -24.66881069040191, -5.722489091533723, -1.1969416340448025, -5.930741298443036, -0.5026322513050725, -11.882773789857106, -0.36268047362983874, -1.1761058559306894, -10.323264527360775, -27.075190531297906, -0.7603114065768609, -0.6153024430104596, -26.76378550154309, -0.8350650603929229, -1.029490178412334, -4.33587304506435, 17.872641079634594, -13.677754358552278, -41.103103766675446, 2.9893260231465204, -67.41178334291008, -48.50968190580077, -0.4680819979127372, -5.722255746040154, -1.6506006200383874, -0.5229746771561022, -0.6681375385605484, -1.2405475750286472, -6.283835501937115]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.557308272774621, "mean_inference_ms": 2.71610717895833, "mean_action_processing_ms": 0.10518704424171768, "mean_env_wait_ms": 3.299061870074675, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 1, "timesteps_total": 1100, "timesteps_this_iter": 0, "agent_timesteps_total": 6600, "timers": {"sample_time_ms": 447.519, "sample_throughput": 111.727, "load_time_ms": 0.426, "load_throughput": 117349.449, "learn_time_ms": 68.856, "learn_throughput": 726.158, "update_time_ms": 1.854}, "info": {"learner": {"policy_leadadversary_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 9.5367431640625e-08, "cur_lr": 0.0005, "total_loss": 2.4727684259414673, "policy_loss": -0.0021964620612560104, "vf_loss": 2.5046896934509277, "vf_explained_var": -0.09323924779891968, "kl": 1.815903729163182e-05, "entropy": 2.972501039505005, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_adversary_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 9.536743164062499e-08, "cur_lr": 0.0005, "total_loss": 3.4037115573883057, "policy_loss": -0.00983071765707185, "vf_loss": 3.4430678884188333, "vf_explained_var": -0.11025857925415039, "kl": 0.0006009023672287247, "entropy": 2.9525810877482095, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_agent_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 9.5367431640625e-08, "cur_lr": 0.0005, "total_loss": 3.7320850491523743, "policy_loss": -0.004890479147434235, "vf_loss": 3.7666423320770264, "vf_explained_var": -0.31079038977622986, "kl": 0.00016221233381319422, "entropy": 2.966671943664551, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}}, "num_steps_sampled": 1100, "num_agent_steps_sampled": 6600, "num_steps_trained": 1100, "num_agent_steps_trained": 6600, "num_steps_trained_this_iter": 0}, "done": false, "episodes_total": 44, "training_iteration": 22, "trial_id": "f5f5f_00000", "experiment_id": "d79be65b63864cf3aab1c0ed35a1f497", "date": "2025-03-09_18-29-02", "timestamp": 1741541342, "time_this_iter_s": 0.4315500259399414, "time_total_s": 9.145342826843262, "pid": 48554, "hostname": "julien-PC", "node_ip": "192.168.89.58", "config": {"num_workers": 1, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 50, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 50, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "Centralized_Critic_Model", "custom_model_config": {"env": "mpe", "env_args": {"continuous_actions": false, "max_cycles": 25, "map_name": "simple_world_comm"}, "mask_flag": false, "global_state_flag": false, "opp_action_in_cc": true, "agent_level_batch_update": false, "force_coop": false, "local_mode": false, "share_policy": "group", "evaluation_interval": 50, "framework": "torch", "num_workers": 1, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "checkpoint_freq": 20, "checkpoint_end": true, "restore_path": {"model_path": "", "params_path": ""}, "stop_iters": 9999999, "stop_timesteps": 2000000, "stop_reward": 999999, "seed": 321, "local_dir": "", "model_arch_args": {"hidden_state_size": 256, "core_arch": "mlp", "fc_layer": 2, "out_dim_fc_0": 128, "out_dim_fc_1": 64, "encode_layer": "128-256"}, "algorithm": "mappo", "space_obs": "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "space_act": "Discrete(20)", "num_agents": 6, "episode_limit": 25, "policy_mapping_info": {"simple_adversary": {"description": "one team attack, one team survive", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_crypto": {"description": "two team cooperate, one team attack", "team_prefix": ["eve_", "bob_", "alice_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_push": {"description": "one team target on landmark, one team attack", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_tag": {"description": "one team attack, one team survive", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_spread": {"description": "one team cooperate", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}, "simple_reference": {"description": "one team cooperate", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}, "simple_world_comm": {"description": "two team cooperate and attack, one team survive", "team_prefix": ["adversary_", "leadadversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_speaker_listener": {"description": "two team cooperate", "team_prefix": ["speaker_", "listener_"], "all_agents_one_policy": true, "one_agent_one_policy": true}}, "agent_name_ls": ["leadadversary_0", "adversary_0", "adversary_1", "adversary_2", "agent_0", "agent_1"]}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "mpe_simple_world_comm", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 50, "evaluation_num_episodes": 10, "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 100, "min_iter_time_s": 0, "timesteps_per_iteration": 0, "seed": 321, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_adversary_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}], "policy_leadadversary_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}], "policy_agent_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x7f88aff35310>", "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "simple_optimizer": false, "monitor": -1, "use_critic": true, "use_gae": true, "lambda": 1.0, "kl_coeff": 0.2, "sgd_minibatch_size": 50, "shuffle_sequences": true, "num_sgd_iter": 2, "lr_schedule": null, "vf_loss_coeff": 1.0, "entropy_coeff": 0.01, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": null, "kl_target": 0.01, "vf_share_layers": -1}, "time_since_restore": 9.145342826843262, "timesteps_since_restore": 0, "iterations_since_restore": 22, "perf": {}}
{"episode_reward_max": 98.42627085353728, "episode_reward_min": -155.78308767969756, "episode_reward_mean": -20.85252228983555, "episode_len_mean": 25.0, "episode_media": {}, "episodes_this_iter": 2, "policy_reward_min": {"policy_leadadversary_": -4.542143749578077, "policy_adversary_": -4.621733620085473, "policy_agent_": -96.35415811344997}, "policy_reward_max": {"policy_leadadversary_": 33.854903602692595, "policy_adversary_": 34.36826010203567, "policy_agent_": 27.837397306389533}, "policy_reward_mean": {"policy_leadadversary_": 0.8263859998712443, "policy_adversary_": 1.1097984943412353, "policy_agent_": -12.504151886365252}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-105.16393252669111, -38.08425587795351, -8.179283354852107, -7.994660195117557, 17.029253709543042, -62.95754389372587, -21.306978110434503, -9.963224993494906, -35.6723275216737, -7.801818117525467, 9.04258457925305, -9.16790291343267, -23.11610041380425, -76.64256918047145, 19.012759487513534, -18.237441060454206, -26.428737443182527, -42.52695238562914, -72.55115007329658, -11.5502931963818, -9.816421651401027, -155.78308767969756, -37.19573589166182, -10.625310490304468, 20.61462187851487, 56.41119583016825, -13.315507414957573, -12.10170378283629, 98.42627085353728, -33.048655056364915, 7.3217210673286806, 7.327237400716829, -23.088022125278687, 21.452118299951458, -34.22571107500617, -37.05712090715862, -7.758585683990852, 7.429427413636523, -64.55081708876132, -72.81867023241337, -57.907825000034876, 3.058827652902239, -9.394727393314803, 5.757473025486409, -37.32645149477665, -38.73999230490706], "episode_lengths": [25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25], "policy_policy_leadadversary__reward": [-2.399035557624381, 8.302786016962129, -1.9111450930988323, -1.90667729707557, -4.038309224748798, -1.1928023284527944, 6.938235712341814, -1.8308626607889866, 8.940230507526055, -2.0965552281416944, 9.03154180074615, -1.2461341680319717, -4.388542193694188, -3.847920286524414, 3.5200136941709386, -2.964544320613795, -2.9630602344890113, -2.5514853414160785, -3.423786335468951, -1.7669051568509833, -1.2819197880175521, -2.793092838548524, -2.715631555274084, -1.888458905121377, 8.491376358690058, 19.490398201983847, -3.414607305270243, -1.9683014538118688, 33.854903602692595, -2.2506002232734774, 2.321446715633231, 2.5983351813573012, -1.5375524704754444, 8.954483912319503, -1.838966463801028, -4.542143749578077, -1.887097192016523, -1.9669168715798406, -4.259228446259269, -3.418530040554258, -1.6405950365018764, 3.4187604704462027, -2.0529470784989825, 2.950861039809551, -1.6417408125125332, -1.173521562486733], "policy_policy_adversary__reward": [-1.6741618220129826, -2.2223381552451884, -1.7978662959280882, 6.195230400528265, 9.085335341319167, 9.161857338996002, -1.1497964664252314, -0.7244895816458103, -2.705072794727737, -1.9452619286739838, -0.5002713058248459, -2.428351018980954, -2.073914420541661, -2.491769450548833, -1.928789335869684, -1.6434950773450425, -1.1869343121273628, -3.094481311936268, 9.22635362970437, 8.437625312282139, 8.558180033954198, -1.474364867288304, -2.699172611779865, -1.9577497509222836, 8.892856901464825, 8.219320737803885, 8.128349998386524, -2.4694792844144473, -0.6928708713293986, -1.5511211993405283, 6.420991744018833, 7.748318394913137, 8.185932035407232, -2.473275836552458, -0.5674346699607982, -2.7263543698194392, -0.5909371354705603, -1.2690244550254388, -1.8793974687070758, -2.8351443039978297, -3.923014083123016, -2.3079879234678007, 4.100268481179938, 3.027702044631114, 4.02297814215707, -2.0777870863086294, -1.9840069073615636, -1.669300530018625, -3.9338029974868833, -1.129238869347308, -1.3580459203598705, -2.250033013063289, -1.3601789265843973, -2.654726573595358, -1.7265988852649663, -3.7396060163471576, -1.820129784531345, -2.9201218301303506, -1.7090159550536768, -2.424621160302172, -2.53712801256559, -2.876435417206376, -1.4196451923447468, -1.5248898471676176, -2.768539034385942, -1.768760251247098, -3.013244773720978, -3.0025095397079458, -1.7121641935407885, -1.6493536919918832, -3.9070691940185345, -1.3224203607070766, 7.256290170226535, 8.009821795532776, 9.511926850083018, 19.35511620124706, 18.84000636214317, 19.519538085643973, -1.4999996116110483, -3.8952631049612574, -2.070937464898907, -2.546883520298444, -2.9137085640161007, -2.9466401471505344, 33.50823641156576, 34.36826010203567, 32.62555861085578, -1.594179646955657, -0.7648866358471844, -2.61833002303639, 3.932860003891398, 4.357704493462412, 3.6291405799201555, 3.6581468826299237, 2.9729168454181787, 4.531212041059542, -1.4324294123408556, -3.5804977490861387, -4.292088229889296, 7.364570933829991, 8.207346016991234, 8.425087820102204, -1.4899061486169511, -0.888369164140079, -2.1729673605733337, -0.9880730925424175, -0.8979613232266417, -3.2498547972579335, -1.4440590307296355, -0.9465905128076592, -1.6162837096317748, -1.481425948122367, -1.476298099855236, -1.1826997013762748, -3.7611817019384506, -0.6523852938800511, -1.0971635214558346, -1.0180042767264177, -1.6408081573998603, -2.3188704379693, -1.7534750970959454, -2.0578000943715407, -3.478190868351961, 2.197108121194587, 4.007253584005375, 0.8085618433346088, -2.7249530991388955, -0.7346005542861018, -2.6911144456741787, 4.318448499420746, 3.402533710413069, 2.6100128528088105, -4.621733620085473, -1.5859334919515493, -3.592891398226516, -1.1648055450308883, -1.7780974600334736, -1.7707289994905449], "policy_policy_agent__reward": [-0.7163725824305123, -96.35415811344997, -3.617249732269532, -67.21221524348954, -1.1897859301531337, -0.49899348880136807, -0.7213000577969194, -0.49279858676528443, 27.837397306389533, -0.27536116513751435, -1.8093468032775135, -54.03048406058688, -42.95203037708829, -11.515342421628725, -1.0331583960521016, -0.9679167066633642, -0.927224282567324, -68.92586138428767, -0.4035124455295982, -0.5882790887698023, -0.3080346699354489, -22.036164725896853, -1.0213367048557638, -1.1333671642122476, -0.6140299114418961, -14.374169249465094, -0.5039952235368317, -63.22450735982157, 9.76231673232199, -5.42051960694752, -0.38719184121053235, -9.154610374941065, -1.2188982803065158, -15.825691141192937, -32.77419517584195, -0.9363333551280546, -61.65847002433208, -0.182559027352075, -1.3441869339869565, -1.3854421600576674, -1.2320086747231598, -0.4692845665436012, -91.95162118944084, -54.97618451890754, -25.81192265544753, -0.9402631739704812, -0.7369098771347249, -1.1210984613308634, -1.2206295166227459, -11.434163779394773, -20.24663057019898, -0.5472324506508123, -0.7910180498339603, -1.6436818783821536, -0.8500054566128976, -0.8761646409464355, -20.63989374553544, -15.290794128077069, -1.1518478368502902, -24.66881069040191, -5.722489091533723, -1.1969416340448025, -5.930741298443036, -0.5026322513050725, -11.882773789857106, -0.36268047362983874, -1.1761058559306894, -10.323264527360775, -27.075190531297906, -0.7603114065768609, -0.6153024430104596, -26.76378550154309, -0.8350650603929229, -1.029490178412334, -4.33587304506435, 17.872641079634594, -13.677754358552278, -41.103103766675446, 2.9893260231465204, -67.41178334291008, -48.50968190580077, -0.4680819979127372, -5.722255746040154, -1.6506006200383874, -0.5229746771561022, -0.6681375385605484, -1.2405475750286472, -6.283835501937115, -0.7738420813283601, -25.11031009067222, -32.51991083703785, -0.3329279008275307]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.5566277275896871, "mean_inference_ms": 2.7166592957484537, "mean_action_processing_ms": 0.10517803279833578, "mean_env_wait_ms": 3.300540510833208, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 1, "timesteps_total": 1150, "timesteps_this_iter": 0, "agent_timesteps_total": 6900, "timers": {"sample_time_ms": 454.477, "sample_throughput": 110.017, "load_time_ms": 0.424, "load_throughput": 117857.255, "learn_time_ms": 69.312, "learn_throughput": 721.373, "update_time_ms": 1.851}, "info": {"learner": {"policy_leadadversary_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 4.76837158203125e-08, "cur_lr": 0.0005, "total_loss": 0.5860121548175812, "policy_loss": -0.001471009282395741, "vf_loss": 0.6172787249088287, "vf_explained_var": -0.10669034719467163, "kl": 9.556715282244e-06, "entropy": 2.9795641899108887, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_adversary_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 4.7683715820312496e-08, "cur_lr": 0.0005, "total_loss": 2.048030138015747, "policy_loss": -0.010714787912244597, "vf_loss": 2.087647020816803, "vf_explained_var": -0.0008669296900431315, "kl": 0.001001979317039788, "entropy": 2.8902196486790976, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_agent_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 4.76837158203125e-08, "cur_lr": 0.0005, "total_loss": 114.8812427520752, "policy_loss": -0.003384530544281006, "vf_loss": 114.91370964050293, "vf_explained_var": -0.0024504363536834717, "kl": 0.00011428092897425302, "entropy": 2.9077488780021667, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}}, "num_steps_sampled": 1150, "num_agent_steps_sampled": 6900, "num_steps_trained": 1150, "num_agent_steps_trained": 6900, "num_steps_trained_this_iter": 0}, "done": false, "episodes_total": 46, "training_iteration": 23, "trial_id": "f5f5f_00000", "experiment_id": "d79be65b63864cf3aab1c0ed35a1f497", "date": "2025-03-09_18-29-03", "timestamp": 1741541343, "time_this_iter_s": 0.45076608657836914, "time_total_s": 9.59610891342163, "pid": 48554, "hostname": "julien-PC", "node_ip": "192.168.89.58", "config": {"num_workers": 1, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 50, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 50, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "Centralized_Critic_Model", "custom_model_config": {"env": "mpe", "env_args": {"continuous_actions": false, "max_cycles": 25, "map_name": "simple_world_comm"}, "mask_flag": false, "global_state_flag": false, "opp_action_in_cc": true, "agent_level_batch_update": false, "force_coop": false, "local_mode": false, "share_policy": "group", "evaluation_interval": 50, "framework": "torch", "num_workers": 1, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "checkpoint_freq": 20, "checkpoint_end": true, "restore_path": {"model_path": "", "params_path": ""}, "stop_iters": 9999999, "stop_timesteps": 2000000, "stop_reward": 999999, "seed": 321, "local_dir": "", "model_arch_args": {"hidden_state_size": 256, "core_arch": "mlp", "fc_layer": 2, "out_dim_fc_0": 128, "out_dim_fc_1": 64, "encode_layer": "128-256"}, "algorithm": "mappo", "space_obs": "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "space_act": "Discrete(20)", "num_agents": 6, "episode_limit": 25, "policy_mapping_info": {"simple_adversary": {"description": "one team attack, one team survive", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_crypto": {"description": "two team cooperate, one team attack", "team_prefix": ["eve_", "bob_", "alice_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_push": {"description": "one team target on landmark, one team attack", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_tag": {"description": "one team attack, one team survive", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_spread": {"description": "one team cooperate", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}, "simple_reference": {"description": "one team cooperate", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}, "simple_world_comm": {"description": "two team cooperate and attack, one team survive", "team_prefix": ["adversary_", "leadadversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_speaker_listener": {"description": "two team cooperate", "team_prefix": ["speaker_", "listener_"], "all_agents_one_policy": true, "one_agent_one_policy": true}}, "agent_name_ls": ["leadadversary_0", "adversary_0", "adversary_1", "adversary_2", "agent_0", "agent_1"]}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "mpe_simple_world_comm", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 50, "evaluation_num_episodes": 10, "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 100, "min_iter_time_s": 0, "timesteps_per_iteration": 0, "seed": 321, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_adversary_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}], "policy_leadadversary_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}], "policy_agent_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x7f88aff354c0>", "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "simple_optimizer": false, "monitor": -1, "use_critic": true, "use_gae": true, "lambda": 1.0, "kl_coeff": 0.2, "sgd_minibatch_size": 50, "shuffle_sequences": true, "num_sgd_iter": 2, "lr_schedule": null, "vf_loss_coeff": 1.0, "entropy_coeff": 0.01, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": null, "kl_target": 0.01, "vf_share_layers": -1}, "time_since_restore": 9.59610891342163, "timesteps_since_restore": 0, "iterations_since_restore": 23, "perf": {"cpu_util_percent": 32.5, "ram_util_percent": 46.5}}
{"episode_reward_max": 98.42627085353728, "episode_reward_min": -155.78308767969756, "episode_reward_mean": -20.23960645342518, "episode_len_mean": 25.0, "episode_media": {}, "episodes_this_iter": 2, "policy_reward_min": {"policy_leadadversary_": -4.542143749578077, "policy_adversary_": -4.621733620085473, "policy_agent_": -96.35415811344997}, "policy_reward_max": {"policy_leadadversary_": 33.854903602692595, "policy_adversary_": 34.36826010203567, "policy_agent_": 27.837397306389533}, "policy_reward_mean": {"policy_leadadversary_": 0.7413443892167383, "policy_adversary_": 1.0141080114459327, "policy_agent_": -12.011637438489856}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-105.16393252669111, -38.08425587795351, -8.179283354852107, -7.994660195117557, 17.029253709543042, -62.95754389372587, -21.306978110434503, -9.963224993494906, -35.6723275216737, -7.801818117525467, 9.04258457925305, -9.16790291343267, -23.11610041380425, -76.64256918047145, 19.012759487513534, -18.237441060454206, -26.428737443182527, -42.52695238562914, -72.55115007329658, -11.5502931963818, -9.816421651401027, -155.78308767969756, -37.19573589166182, -10.625310490304468, 20.61462187851487, 56.41119583016825, -13.315507414957573, -12.10170378283629, 98.42627085353728, -33.048655056364915, 7.3217210673286806, 7.327237400716829, -23.088022125278687, 21.452118299951458, -34.22571107500617, -37.05712090715862, -7.758585683990852, 7.429427413636523, -64.55081708876132, -72.81867023241337, -57.907825000034876, 3.058827652902239, -9.394727393314803, 5.757473025486409, -37.32645149477665, -38.73999230490706, -4.66851184524558, -7.6165725867276715], "episode_lengths": [25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25], "policy_policy_leadadversary__reward": [-2.399035557624381, 8.302786016962129, -1.9111450930988323, -1.90667729707557, -4.038309224748798, -1.1928023284527944, 6.938235712341814, -1.8308626607889866, 8.940230507526055, -2.0965552281416944, 9.03154180074615, -1.2461341680319717, -4.388542193694188, -3.847920286524414, 3.5200136941709386, -2.964544320613795, -2.9630602344890113, -2.5514853414160785, -3.423786335468951, -1.7669051568509833, -1.2819197880175521, -2.793092838548524, -2.715631555274084, -1.888458905121377, 8.491376358690058, 19.490398201983847, -3.414607305270243, -1.9683014538118688, 33.854903602692595, -2.2506002232734774, 2.321446715633231, 2.5983351813573012, -1.5375524704754444, 8.954483912319503, -1.838966463801028, -4.542143749578077, -1.887097192016523, -1.9669168715798406, -4.259228446259269, -3.418530040554258, -1.6405950365018764, 3.4187604704462027, -2.0529470784989825, 2.950861039809551, -1.6417408125125332, -1.173521562486733, -0.8453547453422087, -1.5838705663315764], "policy_policy_adversary__reward": [-1.6741618220129826, -2.2223381552451884, -1.7978662959280882, 6.195230400528265, 9.085335341319167, 9.161857338996002, -1.1497964664252314, -0.7244895816458103, -2.705072794727737, -1.9452619286739838, -0.5002713058248459, -2.428351018980954, -2.073914420541661, -2.491769450548833, -1.928789335869684, -1.6434950773450425, -1.1869343121273628, -3.094481311936268, 9.22635362970437, 8.437625312282139, 8.558180033954198, -1.474364867288304, -2.699172611779865, -1.9577497509222836, 8.892856901464825, 8.219320737803885, 8.128349998386524, -2.4694792844144473, -0.6928708713293986, -1.5511211993405283, 6.420991744018833, 7.748318394913137, 8.185932035407232, -2.473275836552458, -0.5674346699607982, -2.7263543698194392, -0.5909371354705603, -1.2690244550254388, -1.8793974687070758, -2.8351443039978297, -3.923014083123016, -2.3079879234678007, 4.100268481179938, 3.027702044631114, 4.02297814215707, -2.0777870863086294, -1.9840069073615636, -1.669300530018625, -3.9338029974868833, -1.129238869347308, -1.3580459203598705, -2.250033013063289, -1.3601789265843973, -2.654726573595358, -1.7265988852649663, -3.7396060163471576, -1.820129784531345, -2.9201218301303506, -1.7090159550536768, -2.424621160302172, -2.53712801256559, -2.876435417206376, -1.4196451923447468, -1.5248898471676176, -2.768539034385942, -1.768760251247098, -3.013244773720978, -3.0025095397079458, -1.7121641935407885, -1.6493536919918832, -3.9070691940185345, -1.3224203607070766, 7.256290170226535, 8.009821795532776, 9.511926850083018, 19.35511620124706, 18.84000636214317, 19.519538085643973, -1.4999996116110483, -3.8952631049612574, -2.070937464898907, -2.546883520298444, -2.9137085640161007, -2.9466401471505344, 33.50823641156576, 34.36826010203567, 32.62555861085578, -1.594179646955657, -0.7648866358471844, -2.61833002303639, 3.932860003891398, 4.357704493462412, 3.6291405799201555, 3.6581468826299237, 2.9729168454181787, 4.531212041059542, -1.4324294123408556, -3.5804977490861387, -4.292088229889296, 7.364570933829991, 8.207346016991234, 8.425087820102204, -1.4899061486169511, -0.888369164140079, -2.1729673605733337, -0.9880730925424175, -0.8979613232266417, -3.2498547972579335, -1.4440590307296355, -0.9465905128076592, -1.6162837096317748, -1.481425948122367, -1.476298099855236, -1.1826997013762748, -3.7611817019384506, -0.6523852938800511, -1.0971635214558346, -1.0180042767264177, -1.6408081573998603, -2.3188704379693, -1.7534750970959454, -2.0578000943715407, -3.478190868351961, 2.197108121194587, 4.007253584005375, 0.8085618433346088, -2.7249530991388955, -0.7346005542861018, -2.6911144456741787, 4.318448499420746, 3.402533710413069, 2.6100128528088105, -4.621733620085473, -1.5859334919515493, -3.592891398226516, -1.1648055450308883, -1.7780974600334736, -1.7707289994905449, -0.5398622257926751, -1.2153213731765846, -1.0242404139779537, -0.5400433557265615, -1.6776735020958116, -2.1234977001066433], "policy_policy_agent__reward": [-0.7163725824305123, -96.35415811344997, -3.617249732269532, -67.21221524348954, -1.1897859301531337, -0.49899348880136807, -0.7213000577969194, -0.49279858676528443, 27.837397306389533, -0.27536116513751435, -1.8093468032775135, -54.03048406058688, -42.95203037708829, -11.515342421628725, -1.0331583960521016, -0.9679167066633642, -0.927224282567324, -68.92586138428767, -0.4035124455295982, -0.5882790887698023, -0.3080346699354489, -22.036164725896853, -1.0213367048557638, -1.1333671642122476, -0.6140299114418961, -14.374169249465094, -0.5039952235368317, -63.22450735982157, 9.76231673232199, -5.42051960694752, -0.38719184121053235, -9.154610374941065, -1.2188982803065158, -15.825691141192937, -32.77419517584195, -0.9363333551280546, -61.65847002433208, -0.182559027352075, -1.3441869339869565, -1.3854421600576674, -1.2320086747231598, -0.4692845665436012, -91.95162118944084, -54.97618451890754, -25.81192265544753, -0.9402631739704812, -0.7369098771347249, -1.1210984613308634, -1.2206295166227459, -11.434163779394773, -20.24663057019898, -0.5472324506508123, -0.7910180498339603, -1.6436818783821536, -0.8500054566128976, -0.8761646409464355, -20.63989374553544, -15.290794128077069, -1.1518478368502902, -24.66881069040191, -5.722489091533723, -1.1969416340448025, -5.930741298443036, -0.5026322513050725, -11.882773789857106, -0.36268047362983874, -1.1761058559306894, -10.323264527360775, -27.075190531297906, -0.7603114065768609, -0.6153024430104596, -26.76378550154309, -0.8350650603929229, -1.029490178412334, -4.33587304506435, 17.872641079634594, -13.677754358552278, -41.103103766675446, 2.9893260231465204, -67.41178334291008, -48.50968190580077, -0.4680819979127372, -5.722255746040154, -1.6506006200383874, -0.5229746771561022, -0.6681375385605484, -1.2405475750286472, -6.283835501937115, -0.7738420813283601, -25.11031009067222, -32.51991083703785, -0.3329279008275307, -0.6073557419411857, -0.4363773450149738, -0.404379749566543, -1.287107712900539]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.5560033945682037, "mean_inference_ms": 2.7173261511584577, "mean_action_processing_ms": 0.10517979859580097, "mean_env_wait_ms": 3.302345922864982, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 1, "timesteps_total": 1200, "timesteps_this_iter": 0, "agent_timesteps_total": 7200, "timers": {"sample_time_ms": 458.049, "sample_throughput": 109.159, "load_time_ms": 0.424, "load_throughput": 117897.009, "learn_time_ms": 69.326, "learn_throughput": 721.228, "update_time_ms": 1.862}, "info": {"learner": {"policy_leadadversary_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 2.384185791015625e-08, "cur_lr": 0.0005, "total_loss": 0.5131844580173492, "policy_loss": -0.0014950847346337781, "vf_loss": 0.5445477664470673, "vf_explained_var": 0.03960716724395752, "kl": 5.635636352252504e-06, "entropy": 2.9868253469467163, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_adversary_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 2.3841857910156248e-08, "cur_lr": 0.0005, "total_loss": 0.3679552475611369, "policy_loss": -0.006811022758483887, "vf_loss": 0.4035550430417061, "vf_explained_var": -0.29130252202351886, "kl": 0.000884997014085768, "entropy": 2.878881335258484, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_agent_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 2.384185791015625e-08, "cur_lr": 0.0005, "total_loss": 3.6394846439361572, "policy_loss": -0.003060445189476013, "vf_loss": 3.671915650367737, "vf_explained_var": 0.1415557712316513, "kl": 0.00012789472970520244, "entropy": 2.937035620212555, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}}, "num_steps_sampled": 1200, "num_agent_steps_sampled": 7200, "num_steps_trained": 1200, "num_agent_steps_trained": 7200, "num_steps_trained_this_iter": 0}, "done": false, "episodes_total": 48, "training_iteration": 24, "trial_id": "f5f5f_00000", "experiment_id": "d79be65b63864cf3aab1c0ed35a1f497", "date": "2025-03-09_18-29-03", "timestamp": 1741541343, "time_this_iter_s": 0.4314396381378174, "time_total_s": 10.027548551559448, "pid": 48554, "hostname": "julien-PC", "node_ip": "192.168.89.58", "config": {"num_workers": 1, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 50, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 50, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "Centralized_Critic_Model", "custom_model_config": {"env": "mpe", "env_args": {"continuous_actions": false, "max_cycles": 25, "map_name": "simple_world_comm"}, "mask_flag": false, "global_state_flag": false, "opp_action_in_cc": true, "agent_level_batch_update": false, "force_coop": false, "local_mode": false, "share_policy": "group", "evaluation_interval": 50, "framework": "torch", "num_workers": 1, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "checkpoint_freq": 20, "checkpoint_end": true, "restore_path": {"model_path": "", "params_path": ""}, "stop_iters": 9999999, "stop_timesteps": 2000000, "stop_reward": 999999, "seed": 321, "local_dir": "", "model_arch_args": {"hidden_state_size": 256, "core_arch": "mlp", "fc_layer": 2, "out_dim_fc_0": 128, "out_dim_fc_1": 64, "encode_layer": "128-256"}, "algorithm": "mappo", "space_obs": "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "space_act": "Discrete(20)", "num_agents": 6, "episode_limit": 25, "policy_mapping_info": {"simple_adversary": {"description": "one team attack, one team survive", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_crypto": {"description": "two team cooperate, one team attack", "team_prefix": ["eve_", "bob_", "alice_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_push": {"description": "one team target on landmark, one team attack", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_tag": {"description": "one team attack, one team survive", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_spread": {"description": "one team cooperate", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}, "simple_reference": {"description": "one team cooperate", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}, "simple_world_comm": {"description": "two team cooperate and attack, one team survive", "team_prefix": ["adversary_", "leadadversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_speaker_listener": {"description": "two team cooperate", "team_prefix": ["speaker_", "listener_"], "all_agents_one_policy": true, "one_agent_one_policy": true}}, "agent_name_ls": ["leadadversary_0", "adversary_0", "adversary_1", "adversary_2", "agent_0", "agent_1"]}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "mpe_simple_world_comm", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 50, "evaluation_num_episodes": 10, "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 100, "min_iter_time_s": 0, "timesteps_per_iteration": 0, "seed": 321, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_adversary_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}], "policy_leadadversary_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}], "policy_agent_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x7f88afedfca0>", "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "simple_optimizer": false, "monitor": -1, "use_critic": true, "use_gae": true, "lambda": 1.0, "kl_coeff": 0.2, "sgd_minibatch_size": 50, "shuffle_sequences": true, "num_sgd_iter": 2, "lr_schedule": null, "vf_loss_coeff": 1.0, "entropy_coeff": 0.01, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": null, "kl_target": 0.01, "vf_share_layers": -1}, "time_since_restore": 10.027548551559448, "timesteps_since_restore": 0, "iterations_since_restore": 24, "perf": {"cpu_util_percent": 26.8, "ram_util_percent": 46.5}}
{"episode_reward_max": 98.42627085353728, "episode_reward_min": -155.78308767969756, "episode_reward_mean": -20.19107381678426, "episode_len_mean": 25.0, "episode_media": {}, "episodes_this_iter": 2, "policy_reward_min": {"policy_leadadversary_": -4.542143749578077, "policy_adversary_": -4.621733620085473, "policy_agent_": -96.35415811344997}, "policy_reward_max": {"policy_leadadversary_": 33.854903602692595, "policy_adversary_": 34.36826010203567, "policy_agent_": 27.837397306389533}, "policy_reward_mean": {"policy_leadadversary_": 0.9419598303697658, "policy_adversary_": 1.1963802779271069, "policy_agent_": -12.361087240467674}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-105.16393252669111, -38.08425587795351, -8.179283354852107, -7.994660195117557, 17.029253709543042, -62.95754389372587, -21.306978110434503, -9.963224993494906, -35.6723275216737, -7.801818117525467, 9.04258457925305, -9.16790291343267, -23.11610041380425, -76.64256918047145, 19.012759487513534, -18.237441060454206, -26.428737443182527, -42.52695238562914, -72.55115007329658, -11.5502931963818, -9.816421651401027, -155.78308767969756, -37.19573589166182, -10.625310490304468, 20.61462187851487, 56.41119583016825, -13.315507414957573, -12.10170378283629, 98.42627085353728, -33.048655056364915, 7.3217210673286806, 7.327237400716829, -23.088022125278687, 21.452118299951458, -34.22571107500617, -37.05712090715862, -7.758585683990852, 7.429427413636523, -64.55081708876132, -72.81867023241337, -57.907825000034876, 3.058827652902239, -9.394727393314803, 5.757473025486409, -37.32645149477665, -38.73999230490706, -4.66851184524558, -7.6165725867276715, -59.64437090063567, 21.591789825831185], "episode_lengths": [25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25], "policy_policy_leadadversary__reward": [-2.399035557624381, 8.302786016962129, -1.9111450930988323, -1.90667729707557, -4.038309224748798, -1.1928023284527944, 6.938235712341814, -1.8308626607889866, 8.940230507526055, -2.0965552281416944, 9.03154180074615, -1.2461341680319717, -4.388542193694188, -3.847920286524414, 3.5200136941709386, -2.964544320613795, -2.9630602344890113, -2.5514853414160785, -3.423786335468951, -1.7669051568509833, -1.2819197880175521, -2.793092838548524, -2.715631555274084, -1.888458905121377, 8.491376358690058, 19.490398201983847, -3.414607305270243, -1.9683014538118688, 33.854903602692595, -2.2506002232734774, 2.321446715633231, 2.5983351813573012, -1.5375524704754444, 8.954483912319503, -1.838966463801028, -4.542143749578077, -1.887097192016523, -1.9669168715798406, -4.259228446259269, -3.418530040554258, -1.6405950365018764, 3.4187604704462027, -2.0529470784989825, 2.950861039809551, -1.6417408125125332, -1.173521562486733, -0.8453547453422087, -1.5838705663315764, 2.3471854601712994, 9.166275375913555], "policy_policy_adversary__reward": [-1.6741618220129826, -2.2223381552451884, -1.7978662959280882, 6.195230400528265, 9.085335341319167, 9.161857338996002, -1.1497964664252314, -0.7244895816458103, -2.705072794727737, -1.9452619286739838, -0.5002713058248459, -2.428351018980954, -2.073914420541661, -2.491769450548833, -1.928789335869684, -1.6434950773450425, -1.1869343121273628, -3.094481311936268, 9.22635362970437, 8.437625312282139, 8.558180033954198, -1.474364867288304, -2.699172611779865, -1.9577497509222836, 8.892856901464825, 8.219320737803885, 8.128349998386524, -2.4694792844144473, -0.6928708713293986, -1.5511211993405283, 6.420991744018833, 7.748318394913137, 8.185932035407232, -2.473275836552458, -0.5674346699607982, -2.7263543698194392, -0.5909371354705603, -1.2690244550254388, -1.8793974687070758, -2.8351443039978297, -3.923014083123016, -2.3079879234678007, 4.100268481179938, 3.027702044631114, 4.02297814215707, -2.0777870863086294, -1.9840069073615636, -1.669300530018625, -3.9338029974868833, -1.129238869347308, -1.3580459203598705, -2.250033013063289, -1.3601789265843973, -2.654726573595358, -1.7265988852649663, -3.7396060163471576, -1.820129784531345, -2.9201218301303506, -1.7090159550536768, -2.424621160302172, -2.53712801256559, -2.876435417206376, -1.4196451923447468, -1.5248898471676176, -2.768539034385942, -1.768760251247098, -3.013244773720978, -3.0025095397079458, -1.7121641935407885, -1.6493536919918832, -3.9070691940185345, -1.3224203607070766, 7.256290170226535, 8.009821795532776, 9.511926850083018, 19.35511620124706, 18.84000636214317, 19.519538085643973, -1.4999996116110483, -3.8952631049612574, -2.070937464898907, -2.546883520298444, -2.9137085640161007, -2.9466401471505344, 33.50823641156576, 34.36826010203567, 32.62555861085578, -1.594179646955657, -0.7648866358471844, -2.61833002303639, 3.932860003891398, 4.357704493462412, 3.6291405799201555, 3.6581468826299237, 2.9729168454181787, 4.531212041059542, -1.4324294123408556, -3.5804977490861387, -4.292088229889296, 7.364570933829991, 8.207346016991234, 8.425087820102204, -1.4899061486169511, -0.888369164140079, -2.1729673605733337, -0.9880730925424175, -0.8979613232266417, -3.2498547972579335, -1.4440590307296355, -0.9465905128076592, -1.6162837096317748, -1.481425948122367, -1.476298099855236, -1.1826997013762748, -3.7611817019384506, -0.6523852938800511, -1.0971635214558346, -1.0180042767264177, -1.6408081573998603, -2.3188704379693, -1.7534750970959454, -2.0578000943715407, -3.478190868351961, 2.197108121194587, 4.007253584005375, 0.8085618433346088, -2.7249530991388955, -0.7346005542861018, -2.6911144456741787, 4.318448499420746, 3.402533710413069, 2.6100128528088105, -4.621733620085473, -1.5859334919515493, -3.592891398226516, -1.1648055450308883, -1.7780974600334736, -1.7707289994905449, -0.5398622257926751, -1.2153213731765846, -1.0242404139779537, -0.5400433557265615, -1.6776735020958116, -2.1234977001066433, 1.402346749557846, 3.0330542043038955, 4.341864249270812, 8.965739918088033, 7.44370158149564, 8.238781338135515], "policy_policy_agent__reward": [-0.7163725824305123, -96.35415811344997, -3.617249732269532, -67.21221524348954, -1.1897859301531337, -0.49899348880136807, -0.7213000577969194, -0.49279858676528443, 27.837397306389533, -0.27536116513751435, -1.8093468032775135, -54.03048406058688, -42.95203037708829, -11.515342421628725, -1.0331583960521016, -0.9679167066633642, -0.927224282567324, -68.92586138428767, -0.4035124455295982, -0.5882790887698023, -0.3080346699354489, -22.036164725896853, -1.0213367048557638, -1.1333671642122476, -0.6140299114418961, -14.374169249465094, -0.5039952235368317, -63.22450735982157, 9.76231673232199, -5.42051960694752, -0.38719184121053235, -9.154610374941065, -1.2188982803065158, -15.825691141192937, -32.77419517584195, -0.9363333551280546, -61.65847002433208, -0.182559027352075, -1.3441869339869565, -1.3854421600576674, -1.2320086747231598, -0.4692845665436012, -91.95162118944084, -54.97618451890754, -25.81192265544753, -0.9402631739704812, -0.7369098771347249, -1.1210984613308634, -1.2206295166227459, -11.434163779394773, -20.24663057019898, -0.5472324506508123, -0.7910180498339603, -1.6436818783821536, -0.8500054566128976, -0.8761646409464355, -20.63989374553544, -15.290794128077069, -1.1518478368502902, -24.66881069040191, -5.722489091533723, -1.1969416340448025, -5.930741298443036, -0.5026322513050725, -11.882773789857106, -0.36268047362983874, -1.1761058559306894, -10.323264527360775, -27.075190531297906, -0.7603114065768609, -0.6153024430104596, -26.76378550154309, -0.8350650603929229, -1.029490178412334, -4.33587304506435, 17.872641079634594, -13.677754358552278, -41.103103766675446, 2.9893260231465204, -67.41178334291008, -48.50968190580077, -0.4680819979127372, -5.722255746040154, -1.6506006200383874, -0.5229746771561022, -0.6681375385605484, -1.2405475750286472, -6.283835501937115, -0.7738420813283601, -25.11031009067222, -32.51991083703785, -0.3329279008275307, -0.6073557419411857, -0.4363773450149738, -0.404379749566543, -1.287107712900539, -64.61098833998355, -6.157833223955975, -1.401673562237826, -10.821034825563737]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.555416315978393, "mean_inference_ms": 2.717714381045962, "mean_action_processing_ms": 0.10517210502446234, "mean_env_wait_ms": 3.3039871389236293, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 1, "timesteps_total": 1250, "timesteps_this_iter": 0, "agent_timesteps_total": 7500, "timers": {"sample_time_ms": 460.28, "sample_throughput": 108.629, "load_time_ms": 0.427, "load_throughput": 116989.401, "learn_time_ms": 70.19, "learn_throughput": 712.35, "update_time_ms": 1.813}, "info": {"learner": {"policy_leadadversary_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 1.1920928955078126e-08, "cur_lr": 0.0005, "total_loss": 29.853907585144043, "policy_loss": -0.001862792982720407, "vf_loss": 29.885438919067383, "vf_explained_var": 0.01437118649482727, "kl": 1.2392044488329645e-05, "entropy": 2.966990351676941, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_adversary_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 1.1920928955078124e-08, "cur_lr": 0.0005, "total_loss": 25.8471040725708, "policy_loss": -0.0037283276518185935, "vf_loss": 25.879169782002766, "vf_explained_var": 0.0036319990952809653, "kl": 0.0004017737864506765, "entropy": 2.833742340405782, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_agent_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 1.1920928955078126e-08, "cur_lr": 0.0005, "total_loss": 295.41024923324585, "policy_loss": -0.0024713128805160522, "vf_loss": 295.4417200088501, "vf_explained_var": -0.00784476101398468, "kl": 0.00021964962080311956, "entropy": 2.899183511734009, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}}, "num_steps_sampled": 1250, "num_agent_steps_sampled": 7500, "num_steps_trained": 1250, "num_agent_steps_trained": 7500, "num_steps_trained_this_iter": 0}, "done": false, "episodes_total": 50, "training_iteration": 25, "trial_id": "f5f5f_00000", "experiment_id": "d79be65b63864cf3aab1c0ed35a1f497", "date": "2025-03-09_18-29-04", "timestamp": 1741541344, "time_this_iter_s": 0.4108603000640869, "time_total_s": 10.438408851623535, "pid": 48554, "hostname": "julien-PC", "node_ip": "192.168.89.58", "config": {"num_workers": 1, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 50, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 50, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "Centralized_Critic_Model", "custom_model_config": {"env": "mpe", "env_args": {"continuous_actions": false, "max_cycles": 25, "map_name": "simple_world_comm"}, "mask_flag": false, "global_state_flag": false, "opp_action_in_cc": true, "agent_level_batch_update": false, "force_coop": false, "local_mode": false, "share_policy": "group", "evaluation_interval": 50, "framework": "torch", "num_workers": 1, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "checkpoint_freq": 20, "checkpoint_end": true, "restore_path": {"model_path": "", "params_path": ""}, "stop_iters": 9999999, "stop_timesteps": 2000000, "stop_reward": 999999, "seed": 321, "local_dir": "", "model_arch_args": {"hidden_state_size": 256, "core_arch": "mlp", "fc_layer": 2, "out_dim_fc_0": 128, "out_dim_fc_1": 64, "encode_layer": "128-256"}, "algorithm": "mappo", "space_obs": "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "space_act": "Discrete(20)", "num_agents": 6, "episode_limit": 25, "policy_mapping_info": {"simple_adversary": {"description": "one team attack, one team survive", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_crypto": {"description": "two team cooperate, one team attack", "team_prefix": ["eve_", "bob_", "alice_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_push": {"description": "one team target on landmark, one team attack", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_tag": {"description": "one team attack, one team survive", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_spread": {"description": "one team cooperate", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}, "simple_reference": {"description": "one team cooperate", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}, "simple_world_comm": {"description": "two team cooperate and attack, one team survive", "team_prefix": ["adversary_", "leadadversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_speaker_listener": {"description": "two team cooperate", "team_prefix": ["speaker_", "listener_"], "all_agents_one_policy": true, "one_agent_one_policy": true}}, "agent_name_ls": ["leadadversary_0", "adversary_0", "adversary_1", "adversary_2", "agent_0", "agent_1"]}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "mpe_simple_world_comm", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 50, "evaluation_num_episodes": 10, "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 100, "min_iter_time_s": 0, "timesteps_per_iteration": 0, "seed": 321, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_adversary_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}], "policy_leadadversary_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}], "policy_agent_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x7f88afedf160>", "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "simple_optimizer": false, "monitor": -1, "use_critic": true, "use_gae": true, "lambda": 1.0, "kl_coeff": 0.2, "sgd_minibatch_size": 50, "shuffle_sequences": true, "num_sgd_iter": 2, "lr_schedule": null, "vf_loss_coeff": 1.0, "entropy_coeff": 0.01, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": null, "kl_target": 0.01, "vf_share_layers": -1}, "time_since_restore": 10.438408851623535, "timesteps_since_restore": 0, "iterations_since_restore": 25, "perf": {}}
{"episode_reward_max": 98.42627085353728, "episode_reward_min": -155.78308767969756, "episode_reward_mean": -19.035315494409943, "episode_len_mean": 25.0, "episode_media": {}, "episodes_this_iter": 2, "policy_reward_min": {"policy_leadadversary_": -4.542143749578077, "policy_adversary_": -4.621733620085473, "policy_agent_": -96.35415811344997}, "policy_reward_max": {"policy_leadadversary_": 33.854903602692595, "policy_adversary_": 34.36826010203567, "policy_agent_": 27.837397306389533}, "policy_reward_mean": {"policy_leadadversary_": 1.0852184220736942, "policy_adversary_": 1.3542490791336588, "policy_agent_": -12.091640576942307}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-105.16393252669111, -38.08425587795351, -8.179283354852107, -7.994660195117557, 17.029253709543042, -62.95754389372587, -21.306978110434503, -9.963224993494906, -35.6723275216737, -7.801818117525467, 9.04258457925305, -9.16790291343267, -23.11610041380425, -76.64256918047145, 19.012759487513534, -18.237441060454206, -26.428737443182527, -42.52695238562914, -72.55115007329658, -11.5502931963818, -9.816421651401027, -155.78308767969756, -37.19573589166182, -10.625310490304468, 20.61462187851487, 56.41119583016825, -13.315507414957573, -12.10170378283629, 98.42627085353728, -33.048655056364915, 7.3217210673286806, 7.327237400716829, -23.088022125278687, 21.452118299951458, -34.22571107500617, -37.05712090715862, -7.758585683990852, 7.429427413636523, -64.55081708876132, -72.81867023241337, -57.907825000034876, 3.058827652902239, -9.394727393314803, 5.757473025486409, -37.32645149477665, -38.73999230490706, -4.66851184524558, -7.6165725867276715, -59.64437090063567, 21.591789825831185, 3.2949798405375788, 16.422305289358494], "episode_lengths": [25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25], "policy_policy_leadadversary__reward": [-2.399035557624381, 8.302786016962129, -1.9111450930988323, -1.90667729707557, -4.038309224748798, -1.1928023284527944, 6.938235712341814, -1.8308626607889866, 8.940230507526055, -2.0965552281416944, 9.03154180074615, -1.2461341680319717, -4.388542193694188, -3.847920286524414, 3.5200136941709386, -2.964544320613795, -2.9630602344890113, -2.5514853414160785, -3.423786335468951, -1.7669051568509833, -1.2819197880175521, -2.793092838548524, -2.715631555274084, -1.888458905121377, 8.491376358690058, 19.490398201983847, -3.414607305270243, -1.9683014538118688, 33.854903602692595, -2.2506002232734774, 2.321446715633231, 2.5983351813573012, -1.5375524704754444, 8.954483912319503, -1.838966463801028, -4.542143749578077, -1.887097192016523, -1.9669168715798406, -4.259228446259269, -3.418530040554258, -1.6405950365018764, 3.4187604704462027, -2.0529470784989825, 2.950861039809551, -1.6417408125125332, -1.173521562486733, -0.8453547453422087, -1.5838705663315764, 2.3471854601712994, 9.166275375913555, 1.4773564118411457, 7.8560100175026655], "policy_policy_adversary__reward": [-1.6741618220129826, -2.2223381552451884, -1.7978662959280882, 6.195230400528265, 9.085335341319167, 9.161857338996002, -1.1497964664252314, -0.7244895816458103, -2.705072794727737, -1.9452619286739838, -0.5002713058248459, -2.428351018980954, -2.073914420541661, -2.491769450548833, -1.928789335869684, -1.6434950773450425, -1.1869343121273628, -3.094481311936268, 9.22635362970437, 8.437625312282139, 8.558180033954198, -1.474364867288304, -2.699172611779865, -1.9577497509222836, 8.892856901464825, 8.219320737803885, 8.128349998386524, -2.4694792844144473, -0.6928708713293986, -1.5511211993405283, 6.420991744018833, 7.748318394913137, 8.185932035407232, -2.473275836552458, -0.5674346699607982, -2.7263543698194392, -0.5909371354705603, -1.2690244550254388, -1.8793974687070758, -2.8351443039978297, -3.923014083123016, -2.3079879234678007, 4.100268481179938, 3.027702044631114, 4.02297814215707, -2.0777870863086294, -1.9840069073615636, -1.669300530018625, -3.9338029974868833, -1.129238869347308, -1.3580459203598705, -2.250033013063289, -1.3601789265843973, -2.654726573595358, -1.7265988852649663, -3.7396060163471576, -1.820129784531345, -2.9201218301303506, -1.7090159550536768, -2.424621160302172, -2.53712801256559, -2.876435417206376, -1.4196451923447468, -1.5248898471676176, -2.768539034385942, -1.768760251247098, -3.013244773720978, -3.0025095397079458, -1.7121641935407885, -1.6493536919918832, -3.9070691940185345, -1.3224203607070766, 7.256290170226535, 8.009821795532776, 9.511926850083018, 19.35511620124706, 18.84000636214317, 19.519538085643973, -1.4999996116110483, -3.8952631049612574, -2.070937464898907, -2.546883520298444, -2.9137085640161007, -2.9466401471505344, 33.50823641156576, 34.36826010203567, 32.62555861085578, -1.594179646955657, -0.7648866358471844, -2.61833002303639, 3.932860003891398, 4.357704493462412, 3.6291405799201555, 3.6581468826299237, 2.9729168454181787, 4.531212041059542, -1.4324294123408556, -3.5804977490861387, -4.292088229889296, 7.364570933829991, 8.207346016991234, 8.425087820102204, -1.4899061486169511, -0.888369164140079, -2.1729673605733337, -0.9880730925424175, -0.8979613232266417, -3.2498547972579335, -1.4440590307296355, -0.9465905128076592, -1.6162837096317748, -1.481425948122367, -1.476298099855236, -1.1826997013762748, -3.7611817019384506, -0.6523852938800511, -1.0971635214558346, -1.0180042767264177, -1.6408081573998603, -2.3188704379693, -1.7534750970959454, -2.0578000943715407, -3.478190868351961, 2.197108121194587, 4.007253584005375, 0.8085618433346088, -2.7249530991388955, -0.7346005542861018, -2.6911144456741787, 4.318448499420746, 3.402533710413069, 2.6100128528088105, -4.621733620085473, -1.5859334919515493, -3.592891398226516, -1.1648055450308883, -1.7780974600334736, -1.7707289994905449, -0.5398622257926751, -1.2153213731765846, -1.0242404139779537, -0.5400433557265615, -1.6776735020958116, -2.1234977001066433, 1.402346749557846, 3.0330542043038955, 4.341864249270812, 8.965739918088033, 7.44370158149564, 8.238781338135515, 4.30390780446801, 1.0151693211549975, 3.7941534640047134, 6.746279317105572, 6.515020187034392, 9.431284562017067], "policy_policy_agent__reward": [-0.7163725824305123, -96.35415811344997, -3.617249732269532, -67.21221524348954, -1.1897859301531337, -0.49899348880136807, -0.7213000577969194, -0.49279858676528443, 27.837397306389533, -0.27536116513751435, -1.8093468032775135, -54.03048406058688, -42.95203037708829, -11.515342421628725, -1.0331583960521016, -0.9679167066633642, -0.927224282567324, -68.92586138428767, -0.4035124455295982, -0.5882790887698023, -0.3080346699354489, -22.036164725896853, -1.0213367048557638, -1.1333671642122476, -0.6140299114418961, -14.374169249465094, -0.5039952235368317, -63.22450735982157, 9.76231673232199, -5.42051960694752, -0.38719184121053235, -9.154610374941065, -1.2188982803065158, -15.825691141192937, -32.77419517584195, -0.9363333551280546, -61.65847002433208, -0.182559027352075, -1.3441869339869565, -1.3854421600576674, -1.2320086747231598, -0.4692845665436012, -91.95162118944084, -54.97618451890754, -25.81192265544753, -0.9402631739704812, -0.7369098771347249, -1.1210984613308634, -1.2206295166227459, -11.434163779394773, -20.24663057019898, -0.5472324506508123, -0.7910180498339603, -1.6436818783821536, -0.8500054566128976, -0.8761646409464355, -20.63989374553544, -15.290794128077069, -1.1518478368502902, -24.66881069040191, -5.722489091533723, -1.1969416340448025, -5.930741298443036, -0.5026322513050725, -11.882773789857106, -0.36268047362983874, -1.1761058559306894, -10.323264527360775, -27.075190531297906, -0.7603114065768609, -0.6153024430104596, -26.76378550154309, -0.8350650603929229, -1.029490178412334, -4.33587304506435, 17.872641079634594, -13.677754358552278, -41.103103766675446, 2.9893260231465204, -67.41178334291008, -48.50968190580077, -0.4680819979127372, -5.722255746040154, -1.6506006200383874, -0.5229746771561022, -0.6681375385605484, -1.2405475750286472, -6.283835501937115, -0.7738420813283601, -25.11031009067222, -32.51991083703785, -0.3329279008275307, -0.6073557419411857, -0.4363773450149738, -0.404379749566543, -1.287107712900539, -64.61098833998355, -6.157833223955975, -1.401673562237826, -10.821034825563737, -1.2154147079260624, -6.080192453005229, -0.49372985197767705, -13.632558942323527]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.5548874200086867, "mean_inference_ms": 2.718438325258123, "mean_action_processing_ms": 0.10516714973816219, "mean_env_wait_ms": 3.305475198035872, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 1, "timesteps_total": 1300, "timesteps_this_iter": 0, "agent_timesteps_total": 7800, "timers": {"sample_time_ms": 464.201, "sample_throughput": 107.712, "load_time_ms": 0.452, "load_throughput": 110574.291, "learn_time_ms": 72.118, "learn_throughput": 693.304, "update_time_ms": 1.873}, "info": {"learner": {"policy_leadadversary_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 5.960464477539063e-09, "cur_lr": 0.0005, "total_loss": 23.58576774597168, "policy_loss": -0.0017079436313363061, "vf_loss": 23.61724853515625, "vf_explained_var": -0.027253150939941406, "kl": 9.215388521610635e-06, "entropy": 2.977252721786499, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_adversary_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 5.960464477539062e-09, "cur_lr": 0.0005, "total_loss": 23.84520475069682, "policy_loss": -0.0052100615575909615, "vf_loss": 23.877979119618733, "vf_explained_var": -0.003717531760533651, "kl": 0.0006163821958627599, "entropy": 2.7563944657643638, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_agent_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 5.960464477539063e-09, "cur_lr": 0.0005, "total_loss": 12.055982947349548, "policy_loss": -0.003845471888780594, "vf_loss": 12.088718831539154, "vf_explained_var": -0.04525366425514221, "kl": 0.00016987482104902085, "entropy": 2.889138102531433, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}}, "num_steps_sampled": 1300, "num_agent_steps_sampled": 7800, "num_steps_trained": 1300, "num_agent_steps_trained": 7800, "num_steps_trained_this_iter": 0}, "done": false, "episodes_total": 52, "training_iteration": 26, "trial_id": "f5f5f_00000", "experiment_id": "d79be65b63864cf3aab1c0ed35a1f497", "date": "2025-03-09_18-29-04", "timestamp": 1741541344, "time_this_iter_s": 0.44431257247924805, "time_total_s": 10.882721424102783, "pid": 48554, "hostname": "julien-PC", "node_ip": "192.168.89.58", "config": {"num_workers": 1, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 50, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 50, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "Centralized_Critic_Model", "custom_model_config": {"env": "mpe", "env_args": {"continuous_actions": false, "max_cycles": 25, "map_name": "simple_world_comm"}, "mask_flag": false, "global_state_flag": false, "opp_action_in_cc": true, "agent_level_batch_update": false, "force_coop": false, "local_mode": false, "share_policy": "group", "evaluation_interval": 50, "framework": "torch", "num_workers": 1, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "checkpoint_freq": 20, "checkpoint_end": true, "restore_path": {"model_path": "", "params_path": ""}, "stop_iters": 9999999, "stop_timesteps": 2000000, "stop_reward": 999999, "seed": 321, "local_dir": "", "model_arch_args": {"hidden_state_size": 256, "core_arch": "mlp", "fc_layer": 2, "out_dim_fc_0": 128, "out_dim_fc_1": 64, "encode_layer": "128-256"}, "algorithm": "mappo", "space_obs": "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "space_act": "Discrete(20)", "num_agents": 6, "episode_limit": 25, "policy_mapping_info": {"simple_adversary": {"description": "one team attack, one team survive", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_crypto": {"description": "two team cooperate, one team attack", "team_prefix": ["eve_", "bob_", "alice_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_push": {"description": "one team target on landmark, one team attack", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_tag": {"description": "one team attack, one team survive", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_spread": {"description": "one team cooperate", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}, "simple_reference": {"description": "one team cooperate", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}, "simple_world_comm": {"description": "two team cooperate and attack, one team survive", "team_prefix": ["adversary_", "leadadversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_speaker_listener": {"description": "two team cooperate", "team_prefix": ["speaker_", "listener_"], "all_agents_one_policy": true, "one_agent_one_policy": true}}, "agent_name_ls": ["leadadversary_0", "adversary_0", "adversary_1", "adversary_2", "agent_0", "agent_1"]}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "mpe_simple_world_comm", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 50, "evaluation_num_episodes": 10, "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 100, "min_iter_time_s": 0, "timesteps_per_iteration": 0, "seed": 321, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_adversary_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}], "policy_leadadversary_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}], "policy_agent_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x7f88aff1a820>", "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "simple_optimizer": false, "monitor": -1, "use_critic": true, "use_gae": true, "lambda": 1.0, "kl_coeff": 0.2, "sgd_minibatch_size": 50, "shuffle_sequences": true, "num_sgd_iter": 2, "lr_schedule": null, "vf_loss_coeff": 1.0, "entropy_coeff": 0.01, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": null, "kl_target": 0.01, "vf_share_layers": -1}, "time_since_restore": 10.882721424102783, "timesteps_since_restore": 0, "iterations_since_restore": 26, "perf": {"cpu_util_percent": 25.2, "ram_util_percent": 46.5}}
{"episode_reward_max": 98.42627085353728, "episode_reward_min": -155.78308767969756, "episode_reward_mean": -18.700860663170637, "episode_len_mean": 25.0, "episode_media": {}, "episodes_this_iter": 2, "policy_reward_min": {"policy_leadadversary_": -4.542143749578077, "policy_adversary_": -4.621733620085473, "policy_agent_": -96.35415811344997}, "policy_reward_max": {"policy_leadadversary_": 33.854903602692595, "policy_adversary_": 34.36826010203567, "policy_agent_": 27.837397306389533}, "policy_reward_mean": {"policy_leadadversary_": 0.9636729998755557, "policy_adversary_": 1.232196635455192, "policy_agent_": -11.680561784705889}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-105.16393252669111, -38.08425587795351, -8.179283354852107, -7.994660195117557, 17.029253709543042, -62.95754389372587, -21.306978110434503, -9.963224993494906, -35.6723275216737, -7.801818117525467, 9.04258457925305, -9.16790291343267, -23.11610041380425, -76.64256918047145, 19.012759487513534, -18.237441060454206, -26.428737443182527, -42.52695238562914, -72.55115007329658, -11.5502931963818, -9.816421651401027, -155.78308767969756, -37.19573589166182, -10.625310490304468, 20.61462187851487, 56.41119583016825, -13.315507414957573, -12.10170378283629, 98.42627085353728, -33.048655056364915, 7.3217210673286806, 7.327237400716829, -23.088022125278687, 21.452118299951458, -34.22571107500617, -37.05712090715862, -7.758585683990852, 7.429427413636523, -64.55081708876132, -72.81867023241337, -57.907825000034876, 3.058827652902239, -9.394727393314803, 5.757473025486409, -37.32645149477665, -38.73999230490706, -4.66851184524558, -7.6165725867276715, -59.64437090063567, 21.591789825831185, 3.2949798405375788, 16.422305289358494, -10.54155505870399, -9.468515043193399], "episode_lengths": [25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25], "policy_policy_leadadversary__reward": [-2.399035557624381, 8.302786016962129, -1.9111450930988323, -1.90667729707557, -4.038309224748798, -1.1928023284527944, 6.938235712341814, -1.8308626607889866, 8.940230507526055, -2.0965552281416944, 9.03154180074615, -1.2461341680319717, -4.388542193694188, -3.847920286524414, 3.5200136941709386, -2.964544320613795, -2.9630602344890113, -2.5514853414160785, -3.423786335468951, -1.7669051568509833, -1.2819197880175521, -2.793092838548524, -2.715631555274084, -1.888458905121377, 8.491376358690058, 19.490398201983847, -3.414607305270243, -1.9683014538118688, 33.854903602692595, -2.2506002232734774, 2.321446715633231, 2.5983351813573012, -1.5375524704754444, 8.954483912319503, -1.838966463801028, -4.542143749578077, -1.887097192016523, -1.9669168715798406, -4.259228446259269, -3.418530040554258, -1.6405950365018764, 3.4187604704462027, -2.0529470784989825, 2.950861039809551, -1.6417408125125332, -1.173521562486733, -0.8453547453422087, -1.5838705663315764, 2.3471854601712994, 9.166275375913555, 1.4773564118411457, 7.8560100175026655, -2.482234807213631, -1.9107811473384637], "policy_policy_adversary__reward": [-1.6741618220129826, -2.2223381552451884, -1.7978662959280882, 6.195230400528265, 9.085335341319167, 9.161857338996002, -1.1497964664252314, -0.7244895816458103, -2.705072794727737, -1.9452619286739838, -0.5002713058248459, -2.428351018980954, -2.073914420541661, -2.491769450548833, -1.928789335869684, -1.6434950773450425, -1.1869343121273628, -3.094481311936268, 9.22635362970437, 8.437625312282139, 8.558180033954198, -1.474364867288304, -2.699172611779865, -1.9577497509222836, 8.892856901464825, 8.219320737803885, 8.128349998386524, -2.4694792844144473, -0.6928708713293986, -1.5511211993405283, 6.420991744018833, 7.748318394913137, 8.185932035407232, -2.473275836552458, -0.5674346699607982, -2.7263543698194392, -0.5909371354705603, -1.2690244550254388, -1.8793974687070758, -2.8351443039978297, -3.923014083123016, -2.3079879234678007, 4.100268481179938, 3.027702044631114, 4.02297814215707, -2.0777870863086294, -1.9840069073615636, -1.669300530018625, -3.9338029974868833, -1.129238869347308, -1.3580459203598705, -2.250033013063289, -1.3601789265843973, -2.654726573595358, -1.7265988852649663, -3.7396060163471576, -1.820129784531345, -2.9201218301303506, -1.7090159550536768, -2.424621160302172, -2.53712801256559, -2.876435417206376, -1.4196451923447468, -1.5248898471676176, -2.768539034385942, -1.768760251247098, -3.013244773720978, -3.0025095397079458, -1.7121641935407885, -1.6493536919918832, -3.9070691940185345, -1.3224203607070766, 7.256290170226535, 8.009821795532776, 9.511926850083018, 19.35511620124706, 18.84000636214317, 19.519538085643973, -1.4999996116110483, -3.8952631049612574, -2.070937464898907, -2.546883520298444, -2.9137085640161007, -2.9466401471505344, 33.50823641156576, 34.36826010203567, 32.62555861085578, -1.594179646955657, -0.7648866358471844, -2.61833002303639, 3.932860003891398, 4.357704493462412, 3.6291405799201555, 3.6581468826299237, 2.9729168454181787, 4.531212041059542, -1.4324294123408556, -3.5804977490861387, -4.292088229889296, 7.364570933829991, 8.207346016991234, 8.425087820102204, -1.4899061486169511, -0.888369164140079, -2.1729673605733337, -0.9880730925424175, -0.8979613232266417, -3.2498547972579335, -1.4440590307296355, -0.9465905128076592, -1.6162837096317748, -1.481425948122367, -1.476298099855236, -1.1826997013762748, -3.7611817019384506, -0.6523852938800511, -1.0971635214558346, -1.0180042767264177, -1.6408081573998603, -2.3188704379693, -1.7534750970959454, -2.0578000943715407, -3.478190868351961, 2.197108121194587, 4.007253584005375, 0.8085618433346088, -2.7249530991388955, -0.7346005542861018, -2.6911144456741787, 4.318448499420746, 3.402533710413069, 2.6100128528088105, -4.621733620085473, -1.5859334919515493, -3.592891398226516, -1.1648055450308883, -1.7780974600334736, -1.7707289994905449, -0.5398622257926751, -1.2153213731765846, -1.0242404139779537, -0.5400433557265615, -1.6776735020958116, -2.1234977001066433, 1.402346749557846, 3.0330542043038955, 4.341864249270812, 8.965739918088033, 7.44370158149564, 8.238781338135515, 4.30390780446801, 1.0151693211549975, 3.7941534640047134, 6.746279317105572, 6.515020187034392, 9.431284562017067, -1.954449545579552, -2.1242511296450366, -1.8472674506123379, -3.600484033802333, -0.9036071713321907, -1.2169420701381708], "policy_policy_agent__reward": [-0.7163725824305123, -96.35415811344997, -3.617249732269532, -67.21221524348954, -1.1897859301531337, -0.49899348880136807, -0.7213000577969194, -0.49279858676528443, 27.837397306389533, -0.27536116513751435, -1.8093468032775135, -54.03048406058688, -42.95203037708829, -11.515342421628725, -1.0331583960521016, -0.9679167066633642, -0.927224282567324, -68.92586138428767, -0.4035124455295982, -0.5882790887698023, -0.3080346699354489, -22.036164725896853, -1.0213367048557638, -1.1333671642122476, -0.6140299114418961, -14.374169249465094, -0.5039952235368317, -63.22450735982157, 9.76231673232199, -5.42051960694752, -0.38719184121053235, -9.154610374941065, -1.2188982803065158, -15.825691141192937, -32.77419517584195, -0.9363333551280546, -61.65847002433208, -0.182559027352075, -1.3441869339869565, -1.3854421600576674, -1.2320086747231598, -0.4692845665436012, -91.95162118944084, -54.97618451890754, -25.81192265544753, -0.9402631739704812, -0.7369098771347249, -1.1210984613308634, -1.2206295166227459, -11.434163779394773, -20.24663057019898, -0.5472324506508123, -0.7910180498339603, -1.6436818783821536, -0.8500054566128976, -0.8761646409464355, -20.63989374553544, -15.290794128077069, -1.1518478368502902, -24.66881069040191, -5.722489091533723, -1.1969416340448025, -5.930741298443036, -0.5026322513050725, -11.882773789857106, -0.36268047362983874, -1.1761058559306894, -10.323264527360775, -27.075190531297906, -0.7603114065768609, -0.6153024430104596, -26.76378550154309, -0.8350650603929229, -1.029490178412334, -4.33587304506435, 17.872641079634594, -13.677754358552278, -41.103103766675446, 2.9893260231465204, -67.41178334291008, -48.50968190580077, -0.4680819979127372, -5.722255746040154, -1.6506006200383874, -0.5229746771561022, -0.6681375385605484, -1.2405475750286472, -6.283835501937115, -0.7738420813283601, -25.11031009067222, -32.51991083703785, -0.3329279008275307, -0.6073557419411857, -0.4363773450149738, -0.404379749566543, -1.287107712900539, -64.61098833998355, -6.157833223955975, -1.401673562237826, -10.821034825563737, -1.2154147079260624, -6.080192453005229, -0.49372985197767705, -13.632558942323527, -0.7359191942324639, -1.3974329314209728, -1.2580379613038053, -0.5786626592784377]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.5543084219196432, "mean_inference_ms": 2.718708074421534, "mean_action_processing_ms": 0.10514955349211112, "mean_env_wait_ms": 3.306571185186966, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 1, "timesteps_total": 1350, "timesteps_this_iter": 0, "agent_timesteps_total": 8100, "timers": {"sample_time_ms": 466.308, "sample_throughput": 107.225, "load_time_ms": 0.455, "load_throughput": 109896.348, "learn_time_ms": 72.491, "learn_throughput": 689.745, "update_time_ms": 1.876}, "info": {"learner": {"policy_leadadversary_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 2.9802322387695314e-09, "cur_lr": 0.0005, "total_loss": 1.568556010723114, "policy_loss": -0.0018466210458427668, "vf_loss": 1.5999171137809753, "vf_explained_var": -0.019475877285003662, "kl": 1.434912526310228e-05, "entropy": 2.9514663219451904, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_adversary_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 2.980232238769531e-09, "cur_lr": 0.0005, "total_loss": 2.1234968549882374, "policy_loss": -0.006191723048686981, "vf_loss": 2.1580750246842704, "vf_explained_var": -0.10694263378779094, "kl": 0.0012568818234039252, "entropy": 2.8386549154917398, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_agent_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 2.9802322387695314e-09, "cur_lr": 0.0005, "total_loss": 5.424383878707886, "policy_loss": -0.004562601447105408, "vf_loss": 5.458462119102478, "vf_explained_var": 0.004467576742172241, "kl": 0.00013226532561105842, "entropy": 2.9515979290008545, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}}, "num_steps_sampled": 1350, "num_agent_steps_sampled": 8100, "num_steps_trained": 1350, "num_agent_steps_trained": 8100, "num_steps_trained_this_iter": 0}, "done": false, "episodes_total": 54, "training_iteration": 27, "trial_id": "f5f5f_00000", "experiment_id": "d79be65b63864cf3aab1c0ed35a1f497", "date": "2025-03-09_18-29-05", "timestamp": 1741541345, "time_this_iter_s": 0.38730335235595703, "time_total_s": 11.27002477645874, "pid": 48554, "hostname": "julien-PC", "node_ip": "192.168.89.58", "config": {"num_workers": 1, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 50, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 50, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "Centralized_Critic_Model", "custom_model_config": {"env": "mpe", "env_args": {"continuous_actions": false, "max_cycles": 25, "map_name": "simple_world_comm"}, "mask_flag": false, "global_state_flag": false, "opp_action_in_cc": true, "agent_level_batch_update": false, "force_coop": false, "local_mode": false, "share_policy": "group", "evaluation_interval": 50, "framework": "torch", "num_workers": 1, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "checkpoint_freq": 20, "checkpoint_end": true, "restore_path": {"model_path": "", "params_path": ""}, "stop_iters": 9999999, "stop_timesteps": 2000000, "stop_reward": 999999, "seed": 321, "local_dir": "", "model_arch_args": {"hidden_state_size": 256, "core_arch": "mlp", "fc_layer": 2, "out_dim_fc_0": 128, "out_dim_fc_1": 64, "encode_layer": "128-256"}, "algorithm": "mappo", "space_obs": "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "space_act": "Discrete(20)", "num_agents": 6, "episode_limit": 25, "policy_mapping_info": {"simple_adversary": {"description": "one team attack, one team survive", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_crypto": {"description": "two team cooperate, one team attack", "team_prefix": ["eve_", "bob_", "alice_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_push": {"description": "one team target on landmark, one team attack", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_tag": {"description": "one team attack, one team survive", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_spread": {"description": "one team cooperate", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}, "simple_reference": {"description": "one team cooperate", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}, "simple_world_comm": {"description": "two team cooperate and attack, one team survive", "team_prefix": ["adversary_", "leadadversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_speaker_listener": {"description": "two team cooperate", "team_prefix": ["speaker_", "listener_"], "all_agents_one_policy": true, "one_agent_one_policy": true}}, "agent_name_ls": ["leadadversary_0", "adversary_0", "adversary_1", "adversary_2", "agent_0", "agent_1"]}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "mpe_simple_world_comm", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 50, "evaluation_num_episodes": 10, "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 100, "min_iter_time_s": 0, "timesteps_per_iteration": 0, "seed": 321, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_adversary_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}], "policy_leadadversary_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}], "policy_agent_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x7f88aff358b0>", "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "simple_optimizer": false, "monitor": -1, "use_critic": true, "use_gae": true, "lambda": 1.0, "kl_coeff": 0.2, "sgd_minibatch_size": 50, "shuffle_sequences": true, "num_sgd_iter": 2, "lr_schedule": null, "vf_loss_coeff": 1.0, "entropy_coeff": 0.01, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": null, "kl_target": 0.01, "vf_share_layers": -1}, "time_since_restore": 11.27002477645874, "timesteps_since_restore": 0, "iterations_since_restore": 27, "perf": {"cpu_util_percent": 19.2, "ram_util_percent": 46.6}}
{"episode_reward_max": 98.42627085353728, "episode_reward_min": -155.78308767969756, "episode_reward_mean": -20.18349335147402, "episode_len_mean": 25.0, "episode_media": {}, "episodes_this_iter": 2, "policy_reward_min": {"policy_leadadversary_": -4.542143749578077, "policy_adversary_": -4.621733620085473, "policy_agent_": -96.35415811344997}, "policy_reward_max": {"policy_leadadversary_": 33.854903602692595, "policy_adversary_": 34.36826010203567, "policy_agent_": 27.837397306389533}, "policy_reward_mean": {"policy_leadadversary_": 0.8393081291489322, "policy_adversary_": 1.1104506773322291, "policy_agent_": -12.177076756309818}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-105.16393252669111, -38.08425587795351, -8.179283354852107, -7.994660195117557, 17.029253709543042, -62.95754389372587, -21.306978110434503, -9.963224993494906, -35.6723275216737, -7.801818117525467, 9.04258457925305, -9.16790291343267, -23.11610041380425, -76.64256918047145, 19.012759487513534, -18.237441060454206, -26.428737443182527, -42.52695238562914, -72.55115007329658, -11.5502931963818, -9.816421651401027, -155.78308767969756, -37.19573589166182, -10.625310490304468, 20.61462187851487, 56.41119583016825, -13.315507414957573, -12.10170378283629, 98.42627085353728, -33.048655056364915, 7.3217210673286806, 7.327237400716829, -23.088022125278687, 21.452118299951458, -34.22571107500617, -37.05712090715862, -7.758585683990852, 7.429427413636523, -64.55081708876132, -72.81867023241337, -57.907825000034876, 3.058827652902239, -9.394727393314803, 5.757473025486409, -37.32645149477665, -38.73999230490706, -4.66851184524558, -7.6165725867276715, -59.64437090063567, 21.591789825831185, 3.2949798405375788, 16.422305289358494, -10.54155505870399, -9.468515043193399, -66.93944423241335, -53.48970763891736], "episode_lengths": [25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25], "policy_policy_leadadversary__reward": [-2.399035557624381, 8.302786016962129, -1.9111450930988323, -1.90667729707557, -4.038309224748798, -1.1928023284527944, 6.938235712341814, -1.8308626607889866, 8.940230507526055, -2.0965552281416944, 9.03154180074615, -1.2461341680319717, -4.388542193694188, -3.847920286524414, 3.5200136941709386, -2.964544320613795, -2.9630602344890113, -2.5514853414160785, -3.423786335468951, -1.7669051568509833, -1.2819197880175521, -2.793092838548524, -2.715631555274084, -1.888458905121377, 8.491376358690058, 19.490398201983847, -3.414607305270243, -1.9683014538118688, 33.854903602692595, -2.2506002232734774, 2.321446715633231, 2.5983351813573012, -1.5375524704754444, 8.954483912319503, -1.838966463801028, -4.542143749578077, -1.887097192016523, -1.9669168715798406, -4.259228446259269, -3.418530040554258, -1.6405950365018764, 3.4187604704462027, -2.0529470784989825, 2.950861039809551, -1.6417408125125332, -1.173521562486733, -0.8453547453422087, -1.5838705663315764, 2.3471854601712994, 9.166275375913555, 1.4773564118411457, 7.8560100175026655, -2.482234807213631, -1.9107811473384637, -2.8311678664115933, -2.205918894528226], "policy_policy_adversary__reward": [-1.6741618220129826, -2.2223381552451884, -1.7978662959280882, 6.195230400528265, 9.085335341319167, 9.161857338996002, -1.1497964664252314, -0.7244895816458103, -2.705072794727737, -1.9452619286739838, -0.5002713058248459, -2.428351018980954, -2.073914420541661, -2.491769450548833, -1.928789335869684, -1.6434950773450425, -1.1869343121273628, -3.094481311936268, 9.22635362970437, 8.437625312282139, 8.558180033954198, -1.474364867288304, -2.699172611779865, -1.9577497509222836, 8.892856901464825, 8.219320737803885, 8.128349998386524, -2.4694792844144473, -0.6928708713293986, -1.5511211993405283, 6.420991744018833, 7.748318394913137, 8.185932035407232, -2.473275836552458, -0.5674346699607982, -2.7263543698194392, -0.5909371354705603, -1.2690244550254388, -1.8793974687070758, -2.8351443039978297, -3.923014083123016, -2.3079879234678007, 4.100268481179938, 3.027702044631114, 4.02297814215707, -2.0777870863086294, -1.9840069073615636, -1.669300530018625, -3.9338029974868833, -1.129238869347308, -1.3580459203598705, -2.250033013063289, -1.3601789265843973, -2.654726573595358, -1.7265988852649663, -3.7396060163471576, -1.820129784531345, -2.9201218301303506, -1.7090159550536768, -2.424621160302172, -2.53712801256559, -2.876435417206376, -1.4196451923447468, -1.5248898471676176, -2.768539034385942, -1.768760251247098, -3.013244773720978, -3.0025095397079458, -1.7121641935407885, -1.6493536919918832, -3.9070691940185345, -1.3224203607070766, 7.256290170226535, 8.009821795532776, 9.511926850083018, 19.35511620124706, 18.84000636214317, 19.519538085643973, -1.4999996116110483, -3.8952631049612574, -2.070937464898907, -2.546883520298444, -2.9137085640161007, -2.9466401471505344, 33.50823641156576, 34.36826010203567, 32.62555861085578, -1.594179646955657, -0.7648866358471844, -2.61833002303639, 3.932860003891398, 4.357704493462412, 3.6291405799201555, 3.6581468826299237, 2.9729168454181787, 4.531212041059542, -1.4324294123408556, -3.5804977490861387, -4.292088229889296, 7.364570933829991, 8.207346016991234, 8.425087820102204, -1.4899061486169511, -0.888369164140079, -2.1729673605733337, -0.9880730925424175, -0.8979613232266417, -3.2498547972579335, -1.4440590307296355, -0.9465905128076592, -1.6162837096317748, -1.481425948122367, -1.476298099855236, -1.1826997013762748, -3.7611817019384506, -0.6523852938800511, -1.0971635214558346, -1.0180042767264177, -1.6408081573998603, -2.3188704379693, -1.7534750970959454, -2.0578000943715407, -3.478190868351961, 2.197108121194587, 4.007253584005375, 0.8085618433346088, -2.7249530991388955, -0.7346005542861018, -2.6911144456741787, 4.318448499420746, 3.402533710413069, 2.6100128528088105, -4.621733620085473, -1.5859334919515493, -3.592891398226516, -1.1648055450308883, -1.7780974600334736, -1.7707289994905449, -0.5398622257926751, -1.2153213731765846, -1.0242404139779537, -0.5400433557265615, -1.6776735020958116, -2.1234977001066433, 1.402346749557846, 3.0330542043038955, 4.341864249270812, 8.965739918088033, 7.44370158149564, 8.238781338135515, 4.30390780446801, 1.0151693211549975, 3.7941534640047134, 6.746279317105572, 6.515020187034392, 9.431284562017067, -1.954449545579552, -2.1242511296450366, -1.8472674506123379, -3.600484033802333, -0.9036071713321907, -1.2169420701381708, -3.461940161656796, -0.7046985620528773, -2.059661620717326, -2.37213281426346, -2.8425345768730166, -1.6191734163631457], "policy_policy_agent__reward": [-0.7163725824305123, -96.35415811344997, -3.617249732269532, -67.21221524348954, -1.1897859301531337, -0.49899348880136807, -0.7213000577969194, -0.49279858676528443, 27.837397306389533, -0.27536116513751435, -1.8093468032775135, -54.03048406058688, -42.95203037708829, -11.515342421628725, -1.0331583960521016, -0.9679167066633642, -0.927224282567324, -68.92586138428767, -0.4035124455295982, -0.5882790887698023, -0.3080346699354489, -22.036164725896853, -1.0213367048557638, -1.1333671642122476, -0.6140299114418961, -14.374169249465094, -0.5039952235368317, -63.22450735982157, 9.76231673232199, -5.42051960694752, -0.38719184121053235, -9.154610374941065, -1.2188982803065158, -15.825691141192937, -32.77419517584195, -0.9363333551280546, -61.65847002433208, -0.182559027352075, -1.3441869339869565, -1.3854421600576674, -1.2320086747231598, -0.4692845665436012, -91.95162118944084, -54.97618451890754, -25.81192265544753, -0.9402631739704812, -0.7369098771347249, -1.1210984613308634, -1.2206295166227459, -11.434163779394773, -20.24663057019898, -0.5472324506508123, -0.7910180498339603, -1.6436818783821536, -0.8500054566128976, -0.8761646409464355, -20.63989374553544, -15.290794128077069, -1.1518478368502902, -24.66881069040191, -5.722489091533723, -1.1969416340448025, -5.930741298443036, -0.5026322513050725, -11.882773789857106, -0.36268047362983874, -1.1761058559306894, -10.323264527360775, -27.075190531297906, -0.7603114065768609, -0.6153024430104596, -26.76378550154309, -0.8350650603929229, -1.029490178412334, -4.33587304506435, 17.872641079634594, -13.677754358552278, -41.103103766675446, 2.9893260231465204, -67.41178334291008, -48.50968190580077, -0.4680819979127372, -5.722255746040154, -1.6506006200383874, -0.5229746771561022, -0.6681375385605484, -1.2405475750286472, -6.283835501937115, -0.7738420813283601, -25.11031009067222, -32.51991083703785, -0.3329279008275307, -0.6073557419411857, -0.4363773450149738, -0.404379749566543, -1.287107712900539, -64.61098833998355, -6.157833223955975, -1.401673562237826, -10.821034825563737, -1.2154147079260624, -6.080192453005229, -0.49372985197767705, -13.632558942323527, -0.7359191942324639, -1.3974329314209728, -1.2580379613038053, -0.5786626592784377, -45.69097945815503, -12.19099656341974, -43.43331183406106, -1.0166361028284194]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.5537575382410209, "mean_inference_ms": 2.718920112459646, "mean_action_processing_ms": 0.10514021257985216, "mean_env_wait_ms": 3.3077842742553543, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 1, "timesteps_total": 1400, "timesteps_this_iter": 0, "agent_timesteps_total": 8400, "timers": {"sample_time_ms": 469.729, "sample_throughput": 106.444, "load_time_ms": 0.459, "load_throughput": 109044.925, "learn_time_ms": 73.849, "learn_throughput": 677.061, "update_time_ms": 1.87}, "info": {"learner": {"policy_leadadversary_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 1.4901161193847657e-09, "cur_lr": 0.0005, "total_loss": 1.9328494668006897, "policy_loss": -0.0015360379545019676, "vf_loss": 1.964125394821167, "vf_explained_var": 0.0009604692459106445, "kl": 1.3819887064503433e-05, "entropy": 2.973988652229309, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_adversary_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 1.4901161193847655e-09, "cur_lr": 0.0005, "total_loss": 2.4000390768051147, "policy_loss": -0.004593073079983394, "vf_loss": 2.433827519416809, "vf_explained_var": 0.04943173130353292, "kl": 0.0003857775313111193, "entropy": 2.919525980949402, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_agent_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 1.4901161193847657e-09, "cur_lr": 0.0005, "total_loss": 294.1822929382324, "policy_loss": -0.004665561020374298, "vf_loss": 294.2161560058594, "vf_explained_var": 0.0071615129709243774, "kl": 0.00013283345994241635, "entropy": 2.9201648235321045, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}}, "num_steps_sampled": 1400, "num_agent_steps_sampled": 8400, "num_steps_trained": 1400, "num_agent_steps_trained": 8400, "num_steps_trained_this_iter": 0}, "done": false, "episodes_total": 56, "training_iteration": 28, "trial_id": "f5f5f_00000", "experiment_id": "d79be65b63864cf3aab1c0ed35a1f497", "date": "2025-03-09_18-29-05", "timestamp": 1741541345, "time_this_iter_s": 0.43335986137390137, "time_total_s": 11.703384637832642, "pid": 48554, "hostname": "julien-PC", "node_ip": "192.168.89.58", "config": {"num_workers": 1, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 50, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 50, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "Centralized_Critic_Model", "custom_model_config": {"env": "mpe", "env_args": {"continuous_actions": false, "max_cycles": 25, "map_name": "simple_world_comm"}, "mask_flag": false, "global_state_flag": false, "opp_action_in_cc": true, "agent_level_batch_update": false, "force_coop": false, "local_mode": false, "share_policy": "group", "evaluation_interval": 50, "framework": "torch", "num_workers": 1, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "checkpoint_freq": 20, "checkpoint_end": true, "restore_path": {"model_path": "", "params_path": ""}, "stop_iters": 9999999, "stop_timesteps": 2000000, "stop_reward": 999999, "seed": 321, "local_dir": "", "model_arch_args": {"hidden_state_size": 256, "core_arch": "mlp", "fc_layer": 2, "out_dim_fc_0": 128, "out_dim_fc_1": 64, "encode_layer": "128-256"}, "algorithm": "mappo", "space_obs": "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "space_act": "Discrete(20)", "num_agents": 6, "episode_limit": 25, "policy_mapping_info": {"simple_adversary": {"description": "one team attack, one team survive", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_crypto": {"description": "two team cooperate, one team attack", "team_prefix": ["eve_", "bob_", "alice_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_push": {"description": "one team target on landmark, one team attack", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_tag": {"description": "one team attack, one team survive", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_spread": {"description": "one team cooperate", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}, "simple_reference": {"description": "one team cooperate", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}, "simple_world_comm": {"description": "two team cooperate and attack, one team survive", "team_prefix": ["adversary_", "leadadversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_speaker_listener": {"description": "two team cooperate", "team_prefix": ["speaker_", "listener_"], "all_agents_one_policy": true, "one_agent_one_policy": true}}, "agent_name_ls": ["leadadversary_0", "adversary_0", "adversary_1", "adversary_2", "agent_0", "agent_1"]}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "mpe_simple_world_comm", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 50, "evaluation_num_episodes": 10, "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 100, "min_iter_time_s": 0, "timesteps_per_iteration": 0, "seed": 321, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_adversary_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}], "policy_leadadversary_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}], "policy_agent_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x7f88afedf670>", "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "simple_optimizer": false, "monitor": -1, "use_critic": true, "use_gae": true, "lambda": 1.0, "kl_coeff": 0.2, "sgd_minibatch_size": 50, "shuffle_sequences": true, "num_sgd_iter": 2, "lr_schedule": null, "vf_loss_coeff": 1.0, "entropy_coeff": 0.01, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": null, "kl_target": 0.01, "vf_share_layers": -1}, "time_since_restore": 11.703384637832642, "timesteps_since_restore": 0, "iterations_since_restore": 28, "perf": {}}
{"episode_reward_max": 98.42627085353728, "episode_reward_min": -155.78308767969756, "episode_reward_mean": -20.666998211906755, "episode_len_mean": 25.0, "episode_media": {}, "episodes_this_iter": 2, "policy_reward_min": {"policy_leadadversary_": -4.542143749578077, "policy_adversary_": -4.621733620085473, "policy_agent_": -96.35415811344997}, "policy_reward_max": {"policy_leadadversary_": 33.854903602692595, "policy_adversary_": 34.36826010203567, "policy_agent_": 27.837397306389533}, "policy_reward_mean": {"policy_leadadversary_": 0.7007371001182565, "policy_adversary_": 0.9883541982871844, "policy_agent_": -12.166398953443279}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-105.16393252669111, -38.08425587795351, -8.179283354852107, -7.994660195117557, 17.029253709543042, -62.95754389372587, -21.306978110434503, -9.963224993494906, -35.6723275216737, -7.801818117525467, 9.04258457925305, -9.16790291343267, -23.11610041380425, -76.64256918047145, 19.012759487513534, -18.237441060454206, -26.428737443182527, -42.52695238562914, -72.55115007329658, -11.5502931963818, -9.816421651401027, -155.78308767969756, -37.19573589166182, -10.625310490304468, 20.61462187851487, 56.41119583016825, -13.315507414957573, -12.10170378283629, 98.42627085353728, -33.048655056364915, 7.3217210673286806, 7.327237400716829, -23.088022125278687, 21.452118299951458, -34.22571107500617, -37.05712090715862, -7.758585683990852, 7.429427413636523, -64.55081708876132, -72.81867023241337, -57.907825000034876, 3.058827652902239, -9.394727393314803, 5.757473025486409, -37.32645149477665, -38.73999230490706, -4.66851184524558, -7.6165725867276715, -59.64437090063567, 21.591789825831185, 3.2949798405375788, 16.422305289358494, -10.54155505870399, -9.468515043193399, -66.93944423241335, -53.48970763891736, -8.40918682162661, -60.00108178641989], "episode_lengths": [25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25], "policy_policy_leadadversary__reward": [-2.399035557624381, 8.302786016962129, -1.9111450930988323, -1.90667729707557, -4.038309224748798, -1.1928023284527944, 6.938235712341814, -1.8308626607889866, 8.940230507526055, -2.0965552281416944, 9.03154180074615, -1.2461341680319717, -4.388542193694188, -3.847920286524414, 3.5200136941709386, -2.964544320613795, -2.9630602344890113, -2.5514853414160785, -3.423786335468951, -1.7669051568509833, -1.2819197880175521, -2.793092838548524, -2.715631555274084, -1.888458905121377, 8.491376358690058, 19.490398201983847, -3.414607305270243, -1.9683014538118688, 33.854903602692595, -2.2506002232734774, 2.321446715633231, 2.5983351813573012, -1.5375524704754444, 8.954483912319503, -1.838966463801028, -4.542143749578077, -1.887097192016523, -1.9669168715798406, -4.259228446259269, -3.418530040554258, -1.6405950365018764, 3.4187604704462027, -2.0529470784989825, 2.950861039809551, -1.6417408125125332, -1.173521562486733, -0.8453547453422087, -1.5838705663315764, 2.3471854601712994, 9.166275375913555, 1.4773564118411457, 7.8560100175026655, -2.482234807213631, -1.9107811473384637, -2.8311678664115933, -2.205918894528226, -3.1371567855259825, -3.22134663995534], "policy_policy_adversary__reward": [-1.6741618220129826, -2.2223381552451884, -1.7978662959280882, 6.195230400528265, 9.085335341319167, 9.161857338996002, -1.1497964664252314, -0.7244895816458103, -2.705072794727737, -1.9452619286739838, -0.5002713058248459, -2.428351018980954, -2.073914420541661, -2.491769450548833, -1.928789335869684, -1.6434950773450425, -1.1869343121273628, -3.094481311936268, 9.22635362970437, 8.437625312282139, 8.558180033954198, -1.474364867288304, -2.699172611779865, -1.9577497509222836, 8.892856901464825, 8.219320737803885, 8.128349998386524, -2.4694792844144473, -0.6928708713293986, -1.5511211993405283, 6.420991744018833, 7.748318394913137, 8.185932035407232, -2.473275836552458, -0.5674346699607982, -2.7263543698194392, -0.5909371354705603, -1.2690244550254388, -1.8793974687070758, -2.8351443039978297, -3.923014083123016, -2.3079879234678007, 4.100268481179938, 3.027702044631114, 4.02297814215707, -2.0777870863086294, -1.9840069073615636, -1.669300530018625, -3.9338029974868833, -1.129238869347308, -1.3580459203598705, -2.250033013063289, -1.3601789265843973, -2.654726573595358, -1.7265988852649663, -3.7396060163471576, -1.820129784531345, -2.9201218301303506, -1.7090159550536768, -2.424621160302172, -2.53712801256559, -2.876435417206376, -1.4196451923447468, -1.5248898471676176, -2.768539034385942, -1.768760251247098, -3.013244773720978, -3.0025095397079458, -1.7121641935407885, -1.6493536919918832, -3.9070691940185345, -1.3224203607070766, 7.256290170226535, 8.009821795532776, 9.511926850083018, 19.35511620124706, 18.84000636214317, 19.519538085643973, -1.4999996116110483, -3.8952631049612574, -2.070937464898907, -2.546883520298444, -2.9137085640161007, -2.9466401471505344, 33.50823641156576, 34.36826010203567, 32.62555861085578, -1.594179646955657, -0.7648866358471844, -2.61833002303639, 3.932860003891398, 4.357704493462412, 3.6291405799201555, 3.6581468826299237, 2.9729168454181787, 4.531212041059542, -1.4324294123408556, -3.5804977490861387, -4.292088229889296, 7.364570933829991, 8.207346016991234, 8.425087820102204, -1.4899061486169511, -0.888369164140079, -2.1729673605733337, -0.9880730925424175, -0.8979613232266417, -3.2498547972579335, -1.4440590307296355, -0.9465905128076592, -1.6162837096317748, -1.481425948122367, -1.476298099855236, -1.1826997013762748, -3.7611817019384506, -0.6523852938800511, -1.0971635214558346, -1.0180042767264177, -1.6408081573998603, -2.3188704379693, -1.7534750970959454, -2.0578000943715407, -3.478190868351961, 2.197108121194587, 4.007253584005375, 0.8085618433346088, -2.7249530991388955, -0.7346005542861018, -2.6911144456741787, 4.318448499420746, 3.402533710413069, 2.6100128528088105, -4.621733620085473, -1.5859334919515493, -3.592891398226516, -1.1648055450308883, -1.7780974600334736, -1.7707289994905449, -0.5398622257926751, -1.2153213731765846, -1.0242404139779537, -0.5400433557265615, -1.6776735020958116, -2.1234977001066433, 1.402346749557846, 3.0330542043038955, 4.341864249270812, 8.965739918088033, 7.44370158149564, 8.238781338135515, 4.30390780446801, 1.0151693211549975, 3.7941534640047134, 6.746279317105572, 6.515020187034392, 9.431284562017067, -1.954449545579552, -2.1242511296450366, -1.8472674506123379, -3.600484033802333, -0.9036071713321907, -1.2169420701381708, -3.461940161656796, -0.7046985620528773, -2.059661620717326, -2.37213281426346, -2.8425345768730166, -1.6191734163631457, -1.2008251292794836, -1.0586588046576018, -0.9834922225627654, -3.4340751000710297, -3.9758490796042922, -3.9291829536692404], "policy_policy_agent__reward": [-0.7163725824305123, -96.35415811344997, -3.617249732269532, -67.21221524348954, -1.1897859301531337, -0.49899348880136807, -0.7213000577969194, -0.49279858676528443, 27.837397306389533, -0.27536116513751435, -1.8093468032775135, -54.03048406058688, -42.95203037708829, -11.515342421628725, -1.0331583960521016, -0.9679167066633642, -0.927224282567324, -68.92586138428767, -0.4035124455295982, -0.5882790887698023, -0.3080346699354489, -22.036164725896853, -1.0213367048557638, -1.1333671642122476, -0.6140299114418961, -14.374169249465094, -0.5039952235368317, -63.22450735982157, 9.76231673232199, -5.42051960694752, -0.38719184121053235, -9.154610374941065, -1.2188982803065158, -15.825691141192937, -32.77419517584195, -0.9363333551280546, -61.65847002433208, -0.182559027352075, -1.3441869339869565, -1.3854421600576674, -1.2320086747231598, -0.4692845665436012, -91.95162118944084, -54.97618451890754, -25.81192265544753, -0.9402631739704812, -0.7369098771347249, -1.1210984613308634, -1.2206295166227459, -11.434163779394773, -20.24663057019898, -0.5472324506508123, -0.7910180498339603, -1.6436818783821536, -0.8500054566128976, -0.8761646409464355, -20.63989374553544, -15.290794128077069, -1.1518478368502902, -24.66881069040191, -5.722489091533723, -1.1969416340448025, -5.930741298443036, -0.5026322513050725, -11.882773789857106, -0.36268047362983874, -1.1761058559306894, -10.323264527360775, -27.075190531297906, -0.7603114065768609, -0.6153024430104596, -26.76378550154309, -0.8350650603929229, -1.029490178412334, -4.33587304506435, 17.872641079634594, -13.677754358552278, -41.103103766675446, 2.9893260231465204, -67.41178334291008, -48.50968190580077, -0.4680819979127372, -5.722255746040154, -1.6506006200383874, -0.5229746771561022, -0.6681375385605484, -1.2405475750286472, -6.283835501937115, -0.7738420813283601, -25.11031009067222, -32.51991083703785, -0.3329279008275307, -0.6073557419411857, -0.4363773450149738, -0.404379749566543, -1.287107712900539, -64.61098833998355, -6.157833223955975, -1.401673562237826, -10.821034825563737, -1.2154147079260624, -6.080192453005229, -0.49372985197767705, -13.632558942323527, -0.7359191942324639, -1.3974329314209728, -1.2580379613038053, -0.5786626592784377, -45.69097945815503, -12.19099656341974, -43.43331183406106, -1.0166361028284194, -0.8411124476865421, -1.1879414319142383, -44.175348625877135, -1.2652793872428503]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.5532419716417385, "mean_inference_ms": 2.7192203391394876, "mean_action_processing_ms": 0.10512956331912102, "mean_env_wait_ms": 3.309031789279398, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 1, "timesteps_total": 1450, "timesteps_this_iter": 0, "agent_timesteps_total": 8700, "timers": {"sample_time_ms": 472.885, "sample_throughput": 105.734, "load_time_ms": 0.465, "load_throughput": 107524.2, "learn_time_ms": 74.024, "learn_throughput": 675.453, "update_time_ms": 1.864}, "info": {"learner": {"policy_leadadversary_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 7.450580596923829e-10, "cur_lr": 0.0005, "total_loss": 2.682624101638794, "policy_loss": -0.003111848803236228, "vf_loss": 2.7154613733291626, "vf_explained_var": -0.0170171856880188, "kl": 2.6113633423197058e-05, "entropy": 2.9725260734558105, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_adversary_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 7.450580596923828e-10, "cur_lr": 0.0005, "total_loss": 2.363151560227076, "policy_loss": -0.009579576551914215, "vf_loss": 2.401593486467997, "vf_explained_var": -0.07109962900479634, "kl": 0.0005876765227534145, "entropy": 2.886229475339254, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_agent_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 7.450580596923829e-10, "cur_lr": 0.0005, "total_loss": 161.91827070713043, "policy_loss": -0.006405815482139587, "vf_loss": 161.95352363586426, "vf_explained_var": 0.21469029784202576, "kl": 0.0006055994151976662, "entropy": 2.8829548358917236, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}}, "num_steps_sampled": 1450, "num_agent_steps_sampled": 8700, "num_steps_trained": 1450, "num_agent_steps_trained": 8700, "num_steps_trained_this_iter": 0}, "done": false, "episodes_total": 58, "training_iteration": 29, "trial_id": "f5f5f_00000", "experiment_id": "d79be65b63864cf3aab1c0ed35a1f497", "date": "2025-03-09_18-29-06", "timestamp": 1741541346, "time_this_iter_s": 0.425673246383667, "time_total_s": 12.129057884216309, "pid": 48554, "hostname": "julien-PC", "node_ip": "192.168.89.58", "config": {"num_workers": 1, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 50, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 50, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "Centralized_Critic_Model", "custom_model_config": {"env": "mpe", "env_args": {"continuous_actions": false, "max_cycles": 25, "map_name": "simple_world_comm"}, "mask_flag": false, "global_state_flag": false, "opp_action_in_cc": true, "agent_level_batch_update": false, "force_coop": false, "local_mode": false, "share_policy": "group", "evaluation_interval": 50, "framework": "torch", "num_workers": 1, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "checkpoint_freq": 20, "checkpoint_end": true, "restore_path": {"model_path": "", "params_path": ""}, "stop_iters": 9999999, "stop_timesteps": 2000000, "stop_reward": 999999, "seed": 321, "local_dir": "", "model_arch_args": {"hidden_state_size": 256, "core_arch": "mlp", "fc_layer": 2, "out_dim_fc_0": 128, "out_dim_fc_1": 64, "encode_layer": "128-256"}, "algorithm": "mappo", "space_obs": "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "space_act": "Discrete(20)", "num_agents": 6, "episode_limit": 25, "policy_mapping_info": {"simple_adversary": {"description": "one team attack, one team survive", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_crypto": {"description": "two team cooperate, one team attack", "team_prefix": ["eve_", "bob_", "alice_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_push": {"description": "one team target on landmark, one team attack", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_tag": {"description": "one team attack, one team survive", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_spread": {"description": "one team cooperate", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}, "simple_reference": {"description": "one team cooperate", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}, "simple_world_comm": {"description": "two team cooperate and attack, one team survive", "team_prefix": ["adversary_", "leadadversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_speaker_listener": {"description": "two team cooperate", "team_prefix": ["speaker_", "listener_"], "all_agents_one_policy": true, "one_agent_one_policy": true}}, "agent_name_ls": ["leadadversary_0", "adversary_0", "adversary_1", "adversary_2", "agent_0", "agent_1"]}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "mpe_simple_world_comm", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 50, "evaluation_num_episodes": 10, "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 100, "min_iter_time_s": 0, "timesteps_per_iteration": 0, "seed": 321, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_adversary_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}], "policy_leadadversary_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}], "policy_agent_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x7f88afedff70>", "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "simple_optimizer": false, "monitor": -1, "use_critic": true, "use_gae": true, "lambda": 1.0, "kl_coeff": 0.2, "sgd_minibatch_size": 50, "shuffle_sequences": true, "num_sgd_iter": 2, "lr_schedule": null, "vf_loss_coeff": 1.0, "entropy_coeff": 0.01, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": null, "kl_target": 0.01, "vf_share_layers": -1}, "time_since_restore": 12.129057884216309, "timesteps_since_restore": 0, "iterations_since_restore": 29, "perf": {"cpu_util_percent": 18.1, "ram_util_percent": 46.5}}
{"episode_reward_max": 98.42627085353728, "episode_reward_min": -155.78308767969756, "episode_reward_mean": -19.243708894425122, "episode_len_mean": 25.0, "episode_media": {}, "episodes_this_iter": 2, "policy_reward_min": {"policy_leadadversary_": -4.542143749578077, "policy_adversary_": -4.621733620085473, "policy_agent_": -96.35415811344997}, "policy_reward_max": {"policy_leadadversary_": 33.854903602692595, "policy_adversary_": 34.36826010203567, "policy_agent_": 27.837397306389533}, "policy_reward_mean": {"policy_leadadversary_": 0.8848691598642866, "policy_adversary_": 1.1566469177408754, "policy_agent_": -11.799259403756015}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-105.16393252669111, -38.08425587795351, -8.179283354852107, -7.994660195117557, 17.029253709543042, -62.95754389372587, -21.306978110434503, -9.963224993494906, -35.6723275216737, -7.801818117525467, 9.04258457925305, -9.16790291343267, -23.11610041380425, -76.64256918047145, 19.012759487513534, -18.237441060454206, -26.428737443182527, -42.52695238562914, -72.55115007329658, -11.5502931963818, -9.816421651401027, -155.78308767969756, -37.19573589166182, -10.625310490304468, 20.61462187851487, 56.41119583016825, -13.315507414957573, -12.10170378283629, 98.42627085353728, -33.048655056364915, 7.3217210673286806, 7.327237400716829, -23.088022125278687, 21.452118299951458, -34.22571107500617, -37.05712090715862, -7.758585683990852, 7.429427413636523, -64.55081708876132, -72.81867023241337, -57.907825000034876, 3.058827652902239, -9.394727393314803, 5.757473025486409, -37.32645149477665, -38.73999230490706, -4.66851184524558, -7.6165725867276715, -59.64437090063567, 21.591789825831185, 3.2949798405375788, 16.422305289358494, -10.54155505870399, -9.468515043193399, -66.93944423241335, -53.48970763891736, -8.40918682162661, -60.00108178641989, 27.597611762832496, 16.465750862251895], "episode_lengths": [25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25], "policy_policy_leadadversary__reward": [-2.399035557624381, 8.302786016962129, -1.9111450930988323, -1.90667729707557, -4.038309224748798, -1.1928023284527944, 6.938235712341814, -1.8308626607889866, 8.940230507526055, -2.0965552281416944, 9.03154180074615, -1.2461341680319717, -4.388542193694188, -3.847920286524414, 3.5200136941709386, -2.964544320613795, -2.9630602344890113, -2.5514853414160785, -3.423786335468951, -1.7669051568509833, -1.2819197880175521, -2.793092838548524, -2.715631555274084, -1.888458905121377, 8.491376358690058, 19.490398201983847, -3.414607305270243, -1.9683014538118688, 33.854903602692595, -2.2506002232734774, 2.321446715633231, 2.5983351813573012, -1.5375524704754444, 8.954483912319503, -1.838966463801028, -4.542143749578077, -1.887097192016523, -1.9669168715798406, -4.259228446259269, -3.418530040554258, -1.6405950365018764, 3.4187604704462027, -2.0529470784989825, 2.950861039809551, -1.6417408125125332, -1.173521562486733, -0.8453547453422087, -1.5838705663315764, 2.3471854601712994, 9.166275375913555, 1.4773564118411457, 7.8560100175026655, -2.482234807213631, -1.9107811473384637, -2.8311678664115933, -2.205918894528226, -3.1371567855259825, -3.22134663995534, 4.258662230969782, 8.190735554028539], "policy_policy_adversary__reward": [-1.6741618220129826, -2.2223381552451884, -1.7978662959280882, 6.195230400528265, 9.085335341319167, 9.161857338996002, -1.1497964664252314, -0.7244895816458103, -2.705072794727737, -1.9452619286739838, -0.5002713058248459, -2.428351018980954, -2.073914420541661, -2.491769450548833, -1.928789335869684, -1.6434950773450425, -1.1869343121273628, -3.094481311936268, 9.22635362970437, 8.437625312282139, 8.558180033954198, -1.474364867288304, -2.699172611779865, -1.9577497509222836, 8.892856901464825, 8.219320737803885, 8.128349998386524, -2.4694792844144473, -0.6928708713293986, -1.5511211993405283, 6.420991744018833, 7.748318394913137, 8.185932035407232, -2.473275836552458, -0.5674346699607982, -2.7263543698194392, -0.5909371354705603, -1.2690244550254388, -1.8793974687070758, -2.8351443039978297, -3.923014083123016, -2.3079879234678007, 4.100268481179938, 3.027702044631114, 4.02297814215707, -2.0777870863086294, -1.9840069073615636, -1.669300530018625, -3.9338029974868833, -1.129238869347308, -1.3580459203598705, -2.250033013063289, -1.3601789265843973, -2.654726573595358, -1.7265988852649663, -3.7396060163471576, -1.820129784531345, -2.9201218301303506, -1.7090159550536768, -2.424621160302172, -2.53712801256559, -2.876435417206376, -1.4196451923447468, -1.5248898471676176, -2.768539034385942, -1.768760251247098, -3.013244773720978, -3.0025095397079458, -1.7121641935407885, -1.6493536919918832, -3.9070691940185345, -1.3224203607070766, 7.256290170226535, 8.009821795532776, 9.511926850083018, 19.35511620124706, 18.84000636214317, 19.519538085643973, -1.4999996116110483, -3.8952631049612574, -2.070937464898907, -2.546883520298444, -2.9137085640161007, -2.9466401471505344, 33.50823641156576, 34.36826010203567, 32.62555861085578, -1.594179646955657, -0.7648866358471844, -2.61833002303639, 3.932860003891398, 4.357704493462412, 3.6291405799201555, 3.6581468826299237, 2.9729168454181787, 4.531212041059542, -1.4324294123408556, -3.5804977490861387, -4.292088229889296, 7.364570933829991, 8.207346016991234, 8.425087820102204, -1.4899061486169511, -0.888369164140079, -2.1729673605733337, -0.9880730925424175, -0.8979613232266417, -3.2498547972579335, -1.4440590307296355, -0.9465905128076592, -1.6162837096317748, -1.481425948122367, -1.476298099855236, -1.1826997013762748, -3.7611817019384506, -0.6523852938800511, -1.0971635214558346, -1.0180042767264177, -1.6408081573998603, -2.3188704379693, -1.7534750970959454, -2.0578000943715407, -3.478190868351961, 2.197108121194587, 4.007253584005375, 0.8085618433346088, -2.7249530991388955, -0.7346005542861018, -2.6911144456741787, 4.318448499420746, 3.402533710413069, 2.6100128528088105, -4.621733620085473, -1.5859334919515493, -3.592891398226516, -1.1648055450308883, -1.7780974600334736, -1.7707289994905449, -0.5398622257926751, -1.2153213731765846, -1.0242404139779537, -0.5400433557265615, -1.6776735020958116, -2.1234977001066433, 1.402346749557846, 3.0330542043038955, 4.341864249270812, 8.965739918088033, 7.44370158149564, 8.238781338135515, 4.30390780446801, 1.0151693211549975, 3.7941534640047134, 6.746279317105572, 6.515020187034392, 9.431284562017067, -1.954449545579552, -2.1242511296450366, -1.8472674506123379, -3.600484033802333, -0.9036071713321907, -1.2169420701381708, -3.461940161656796, -0.7046985620528773, -2.059661620717326, -2.37213281426346, -2.8425345768730166, -1.6191734163631457, -1.2008251292794836, -1.0586588046576018, -0.9834922225627654, -3.4340751000710297, -3.9758490796042922, -3.9291829536692404, 2.716099370548289, 3.022756736316401, 4.46027632001432, 8.389422832182536, 8.331942051357524, 9.302317380968413], "policy_policy_agent__reward": [-0.7163725824305123, -96.35415811344997, -3.617249732269532, -67.21221524348954, -1.1897859301531337, -0.49899348880136807, -0.7213000577969194, -0.49279858676528443, 27.837397306389533, -0.27536116513751435, -1.8093468032775135, -54.03048406058688, -42.95203037708829, -11.515342421628725, -1.0331583960521016, -0.9679167066633642, -0.927224282567324, -68.92586138428767, -0.4035124455295982, -0.5882790887698023, -0.3080346699354489, -22.036164725896853, -1.0213367048557638, -1.1333671642122476, -0.6140299114418961, -14.374169249465094, -0.5039952235368317, -63.22450735982157, 9.76231673232199, -5.42051960694752, -0.38719184121053235, -9.154610374941065, -1.2188982803065158, -15.825691141192937, -32.77419517584195, -0.9363333551280546, -61.65847002433208, -0.182559027352075, -1.3441869339869565, -1.3854421600576674, -1.2320086747231598, -0.4692845665436012, -91.95162118944084, -54.97618451890754, -25.81192265544753, -0.9402631739704812, -0.7369098771347249, -1.1210984613308634, -1.2206295166227459, -11.434163779394773, -20.24663057019898, -0.5472324506508123, -0.7910180498339603, -1.6436818783821536, -0.8500054566128976, -0.8761646409464355, -20.63989374553544, -15.290794128077069, -1.1518478368502902, -24.66881069040191, -5.722489091533723, -1.1969416340448025, -5.930741298443036, -0.5026322513050725, -11.882773789857106, -0.36268047362983874, -1.1761058559306894, -10.323264527360775, -27.075190531297906, -0.7603114065768609, -0.6153024430104596, -26.76378550154309, -0.8350650603929229, -1.029490178412334, -4.33587304506435, 17.872641079634594, -13.677754358552278, -41.103103766675446, 2.9893260231465204, -67.41178334291008, -48.50968190580077, -0.4680819979127372, -5.722255746040154, -1.6506006200383874, -0.5229746771561022, -0.6681375385605484, -1.2405475750286472, -6.283835501937115, -0.7738420813283601, -25.11031009067222, -32.51991083703785, -0.3329279008275307, -0.6073557419411857, -0.4363773450149738, -0.404379749566543, -1.287107712900539, -64.61098833998355, -6.157833223955975, -1.401673562237826, -10.821034825563737, -1.2154147079260624, -6.080192453005229, -0.49372985197767705, -13.632558942323527, -0.7359191942324639, -1.3974329314209728, -1.2580379613038053, -0.5786626592784377, -45.69097945815503, -12.19099656341974, -43.43331183406106, -1.0166361028284194, -0.8411124476865421, -1.1879414319142383, -44.175348625877135, -1.2652793872428503, -1.7295602169250743, 14.86937732190877, -17.372234795940894, -0.37643216034422006]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.5527034941107398, "mean_inference_ms": 2.7192731885660573, "mean_action_processing_ms": 0.10511091569682442, "mean_env_wait_ms": 3.3100505985661592, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 1, "timesteps_total": 1500, "timesteps_this_iter": 0, "agent_timesteps_total": 9000, "timers": {"sample_time_ms": 468.991, "sample_throughput": 106.612, "load_time_ms": 0.472, "load_throughput": 105831.247, "learn_time_ms": 72.457, "learn_throughput": 690.067, "update_time_ms": 1.831}, "info": {"learner": {"policy_leadadversary_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 3.7252902984619143e-10, "cur_lr": 0.0005, "total_loss": 21.715320587158203, "policy_loss": -0.003997464329003719, "vf_loss": 21.7489070892334, "vf_explained_var": 0.007611721754074097, "kl": 4.2586874112515716e-05, "entropy": 2.9585859775543213, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_adversary_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 3.725290298461914e-10, "cur_lr": 0.0005, "total_loss": 18.840110540390015, "policy_loss": -0.0074395133803288145, "vf_loss": 18.876116633415222, "vf_explained_var": 0.005158970753351848, "kl": 0.00046566398285359983, "entropy": 2.8568060795466104, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_agent_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 3.7252902984619143e-10, "cur_lr": 0.0005, "total_loss": 28.640550136566162, "policy_loss": -0.003766462206840515, "vf_loss": 28.67333984375, "vf_explained_var": -0.017105311155319214, "kl": 0.0003245678673133301, "entropy": 2.9023062586784363, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}}, "num_steps_sampled": 1500, "num_agent_steps_sampled": 9000, "num_steps_trained": 1500, "num_agent_steps_trained": 9000, "num_steps_trained_this_iter": 0}, "done": false, "episodes_total": 60, "training_iteration": 30, "trial_id": "f5f5f_00000", "experiment_id": "d79be65b63864cf3aab1c0ed35a1f497", "date": "2025-03-09_18-29-06", "timestamp": 1741541346, "time_this_iter_s": 0.3914804458618164, "time_total_s": 12.520538330078125, "pid": 48554, "hostname": "julien-PC", "node_ip": "192.168.89.58", "config": {"num_workers": 1, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 50, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 50, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "Centralized_Critic_Model", "custom_model_config": {"env": "mpe", "env_args": {"continuous_actions": false, "max_cycles": 25, "map_name": "simple_world_comm"}, "mask_flag": false, "global_state_flag": false, "opp_action_in_cc": true, "agent_level_batch_update": false, "force_coop": false, "local_mode": false, "share_policy": "group", "evaluation_interval": 50, "framework": "torch", "num_workers": 1, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "checkpoint_freq": 20, "checkpoint_end": true, "restore_path": {"model_path": "", "params_path": ""}, "stop_iters": 9999999, "stop_timesteps": 2000000, "stop_reward": 999999, "seed": 321, "local_dir": "", "model_arch_args": {"hidden_state_size": 256, "core_arch": "mlp", "fc_layer": 2, "out_dim_fc_0": 128, "out_dim_fc_1": 64, "encode_layer": "128-256"}, "algorithm": "mappo", "space_obs": "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "space_act": "Discrete(20)", "num_agents": 6, "episode_limit": 25, "policy_mapping_info": {"simple_adversary": {"description": "one team attack, one team survive", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_crypto": {"description": "two team cooperate, one team attack", "team_prefix": ["eve_", "bob_", "alice_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_push": {"description": "one team target on landmark, one team attack", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_tag": {"description": "one team attack, one team survive", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_spread": {"description": "one team cooperate", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}, "simple_reference": {"description": "one team cooperate", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}, "simple_world_comm": {"description": "two team cooperate and attack, one team survive", "team_prefix": ["adversary_", "leadadversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_speaker_listener": {"description": "two team cooperate", "team_prefix": ["speaker_", "listener_"], "all_agents_one_policy": true, "one_agent_one_policy": true}}, "agent_name_ls": ["leadadversary_0", "adversary_0", "adversary_1", "adversary_2", "agent_0", "agent_1"]}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "mpe_simple_world_comm", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 50, "evaluation_num_episodes": 10, "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 100, "min_iter_time_s": 0, "timesteps_per_iteration": 0, "seed": 321, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_adversary_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}], "policy_leadadversary_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}], "policy_agent_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x7f88aff9ca60>", "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "simple_optimizer": false, "monitor": -1, "use_critic": true, "use_gae": true, "lambda": 1.0, "kl_coeff": 0.2, "sgd_minibatch_size": 50, "shuffle_sequences": true, "num_sgd_iter": 2, "lr_schedule": null, "vf_loss_coeff": 1.0, "entropy_coeff": 0.01, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": null, "kl_target": 0.01, "vf_share_layers": -1}, "time_since_restore": 12.520538330078125, "timesteps_since_restore": 0, "iterations_since_restore": 30, "perf": {"cpu_util_percent": 18.5, "ram_util_percent": 46.6}}
{"episode_reward_max": 98.42627085353728, "episode_reward_min": -155.78308767969756, "episode_reward_mean": -19.87354217726406, "episode_len_mean": 25.0, "episode_media": {}, "episodes_this_iter": 2, "policy_reward_min": {"policy_leadadversary_": -4.542143749578077, "policy_adversary_": -4.621733620085473, "policy_agent_": -96.35415811344997}, "policy_reward_max": {"policy_leadadversary_": 33.854903602692595, "policy_adversary_": 34.36826010203567, "policy_agent_": 27.837397306389533}, "policy_reward_mean": {"policy_leadadversary_": 0.8006798692641371, "policy_adversary_": 1.0560830546318525, "policy_agent_": -11.921235605211873}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-105.16393252669111, -38.08425587795351, -8.179283354852107, -7.994660195117557, 17.029253709543042, -62.95754389372587, -21.306978110434503, -9.963224993494906, -35.6723275216737, -7.801818117525467, 9.04258457925305, -9.16790291343267, -23.11610041380425, -76.64256918047145, 19.012759487513534, -18.237441060454206, -26.428737443182527, -42.52695238562914, -72.55115007329658, -11.5502931963818, -9.816421651401027, -155.78308767969756, -37.19573589166182, -10.625310490304468, 20.61462187851487, 56.41119583016825, -13.315507414957573, -12.10170378283629, 98.42627085353728, -33.048655056364915, 7.3217210673286806, 7.327237400716829, -23.088022125278687, 21.452118299951458, -34.22571107500617, -37.05712090715862, -7.758585683990852, 7.429427413636523, -64.55081708876132, -72.81867023241337, -57.907825000034876, 3.058827652902239, -9.394727393314803, 5.757473025486409, -37.32645149477665, -38.73999230490706, -4.66851184524558, -7.6165725867276715, -59.64437090063567, 21.591789825831185, 3.2949798405375788, 16.422305289358494, -10.54155505870399, -9.468515043193399, -66.93944423241335, -53.48970763891736, -8.40918682162661, -60.00108178641989, 27.597611762832496, 16.465750862251895, -2.5964712051049843, -74.94061011975946], "episode_lengths": [25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25], "policy_policy_leadadversary__reward": [-2.399035557624381, 8.302786016962129, -1.9111450930988323, -1.90667729707557, -4.038309224748798, -1.1928023284527944, 6.938235712341814, -1.8308626607889866, 8.940230507526055, -2.0965552281416944, 9.03154180074615, -1.2461341680319717, -4.388542193694188, -3.847920286524414, 3.5200136941709386, -2.964544320613795, -2.9630602344890113, -2.5514853414160785, -3.423786335468951, -1.7669051568509833, -1.2819197880175521, -2.793092838548524, -2.715631555274084, -1.888458905121377, 8.491376358690058, 19.490398201983847, -3.414607305270243, -1.9683014538118688, 33.854903602692595, -2.2506002232734774, 2.321446715633231, 2.5983351813573012, -1.5375524704754444, 8.954483912319503, -1.838966463801028, -4.542143749578077, -1.887097192016523, -1.9669168715798406, -4.259228446259269, -3.418530040554258, -1.6405950365018764, 3.4187604704462027, -2.0529470784989825, 2.950861039809551, -1.6417408125125332, -1.173521562486733, -0.8453547453422087, -1.5838705663315764, 2.3471854601712994, 9.166275375913555, 1.4773564118411457, 7.8560100175026655, -2.482234807213631, -1.9107811473384637, -2.8311678664115933, -2.205918894528226, -3.1371567855259825, -3.22134663995534, 4.258662230969782, 8.190735554028539, -2.390285673170561, -1.0597120243101323], "policy_policy_adversary__reward": [-1.6741618220129826, -2.2223381552451884, -1.7978662959280882, 6.195230400528265, 9.085335341319167, 9.161857338996002, -1.1497964664252314, -0.7244895816458103, -2.705072794727737, -1.9452619286739838, -0.5002713058248459, -2.428351018980954, -2.073914420541661, -2.491769450548833, -1.928789335869684, -1.6434950773450425, -1.1869343121273628, -3.094481311936268, 9.22635362970437, 8.437625312282139, 8.558180033954198, -1.474364867288304, -2.699172611779865, -1.9577497509222836, 8.892856901464825, 8.219320737803885, 8.128349998386524, -2.4694792844144473, -0.6928708713293986, -1.5511211993405283, 6.420991744018833, 7.748318394913137, 8.185932035407232, -2.473275836552458, -0.5674346699607982, -2.7263543698194392, -0.5909371354705603, -1.2690244550254388, -1.8793974687070758, -2.8351443039978297, -3.923014083123016, -2.3079879234678007, 4.100268481179938, 3.027702044631114, 4.02297814215707, -2.0777870863086294, -1.9840069073615636, -1.669300530018625, -3.9338029974868833, -1.129238869347308, -1.3580459203598705, -2.250033013063289, -1.3601789265843973, -2.654726573595358, -1.7265988852649663, -3.7396060163471576, -1.820129784531345, -2.9201218301303506, -1.7090159550536768, -2.424621160302172, -2.53712801256559, -2.876435417206376, -1.4196451923447468, -1.5248898471676176, -2.768539034385942, -1.768760251247098, -3.013244773720978, -3.0025095397079458, -1.7121641935407885, -1.6493536919918832, -3.9070691940185345, -1.3224203607070766, 7.256290170226535, 8.009821795532776, 9.511926850083018, 19.35511620124706, 18.84000636214317, 19.519538085643973, -1.4999996116110483, -3.8952631049612574, -2.070937464898907, -2.546883520298444, -2.9137085640161007, -2.9466401471505344, 33.50823641156576, 34.36826010203567, 32.62555861085578, -1.594179646955657, -0.7648866358471844, -2.61833002303639, 3.932860003891398, 4.357704493462412, 3.6291405799201555, 3.6581468826299237, 2.9729168454181787, 4.531212041059542, -1.4324294123408556, -3.5804977490861387, -4.292088229889296, 7.364570933829991, 8.207346016991234, 8.425087820102204, -1.4899061486169511, -0.888369164140079, -2.1729673605733337, -0.9880730925424175, -0.8979613232266417, -3.2498547972579335, -1.4440590307296355, -0.9465905128076592, -1.6162837096317748, -1.481425948122367, -1.476298099855236, -1.1826997013762748, -3.7611817019384506, -0.6523852938800511, -1.0971635214558346, -1.0180042767264177, -1.6408081573998603, -2.3188704379693, -1.7534750970959454, -2.0578000943715407, -3.478190868351961, 2.197108121194587, 4.007253584005375, 0.8085618433346088, -2.7249530991388955, -0.7346005542861018, -2.6911144456741787, 4.318448499420746, 3.402533710413069, 2.6100128528088105, -4.621733620085473, -1.5859334919515493, -3.592891398226516, -1.1648055450308883, -1.7780974600334736, -1.7707289994905449, -0.5398622257926751, -1.2153213731765846, -1.0242404139779537, -0.5400433557265615, -1.6776735020958116, -2.1234977001066433, 1.402346749557846, 3.0330542043038955, 4.341864249270812, 8.965739918088033, 7.44370158149564, 8.238781338135515, 4.30390780446801, 1.0151693211549975, 3.7941534640047134, 6.746279317105572, 6.515020187034392, 9.431284562017067, -1.954449545579552, -2.1242511296450366, -1.8472674506123379, -3.600484033802333, -0.9036071713321907, -1.2169420701381708, -3.461940161656796, -0.7046985620528773, -2.059661620717326, -2.37213281426346, -2.8425345768730166, -1.6191734163631457, -1.2008251292794836, -1.0586588046576018, -0.9834922225627654, -3.4340751000710297, -3.9758490796042922, -3.9291829536692404, 2.716099370548289, 3.022756736316401, 4.46027632001432, 8.389422832182536, 8.331942051357524, 9.302317380968413, -3.1651149190836247, -1.9632954556730522, -3.6432896192759783, -0.5685048125393075, -0.8833907751934374, -1.5414014500676259], "policy_policy_agent__reward": [-0.7163725824305123, -96.35415811344997, -3.617249732269532, -67.21221524348954, -1.1897859301531337, -0.49899348880136807, -0.7213000577969194, -0.49279858676528443, 27.837397306389533, -0.27536116513751435, -1.8093468032775135, -54.03048406058688, -42.95203037708829, -11.515342421628725, -1.0331583960521016, -0.9679167066633642, -0.927224282567324, -68.92586138428767, -0.4035124455295982, -0.5882790887698023, -0.3080346699354489, -22.036164725896853, -1.0213367048557638, -1.1333671642122476, -0.6140299114418961, -14.374169249465094, -0.5039952235368317, -63.22450735982157, 9.76231673232199, -5.42051960694752, -0.38719184121053235, -9.154610374941065, -1.2188982803065158, -15.825691141192937, -32.77419517584195, -0.9363333551280546, -61.65847002433208, -0.182559027352075, -1.3441869339869565, -1.3854421600576674, -1.2320086747231598, -0.4692845665436012, -91.95162118944084, -54.97618451890754, -25.81192265544753, -0.9402631739704812, -0.7369098771347249, -1.1210984613308634, -1.2206295166227459, -11.434163779394773, -20.24663057019898, -0.5472324506508123, -0.7910180498339603, -1.6436818783821536, -0.8500054566128976, -0.8761646409464355, -20.63989374553544, -15.290794128077069, -1.1518478368502902, -24.66881069040191, -5.722489091533723, -1.1969416340448025, -5.930741298443036, -0.5026322513050725, -11.882773789857106, -0.36268047362983874, -1.1761058559306894, -10.323264527360775, -27.075190531297906, -0.7603114065768609, -0.6153024430104596, -26.76378550154309, -0.8350650603929229, -1.029490178412334, -4.33587304506435, 17.872641079634594, -13.677754358552278, -41.103103766675446, 2.9893260231465204, -67.41178334291008, -48.50968190580077, -0.4680819979127372, -5.722255746040154, -1.6506006200383874, -0.5229746771561022, -0.6681375385605484, -1.2405475750286472, -6.283835501937115, -0.7738420813283601, -25.11031009067222, -32.51991083703785, -0.3329279008275307, -0.6073557419411857, -0.4363773450149738, -0.404379749566543, -1.287107712900539, -64.61098833998355, -6.157833223955975, -1.401673562237826, -10.821034825563737, -1.2154147079260624, -6.080192453005229, -0.49372985197767705, -13.632558942323527, -0.7359191942324639, -1.3974329314209728, -1.2580379613038053, -0.5786626592784377, -45.69097945815503, -12.19099656341974, -43.43331183406106, -1.0166361028284194, -0.8411124476865421, -1.1879414319142383, -44.175348625877135, -1.2652793872428503, -1.7295602169250743, 14.86937732190877, -17.372234795940894, -0.37643216034422006, -1.1071602455961107, 9.672674707694336, -48.33547855557615, -22.552122502072827]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.5521903532001672, "mean_inference_ms": 2.719507337732179, "mean_action_processing_ms": 0.10510105324203677, "mean_env_wait_ms": 3.3112082237327938, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 1, "timesteps_total": 1550, "timesteps_this_iter": 0, "agent_timesteps_total": 9300, "timers": {"sample_time_ms": 455.602, "sample_throughput": 109.745, "load_time_ms": 0.474, "load_throughput": 105533.011, "learn_time_ms": 72.087, "learn_throughput": 693.606, "update_time_ms": 1.799}, "info": {"learner": {"policy_leadadversary_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 1.8626451492309571e-10, "cur_lr": 0.0005, "total_loss": 1.1272319555282593, "policy_loss": -0.0010922669852160283, "vf_loss": 1.1578488945960999, "vf_explained_var": 0.20978310704231262, "kl": 1.6524648733806657e-05, "entropy": 2.9524747133255005, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_adversary_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 1.862645149230957e-10, "cur_lr": 0.0005, "total_loss": 1.3641142845153809, "policy_loss": -0.008851847300926844, "vf_loss": 1.4012068212032318, "vf_explained_var": -0.05150705575942993, "kl": 0.0014136262663895967, "entropy": 2.8240681091944375, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_agent_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 1.8626451492309571e-10, "cur_lr": 0.0005, "total_loss": 173.9496898651123, "policy_loss": -0.004672005772590637, "vf_loss": 173.98351287841797, "vf_explained_var": -0.06602755188941956, "kl": 0.00032696084513701706, "entropy": 2.915328800678253, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}}, "num_steps_sampled": 1550, "num_agent_steps_sampled": 9300, "num_steps_trained": 1550, "num_agent_steps_trained": 9300, "num_steps_trained_this_iter": 0}, "done": false, "episodes_total": 62, "training_iteration": 31, "trial_id": "f5f5f_00000", "experiment_id": "d79be65b63864cf3aab1c0ed35a1f497", "date": "2025-03-09_18-29-07", "timestamp": 1741541347, "time_this_iter_s": 0.4333639144897461, "time_total_s": 12.953902244567871, "pid": 48554, "hostname": "julien-PC", "node_ip": "192.168.89.58", "config": {"num_workers": 1, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 50, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 50, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "Centralized_Critic_Model", "custom_model_config": {"env": "mpe", "env_args": {"continuous_actions": false, "max_cycles": 25, "map_name": "simple_world_comm"}, "mask_flag": false, "global_state_flag": false, "opp_action_in_cc": true, "agent_level_batch_update": false, "force_coop": false, "local_mode": false, "share_policy": "group", "evaluation_interval": 50, "framework": "torch", "num_workers": 1, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "checkpoint_freq": 20, "checkpoint_end": true, "restore_path": {"model_path": "", "params_path": ""}, "stop_iters": 9999999, "stop_timesteps": 2000000, "stop_reward": 999999, "seed": 321, "local_dir": "", "model_arch_args": {"hidden_state_size": 256, "core_arch": "mlp", "fc_layer": 2, "out_dim_fc_0": 128, "out_dim_fc_1": 64, "encode_layer": "128-256"}, "algorithm": "mappo", "space_obs": "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "space_act": "Discrete(20)", "num_agents": 6, "episode_limit": 25, "policy_mapping_info": {"simple_adversary": {"description": "one team attack, one team survive", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_crypto": {"description": "two team cooperate, one team attack", "team_prefix": ["eve_", "bob_", "alice_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_push": {"description": "one team target on landmark, one team attack", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_tag": {"description": "one team attack, one team survive", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_spread": {"description": "one team cooperate", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}, "simple_reference": {"description": "one team cooperate", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}, "simple_world_comm": {"description": "two team cooperate and attack, one team survive", "team_prefix": ["adversary_", "leadadversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_speaker_listener": {"description": "two team cooperate", "team_prefix": ["speaker_", "listener_"], "all_agents_one_policy": true, "one_agent_one_policy": true}}, "agent_name_ls": ["leadadversary_0", "adversary_0", "adversary_1", "adversary_2", "agent_0", "agent_1"]}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "mpe_simple_world_comm", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 50, "evaluation_num_episodes": 10, "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 100, "min_iter_time_s": 0, "timesteps_per_iteration": 0, "seed": 321, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_adversary_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}], "policy_leadadversary_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}], "policy_agent_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x7f88aff1a820>", "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "simple_optimizer": false, "monitor": -1, "use_critic": true, "use_gae": true, "lambda": 1.0, "kl_coeff": 0.2, "sgd_minibatch_size": 50, "shuffle_sequences": true, "num_sgd_iter": 2, "lr_schedule": null, "vf_loss_coeff": 1.0, "entropy_coeff": 0.01, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": null, "kl_target": 0.01, "vf_share_layers": -1}, "time_since_restore": 12.953902244567871, "timesteps_since_restore": 0, "iterations_since_restore": 31, "perf": {}}
{"episode_reward_max": 98.42627085353728, "episode_reward_min": -155.78308767969756, "episode_reward_mean": -20.30106901660203, "episode_len_mean": 25.0, "episode_media": {}, "episodes_this_iter": 2, "policy_reward_min": {"policy_leadadversary_": -4.542143749578077, "policy_adversary_": -4.621733620085473, "policy_agent_": -96.35415811344997}, "policy_reward_max": {"policy_leadadversary_": 33.854903602692595, "policy_adversary_": 34.36826010203567, "policy_agent_": 27.837397306389533}, "policy_reward_mean": {"policy_leadadversary_": 0.6851231208464315, "policy_adversary_": 0.9670125789555062, "policy_agent_": -11.94361493715749}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-105.16393252669111, -38.08425587795351, -8.179283354852107, -7.994660195117557, 17.029253709543042, -62.95754389372587, -21.306978110434503, -9.963224993494906, -35.6723275216737, -7.801818117525467, 9.04258457925305, -9.16790291343267, -23.11610041380425, -76.64256918047145, 19.012759487513534, -18.237441060454206, -26.428737443182527, -42.52695238562914, -72.55115007329658, -11.5502931963818, -9.816421651401027, -155.78308767969756, -37.19573589166182, -10.625310490304468, 20.61462187851487, 56.41119583016825, -13.315507414957573, -12.10170378283629, 98.42627085353728, -33.048655056364915, 7.3217210673286806, 7.327237400716829, -23.088022125278687, 21.452118299951458, -34.22571107500617, -37.05712090715862, -7.758585683990852, 7.429427413636523, -64.55081708876132, -72.81867023241337, -57.907825000034876, 3.058827652902239, -9.394727393314803, 5.757473025486409, -37.32645149477665, -38.73999230490706, -4.66851184524558, -7.6165725867276715, -59.64437090063567, 21.591789825831185, 3.2949798405375788, 16.422305289358494, -10.54155505870399, -9.468515043193399, -66.93944423241335, -53.48970763891736, -8.40918682162661, -60.00108178641989, 27.597611762832496, 16.465750862251895, -2.5964712051049843, -74.94061011975946, -34.58238754191785, -32.52641453024035], "episode_lengths": [25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25], "policy_policy_leadadversary__reward": [-2.399035557624381, 8.302786016962129, -1.9111450930988323, -1.90667729707557, -4.038309224748798, -1.1928023284527944, 6.938235712341814, -1.8308626607889866, 8.940230507526055, -2.0965552281416944, 9.03154180074615, -1.2461341680319717, -4.388542193694188, -3.847920286524414, 3.5200136941709386, -2.964544320613795, -2.9630602344890113, -2.5514853414160785, -3.423786335468951, -1.7669051568509833, -1.2819197880175521, -2.793092838548524, -2.715631555274084, -1.888458905121377, 8.491376358690058, 19.490398201983847, -3.414607305270243, -1.9683014538118688, 33.854903602692595, -2.2506002232734774, 2.321446715633231, 2.5983351813573012, -1.5375524704754444, 8.954483912319503, -1.838966463801028, -4.542143749578077, -1.887097192016523, -1.9669168715798406, -4.259228446259269, -3.418530040554258, -1.6405950365018764, 3.4187604704462027, -2.0529470784989825, 2.950861039809551, -1.6417408125125332, -1.173521562486733, -0.8453547453422087, -1.5838705663315764, 2.3471854601712994, 9.166275375913555, 1.4773564118411457, 7.8560100175026655, -2.482234807213631, -1.9107811473384637, -2.8311678664115933, -2.205918894528226, -3.1371567855259825, -3.22134663995534, 4.258662230969782, 8.190735554028539, -2.390285673170561, -1.0597120243101323, -2.315077393165926, -3.4791947670389614], "policy_policy_adversary__reward": [-1.6741618220129826, -2.2223381552451884, -1.7978662959280882, 6.195230400528265, 9.085335341319167, 9.161857338996002, -1.1497964664252314, -0.7244895816458103, -2.705072794727737, -1.9452619286739838, -0.5002713058248459, -2.428351018980954, -2.073914420541661, -2.491769450548833, -1.928789335869684, -1.6434950773450425, -1.1869343121273628, -3.094481311936268, 9.22635362970437, 8.437625312282139, 8.558180033954198, -1.474364867288304, -2.699172611779865, -1.9577497509222836, 8.892856901464825, 8.219320737803885, 8.128349998386524, -2.4694792844144473, -0.6928708713293986, -1.5511211993405283, 6.420991744018833, 7.748318394913137, 8.185932035407232, -2.473275836552458, -0.5674346699607982, -2.7263543698194392, -0.5909371354705603, -1.2690244550254388, -1.8793974687070758, -2.8351443039978297, -3.923014083123016, -2.3079879234678007, 4.100268481179938, 3.027702044631114, 4.02297814215707, -2.0777870863086294, -1.9840069073615636, -1.669300530018625, -3.9338029974868833, -1.129238869347308, -1.3580459203598705, -2.250033013063289, -1.3601789265843973, -2.654726573595358, -1.7265988852649663, -3.7396060163471576, -1.820129784531345, -2.9201218301303506, -1.7090159550536768, -2.424621160302172, -2.53712801256559, -2.876435417206376, -1.4196451923447468, -1.5248898471676176, -2.768539034385942, -1.768760251247098, -3.013244773720978, -3.0025095397079458, -1.7121641935407885, -1.6493536919918832, -3.9070691940185345, -1.3224203607070766, 7.256290170226535, 8.009821795532776, 9.511926850083018, 19.35511620124706, 18.84000636214317, 19.519538085643973, -1.4999996116110483, -3.8952631049612574, -2.070937464898907, -2.546883520298444, -2.9137085640161007, -2.9466401471505344, 33.50823641156576, 34.36826010203567, 32.62555861085578, -1.594179646955657, -0.7648866358471844, -2.61833002303639, 3.932860003891398, 4.357704493462412, 3.6291405799201555, 3.6581468826299237, 2.9729168454181787, 4.531212041059542, -1.4324294123408556, -3.5804977490861387, -4.292088229889296, 7.364570933829991, 8.207346016991234, 8.425087820102204, -1.4899061486169511, -0.888369164140079, -2.1729673605733337, -0.9880730925424175, -0.8979613232266417, -3.2498547972579335, -1.4440590307296355, -0.9465905128076592, -1.6162837096317748, -1.481425948122367, -1.476298099855236, -1.1826997013762748, -3.7611817019384506, -0.6523852938800511, -1.0971635214558346, -1.0180042767264177, -1.6408081573998603, -2.3188704379693, -1.7534750970959454, -2.0578000943715407, -3.478190868351961, 2.197108121194587, 4.007253584005375, 0.8085618433346088, -2.7249530991388955, -0.7346005542861018, -2.6911144456741787, 4.318448499420746, 3.402533710413069, 2.6100128528088105, -4.621733620085473, -1.5859334919515493, -3.592891398226516, -1.1648055450308883, -1.7780974600334736, -1.7707289994905449, -0.5398622257926751, -1.2153213731765846, -1.0242404139779537, -0.5400433557265615, -1.6776735020958116, -2.1234977001066433, 1.402346749557846, 3.0330542043038955, 4.341864249270812, 8.965739918088033, 7.44370158149564, 8.238781338135515, 4.30390780446801, 1.0151693211549975, 3.7941534640047134, 6.746279317105572, 6.515020187034392, 9.431284562017067, -1.954449545579552, -2.1242511296450366, -1.8472674506123379, -3.600484033802333, -0.9036071713321907, -1.2169420701381708, -3.461940161656796, -0.7046985620528773, -2.059661620717326, -2.37213281426346, -2.8425345768730166, -1.6191734163631457, -1.2008251292794836, -1.0586588046576018, -0.9834922225627654, -3.4340751000710297, -3.9758490796042922, -3.9291829536692404, 2.716099370548289, 3.022756736316401, 4.46027632001432, 8.389422832182536, 8.331942051357524, 9.302317380968413, -3.1651149190836247, -1.9632954556730522, -3.6432896192759783, -0.5685048125393075, -0.8833907751934374, -1.5414014500676259, -1.6534231889245965, -2.438729953127392, -1.804195395154173, -2.117274168020702, -2.2385839459439816, -0.5128263508965348], "policy_policy_agent__reward": [-0.7163725824305123, -96.35415811344997, -3.617249732269532, -67.21221524348954, -1.1897859301531337, -0.49899348880136807, -0.7213000577969194, -0.49279858676528443, 27.837397306389533, -0.27536116513751435, -1.8093468032775135, -54.03048406058688, -42.95203037708829, -11.515342421628725, -1.0331583960521016, -0.9679167066633642, -0.927224282567324, -68.92586138428767, -0.4035124455295982, -0.5882790887698023, -0.3080346699354489, -22.036164725896853, -1.0213367048557638, -1.1333671642122476, -0.6140299114418961, -14.374169249465094, -0.5039952235368317, -63.22450735982157, 9.76231673232199, -5.42051960694752, -0.38719184121053235, -9.154610374941065, -1.2188982803065158, -15.825691141192937, -32.77419517584195, -0.9363333551280546, -61.65847002433208, -0.182559027352075, -1.3441869339869565, -1.3854421600576674, -1.2320086747231598, -0.4692845665436012, -91.95162118944084, -54.97618451890754, -25.81192265544753, -0.9402631739704812, -0.7369098771347249, -1.1210984613308634, -1.2206295166227459, -11.434163779394773, -20.24663057019898, -0.5472324506508123, -0.7910180498339603, -1.6436818783821536, -0.8500054566128976, -0.8761646409464355, -20.63989374553544, -15.290794128077069, -1.1518478368502902, -24.66881069040191, -5.722489091533723, -1.1969416340448025, -5.930741298443036, -0.5026322513050725, -11.882773789857106, -0.36268047362983874, -1.1761058559306894, -10.323264527360775, -27.075190531297906, -0.7603114065768609, -0.6153024430104596, -26.76378550154309, -0.8350650603929229, -1.029490178412334, -4.33587304506435, 17.872641079634594, -13.677754358552278, -41.103103766675446, 2.9893260231465204, -67.41178334291008, -48.50968190580077, -0.4680819979127372, -5.722255746040154, -1.6506006200383874, -0.5229746771561022, -0.6681375385605484, -1.2405475750286472, -6.283835501937115, -0.7738420813283601, -25.11031009067222, -32.51991083703785, -0.3329279008275307, -0.6073557419411857, -0.4363773450149738, -0.404379749566543, -1.287107712900539, -64.61098833998355, -6.157833223955975, -1.401673562237826, -10.821034825563737, -1.2154147079260624, -6.080192453005229, -0.49372985197767705, -13.632558942323527, -0.7359191942324639, -1.3974329314209728, -1.2580379613038053, -0.5786626592784377, -45.69097945815503, -12.19099656341974, -43.43331183406106, -1.0166361028284194, -0.8411124476865421, -1.1879414319142383, -44.175348625877135, -1.2652793872428503, -1.7295602169250743, 14.86937732190877, -17.372234795940894, -0.37643216034422006, -1.1071602455961107, 9.672674707694336, -48.33547855557615, -22.552122502072827, -25.597829964433096, -0.7731316471126801, -23.05360110756041, -1.1249341907797623]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.5516693394242145, "mean_inference_ms": 2.7195658336412483, "mean_action_processing_ms": 0.10508478829140856, "mean_env_wait_ms": 3.31211372546121, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 1, "timesteps_total": 1600, "timesteps_this_iter": 0, "agent_timesteps_total": 9600, "timers": {"sample_time_ms": 451.277, "sample_throughput": 110.797, "load_time_ms": 0.469, "load_throughput": 106530.123, "learn_time_ms": 71.769, "learn_throughput": 696.678, "update_time_ms": 1.809}, "info": {"learner": {"policy_leadadversary_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 9.313225746154786e-11, "cur_lr": 0.0005, "total_loss": 2.478871703147888, "policy_loss": -0.002537372019141948, "vf_loss": 2.510625123977661, "vf_explained_var": 0.012240111827850342, "kl": 4.1897483361807986e-05, "entropy": 2.9216017723083496, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_adversary_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 9.313225746154784e-11, "cur_lr": 0.0005, "total_loss": 1.6385506478448708, "policy_loss": -0.005520336329936981, "vf_loss": 1.6723754107952118, "vf_explained_var": -0.016980677843093872, "kl": 0.000808909174724981, "entropy": 2.830439885457357, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_agent_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 9.313225746154786e-11, "cur_lr": 0.0005, "total_loss": 100.42593574523926, "policy_loss": -0.003675432875752449, "vf_loss": 100.4582576751709, "vf_explained_var": -0.0032188743352890015, "kl": 0.00027265957085820247, "entropy": 2.8651849031448364, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}}, "num_steps_sampled": 1600, "num_agent_steps_sampled": 9600, "num_steps_trained": 1600, "num_agent_steps_trained": 9600, "num_steps_trained_this_iter": 0}, "done": false, "episodes_total": 64, "training_iteration": 32, "trial_id": "f5f5f_00000", "experiment_id": "d79be65b63864cf3aab1c0ed35a1f497", "date": "2025-03-09_18-29-07", "timestamp": 1741541347, "time_this_iter_s": 0.3923192024230957, "time_total_s": 13.346221446990967, "pid": 48554, "hostname": "julien-PC", "node_ip": "192.168.89.58", "config": {"num_workers": 1, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 50, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 50, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "Centralized_Critic_Model", "custom_model_config": {"env": "mpe", "env_args": {"continuous_actions": false, "max_cycles": 25, "map_name": "simple_world_comm"}, "mask_flag": false, "global_state_flag": false, "opp_action_in_cc": true, "agent_level_batch_update": false, "force_coop": false, "local_mode": false, "share_policy": "group", "evaluation_interval": 50, "framework": "torch", "num_workers": 1, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "checkpoint_freq": 20, "checkpoint_end": true, "restore_path": {"model_path": "", "params_path": ""}, "stop_iters": 9999999, "stop_timesteps": 2000000, "stop_reward": 999999, "seed": 321, "local_dir": "", "model_arch_args": {"hidden_state_size": 256, "core_arch": "mlp", "fc_layer": 2, "out_dim_fc_0": 128, "out_dim_fc_1": 64, "encode_layer": "128-256"}, "algorithm": "mappo", "space_obs": "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "space_act": "Discrete(20)", "num_agents": 6, "episode_limit": 25, "policy_mapping_info": {"simple_adversary": {"description": "one team attack, one team survive", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_crypto": {"description": "two team cooperate, one team attack", "team_prefix": ["eve_", "bob_", "alice_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_push": {"description": "one team target on landmark, one team attack", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_tag": {"description": "one team attack, one team survive", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_spread": {"description": "one team cooperate", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}, "simple_reference": {"description": "one team cooperate", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}, "simple_world_comm": {"description": "two team cooperate and attack, one team survive", "team_prefix": ["adversary_", "leadadversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_speaker_listener": {"description": "two team cooperate", "team_prefix": ["speaker_", "listener_"], "all_agents_one_policy": true, "one_agent_one_policy": true}}, "agent_name_ls": ["leadadversary_0", "adversary_0", "adversary_1", "adversary_2", "agent_0", "agent_1"]}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "mpe_simple_world_comm", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 50, "evaluation_num_episodes": 10, "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 100, "min_iter_time_s": 0, "timesteps_per_iteration": 0, "seed": 321, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_adversary_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}], "policy_leadadversary_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}], "policy_agent_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x7f88afdd7700>", "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "simple_optimizer": false, "monitor": -1, "use_critic": true, "use_gae": true, "lambda": 1.0, "kl_coeff": 0.2, "sgd_minibatch_size": 50, "shuffle_sequences": true, "num_sgd_iter": 2, "lr_schedule": null, "vf_loss_coeff": 1.0, "entropy_coeff": 0.01, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": null, "kl_target": 0.01, "vf_share_layers": -1}, "time_since_restore": 13.346221446990967, "timesteps_since_restore": 0, "iterations_since_restore": 32, "perf": {"cpu_util_percent": 20.1, "ram_util_percent": 46.6}}
{"episode_reward_max": 98.42627085353728, "episode_reward_min": -155.78308767969756, "episode_reward_mean": -19.656700880805477, "episode_len_mean": 25.0, "episode_media": {}, "episodes_this_iter": 2, "policy_reward_min": {"policy_leadadversary_": -4.542143749578077, "policy_adversary_": -4.621733620085473, "policy_agent_": -96.35415811344997}, "policy_reward_max": {"policy_leadadversary_": 33.854903602692595, "policy_adversary_": 34.36826010203567, "policy_agent_": 27.837397306389533}, "policy_reward_mean": {"policy_leadadversary_": 0.6782364057299864, "policy_adversary_": 0.9794098314924011, "policy_agent_": -11.636583390506333}, "custom_metrics": {}, "hist_stats": {"episode_reward": [-105.16393252669111, -38.08425587795351, -8.179283354852107, -7.994660195117557, 17.029253709543042, -62.95754389372587, -21.306978110434503, -9.963224993494906, -35.6723275216737, -7.801818117525467, 9.04258457925305, -9.16790291343267, -23.11610041380425, -76.64256918047145, 19.012759487513534, -18.237441060454206, -26.428737443182527, -42.52695238562914, -72.55115007329658, -11.5502931963818, -9.816421651401027, -155.78308767969756, -37.19573589166182, -10.625310490304468, 20.61462187851487, 56.41119583016825, -13.315507414957573, -12.10170378283629, 98.42627085353728, -33.048655056364915, 7.3217210673286806, 7.327237400716829, -23.088022125278687, 21.452118299951458, -34.22571107500617, -37.05712090715862, -7.758585683990852, 7.429427413636523, -64.55081708876132, -72.81867023241337, -57.907825000034876, 3.058827652902239, -9.394727393314803, 5.757473025486409, -37.32645149477665, -38.73999230490706, -4.66851184524558, -7.6165725867276715, -59.64437090063567, 21.591789825831185, 3.2949798405375788, 16.422305289358494, -10.54155505870399, -9.468515043193399, -66.93944423241335, -53.48970763891736, -8.40918682162661, -60.00108178641989, 27.597611762832496, 16.465750862251895, -2.5964712051049843, -74.94061011975946, -34.58238754191785, -32.52641453024035, 8.50664771905189, -6.580488789683591], "episode_lengths": [25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25, 25], "policy_policy_leadadversary__reward": [-2.399035557624381, 8.302786016962129, -1.9111450930988323, -1.90667729707557, -4.038309224748798, -1.1928023284527944, 6.938235712341814, -1.8308626607889866, 8.940230507526055, -2.0965552281416944, 9.03154180074615, -1.2461341680319717, -4.388542193694188, -3.847920286524414, 3.5200136941709386, -2.964544320613795, -2.9630602344890113, -2.5514853414160785, -3.423786335468951, -1.7669051568509833, -1.2819197880175521, -2.793092838548524, -2.715631555274084, -1.888458905121377, 8.491376358690058, 19.490398201983847, -3.414607305270243, -1.9683014538118688, 33.854903602692595, -2.2506002232734774, 2.321446715633231, 2.5983351813573012, -1.5375524704754444, 8.954483912319503, -1.838966463801028, -4.542143749578077, -1.887097192016523, -1.9669168715798406, -4.259228446259269, -3.418530040554258, -1.6405950365018764, 3.4187604704462027, -2.0529470784989825, 2.950861039809551, -1.6417408125125332, -1.173521562486733, -0.8453547453422087, -1.5838705663315764, 2.3471854601712994, 9.166275375913555, 1.4773564118411457, 7.8560100175026655, -2.482234807213631, -1.9107811473384637, -2.8311678664115933, -2.205918894528226, -3.1371567855259825, -3.22134663995534, 4.258662230969782, 8.190735554028539, -2.390285673170561, -1.0597120243101323, -2.315077393165926, -3.4791947670389614, 3.172353305618572, -2.256630261611079], "policy_policy_adversary__reward": [-1.6741618220129826, -2.2223381552451884, -1.7978662959280882, 6.195230400528265, 9.085335341319167, 9.161857338996002, -1.1497964664252314, -0.7244895816458103, -2.705072794727737, -1.9452619286739838, -0.5002713058248459, -2.428351018980954, -2.073914420541661, -2.491769450548833, -1.928789335869684, -1.6434950773450425, -1.1869343121273628, -3.094481311936268, 9.22635362970437, 8.437625312282139, 8.558180033954198, -1.474364867288304, -2.699172611779865, -1.9577497509222836, 8.892856901464825, 8.219320737803885, 8.128349998386524, -2.4694792844144473, -0.6928708713293986, -1.5511211993405283, 6.420991744018833, 7.748318394913137, 8.185932035407232, -2.473275836552458, -0.5674346699607982, -2.7263543698194392, -0.5909371354705603, -1.2690244550254388, -1.8793974687070758, -2.8351443039978297, -3.923014083123016, -2.3079879234678007, 4.100268481179938, 3.027702044631114, 4.02297814215707, -2.0777870863086294, -1.9840069073615636, -1.669300530018625, -3.9338029974868833, -1.129238869347308, -1.3580459203598705, -2.250033013063289, -1.3601789265843973, -2.654726573595358, -1.7265988852649663, -3.7396060163471576, -1.820129784531345, -2.9201218301303506, -1.7090159550536768, -2.424621160302172, -2.53712801256559, -2.876435417206376, -1.4196451923447468, -1.5248898471676176, -2.768539034385942, -1.768760251247098, -3.013244773720978, -3.0025095397079458, -1.7121641935407885, -1.6493536919918832, -3.9070691940185345, -1.3224203607070766, 7.256290170226535, 8.009821795532776, 9.511926850083018, 19.35511620124706, 18.84000636214317, 19.519538085643973, -1.4999996116110483, -3.8952631049612574, -2.070937464898907, -2.546883520298444, -2.9137085640161007, -2.9466401471505344, 33.50823641156576, 34.36826010203567, 32.62555861085578, -1.594179646955657, -0.7648866358471844, -2.61833002303639, 3.932860003891398, 4.357704493462412, 3.6291405799201555, 3.6581468826299237, 2.9729168454181787, 4.531212041059542, -1.4324294123408556, -3.5804977490861387, -4.292088229889296, 7.364570933829991, 8.207346016991234, 8.425087820102204, -1.4899061486169511, -0.888369164140079, -2.1729673605733337, -0.9880730925424175, -0.8979613232266417, -3.2498547972579335, -1.4440590307296355, -0.9465905128076592, -1.6162837096317748, -1.481425948122367, -1.476298099855236, -1.1826997013762748, -3.7611817019384506, -0.6523852938800511, -1.0971635214558346, -1.0180042767264177, -1.6408081573998603, -2.3188704379693, -1.7534750970959454, -2.0578000943715407, -3.478190868351961, 2.197108121194587, 4.007253584005375, 0.8085618433346088, -2.7249530991388955, -0.7346005542861018, -2.6911144456741787, 4.318448499420746, 3.402533710413069, 2.6100128528088105, -4.621733620085473, -1.5859334919515493, -3.592891398226516, -1.1648055450308883, -1.7780974600334736, -1.7707289994905449, -0.5398622257926751, -1.2153213731765846, -1.0242404139779537, -0.5400433557265615, -1.6776735020958116, -2.1234977001066433, 1.402346749557846, 3.0330542043038955, 4.341864249270812, 8.965739918088033, 7.44370158149564, 8.238781338135515, 4.30390780446801, 1.0151693211549975, 3.7941534640047134, 6.746279317105572, 6.515020187034392, 9.431284562017067, -1.954449545579552, -2.1242511296450366, -1.8472674506123379, -3.600484033802333, -0.9036071713321907, -1.2169420701381708, -3.461940161656796, -0.7046985620528773, -2.059661620717326, -2.37213281426346, -2.8425345768730166, -1.6191734163631457, -1.2008251292794836, -1.0586588046576018, -0.9834922225627654, -3.4340751000710297, -3.9758490796042922, -3.9291829536692404, 2.716099370548289, 3.022756736316401, 4.46027632001432, 8.389422832182536, 8.331942051357524, 9.302317380968413, -3.1651149190836247, -1.9632954556730522, -3.6432896192759783, -0.5685048125393075, -0.8833907751934374, -1.5414014500676259, -1.6534231889245965, -2.438729953127392, -1.804195395154173, -2.117274168020702, -2.2385839459439816, -0.5128263508965348, 3.9706369675155546, 3.4960555199293473, 4.460879898147314, -2.56279306311277, -0.5209964890499409, -0.587051357391253], "policy_policy_agent__reward": [-0.7163725824305123, -96.35415811344997, -3.617249732269532, -67.21221524348954, -1.1897859301531337, -0.49899348880136807, -0.7213000577969194, -0.49279858676528443, 27.837397306389533, -0.27536116513751435, -1.8093468032775135, -54.03048406058688, -42.95203037708829, -11.515342421628725, -1.0331583960521016, -0.9679167066633642, -0.927224282567324, -68.92586138428767, -0.4035124455295982, -0.5882790887698023, -0.3080346699354489, -22.036164725896853, -1.0213367048557638, -1.1333671642122476, -0.6140299114418961, -14.374169249465094, -0.5039952235368317, -63.22450735982157, 9.76231673232199, -5.42051960694752, -0.38719184121053235, -9.154610374941065, -1.2188982803065158, -15.825691141192937, -32.77419517584195, -0.9363333551280546, -61.65847002433208, -0.182559027352075, -1.3441869339869565, -1.3854421600576674, -1.2320086747231598, -0.4692845665436012, -91.95162118944084, -54.97618451890754, -25.81192265544753, -0.9402631739704812, -0.7369098771347249, -1.1210984613308634, -1.2206295166227459, -11.434163779394773, -20.24663057019898, -0.5472324506508123, -0.7910180498339603, -1.6436818783821536, -0.8500054566128976, -0.8761646409464355, -20.63989374553544, -15.290794128077069, -1.1518478368502902, -24.66881069040191, -5.722489091533723, -1.1969416340448025, -5.930741298443036, -0.5026322513050725, -11.882773789857106, -0.36268047362983874, -1.1761058559306894, -10.323264527360775, -27.075190531297906, -0.7603114065768609, -0.6153024430104596, -26.76378550154309, -0.8350650603929229, -1.029490178412334, -4.33587304506435, 17.872641079634594, -13.677754358552278, -41.103103766675446, 2.9893260231465204, -67.41178334291008, -48.50968190580077, -0.4680819979127372, -5.722255746040154, -1.6506006200383874, -0.5229746771561022, -0.6681375385605484, -1.2405475750286472, -6.283835501937115, -0.7738420813283601, -25.11031009067222, -32.51991083703785, -0.3329279008275307, -0.6073557419411857, -0.4363773450149738, -0.404379749566543, -1.287107712900539, -64.61098833998355, -6.157833223955975, -1.401673562237826, -10.821034825563737, -1.2154147079260624, -6.080192453005229, -0.49372985197767705, -13.632558942323527, -0.7359191942324639, -1.3974329314209728, -1.2580379613038053, -0.5786626592784377, -45.69097945815503, -12.19099656341974, -43.43331183406106, -1.0166361028284194, -0.8411124476865421, -1.1879414319142383, -44.175348625877135, -1.2652793872428503, -1.7295602169250743, 14.86937732190877, -17.372234795940894, -0.37643216034422006, -1.1071602455961107, 9.672674707694336, -48.33547855557615, -22.552122502072827, -25.597829964433096, -0.7731316471126801, -23.05360110756041, -1.1249341907797623, -5.848719488981392, -0.7445584831775073, -0.4649450754489623, -0.1880725430695844]}, "sampler_perf": {"mean_raw_obs_processing_ms": 0.5511694599852945, "mean_inference_ms": 2.71948553657847, "mean_action_processing_ms": 0.10506331643428299, "mean_env_wait_ms": 3.31282316739778, "mean_env_render_ms": 0.0}, "off_policy_estimator": {}, "num_healthy_workers": 1, "timesteps_total": 1650, "timesteps_this_iter": 0, "agent_timesteps_total": 9900, "timers": {"sample_time_ms": 445.457, "sample_throughput": 112.244, "load_time_ms": 0.473, "load_throughput": 105596.777, "learn_time_ms": 71.912, "learn_throughput": 695.298, "update_time_ms": 1.791}, "info": {"learner": {"policy_leadadversary_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 4.656612873077393e-11, "cur_lr": 0.0005, "total_loss": 3.4376217126846313, "policy_loss": -0.0023489254713062735, "vf_loss": 3.469106674194336, "vf_explained_var": -0.03113102912902832, "kl": 5.0961609952382414e-05, "entropy": 2.9136141538619995, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_adversary_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 4.656612873077392e-11, "cur_lr": 0.0005, "total_loss": 3.6235801031192145, "policy_loss": -0.002040280650059382, "vf_loss": 3.654210908959309, "vf_explained_var": 0.03625932335853577, "kl": 0.0005381681892580777, "entropy": 2.859049598375956, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}, "policy_agent_": {"learner_stats": {"allreduce_latency": 0.0, "cur_kl_coeff": 4.656612873077393e-11, "cur_lr": 0.0005, "total_loss": 15.793254375457764, "policy_loss": -0.0043705180287361145, "vf_loss": 15.827337265014648, "vf_explained_var": -0.550746738910675, "kl": 7.547676814267934e-05, "entropy": 2.9710968136787415, "entropy_coeff": 0.01}, "model": {}, "custom_metrics": {}}}, "num_steps_sampled": 1650, "num_agent_steps_sampled": 9900, "num_steps_trained": 1650, "num_agent_steps_trained": 9900, "num_steps_trained_this_iter": 0}, "done": false, "episodes_total": 66, "training_iteration": 33, "trial_id": "f5f5f_00000", "experiment_id": "d79be65b63864cf3aab1c0ed35a1f497", "date": "2025-03-09_18-29-07", "timestamp": 1741541347, "time_this_iter_s": 0.39995288848876953, "time_total_s": 13.746174335479736, "pid": 48554, "hostname": "julien-PC", "node_ip": "192.168.89.58", "config": {"num_workers": 1, "num_envs_per_worker": 1, "create_env_on_driver": false, "rollout_fragment_length": 50, "batch_mode": "truncate_episodes", "gamma": 0.99, "lr": 0.0005, "train_batch_size": 50, "model": {"_use_default_native_models": false, "_disable_preprocessor_api": false, "fcnet_hiddens": [256, 256], "fcnet_activation": "tanh", "conv_filters": null, "conv_activation": "relu", "post_fcnet_hiddens": [], "post_fcnet_activation": "relu", "free_log_std": false, "no_final_linear": false, "vf_share_layers": false, "use_lstm": false, "max_seq_len": 20, "lstm_cell_size": 256, "lstm_use_prev_action": false, "lstm_use_prev_reward": false, "_time_major": false, "use_attention": false, "attention_num_transformer_units": 1, "attention_dim": 64, "attention_num_heads": 1, "attention_head_dim": 32, "attention_memory_inference": 50, "attention_memory_training": 50, "attention_position_wise_mlp_dim": 32, "attention_init_gru_gate_bias": 2.0, "attention_use_n_prev_actions": 0, "attention_use_n_prev_rewards": 0, "framestack": true, "dim": 84, "grayscale": false, "zero_mean": true, "custom_model": "Centralized_Critic_Model", "custom_model_config": {"env": "mpe", "env_args": {"continuous_actions": false, "max_cycles": 25, "map_name": "simple_world_comm"}, "mask_flag": false, "global_state_flag": false, "opp_action_in_cc": true, "agent_level_batch_update": false, "force_coop": false, "local_mode": false, "share_policy": "group", "evaluation_interval": 50, "framework": "torch", "num_workers": 1, "num_gpus": 0, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "checkpoint_freq": 20, "checkpoint_end": true, "restore_path": {"model_path": "", "params_path": ""}, "stop_iters": 9999999, "stop_timesteps": 2000000, "stop_reward": 999999, "seed": 321, "local_dir": "", "model_arch_args": {"hidden_state_size": 256, "core_arch": "mlp", "fc_layer": 2, "out_dim_fc_0": 128, "out_dim_fc_1": 64, "encode_layer": "128-256"}, "algorithm": "mappo", "space_obs": "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "space_act": "Discrete(20)", "num_agents": 6, "episode_limit": 25, "policy_mapping_info": {"simple_adversary": {"description": "one team attack, one team survive", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_crypto": {"description": "two team cooperate, one team attack", "team_prefix": ["eve_", "bob_", "alice_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_push": {"description": "one team target on landmark, one team attack", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_tag": {"description": "one team attack, one team survive", "team_prefix": ["adversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_spread": {"description": "one team cooperate", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}, "simple_reference": {"description": "one team cooperate", "team_prefix": ["agent_"], "all_agents_one_policy": true, "one_agent_one_policy": true}, "simple_world_comm": {"description": "two team cooperate and attack, one team survive", "team_prefix": ["adversary_", "leadadversary_", "agent_"], "all_agents_one_policy": false, "one_agent_one_policy": true}, "simple_speaker_listener": {"description": "two team cooperate", "team_prefix": ["speaker_", "listener_"], "all_agents_one_policy": true, "one_agent_one_policy": true}}, "agent_name_ls": ["leadadversary_0", "adversary_0", "adversary_1", "adversary_2", "agent_0", "agent_1"]}, "custom_action_dist": null, "custom_preprocessor": null, "lstm_use_prev_action_reward": -1}, "optimizer": {}, "horizon": null, "soft_horizon": false, "no_done_at_end": false, "env": "mpe_simple_world_comm", "observation_space": null, "action_space": null, "env_config": {}, "remote_worker_envs": false, "remote_env_batch_wait_ms": 0, "env_task_fn": null, "render_env": false, "record_env": false, "clip_rewards": null, "normalize_actions": true, "clip_actions": false, "preprocessor_pref": "deepmind", "log_level": "WARN", "callbacks": "<class 'ray.rllib.agents.callbacks.DefaultCallbacks'>", "ignore_worker_failures": false, "log_sys_usage": true, "fake_sampler": false, "framework": "torch", "eager_tracing": false, "explore": true, "exploration_config": {"type": "StochasticSampling"}, "evaluation_interval": 50, "evaluation_num_episodes": 10, "evaluation_parallel_to_training": false, "in_evaluation": false, "evaluation_config": {}, "evaluation_num_workers": 0, "custom_eval_function": null, "sample_async": false, "sample_collector": "<class 'ray.rllib.evaluation.collectors.simple_list_collector.SimpleListCollector'>", "observation_filter": "NoFilter", "synchronize_filters": true, "tf_session_args": {"intra_op_parallelism_threads": 2, "inter_op_parallelism_threads": 2, "gpu_options": {"allow_growth": true}, "log_device_placement": false, "device_count": {"CPU": 1}, "allow_soft_placement": true}, "local_tf_session_args": {"intra_op_parallelism_threads": 8, "inter_op_parallelism_threads": 8}, "compress_observations": false, "collect_metrics_timeout": 180, "metrics_smoothing_episodes": 100, "min_iter_time_s": 0, "timesteps_per_iteration": 0, "seed": 321, "extra_python_environs_for_driver": {}, "extra_python_environs_for_worker": {}, "num_gpus": 0, "_fake_gpus": false, "num_cpus_per_worker": 1, "num_gpus_per_worker": 0, "custom_resources_per_worker": {}, "num_cpus_for_driver": 1, "placement_strategy": "PACK", "input": "sampler", "input_config": {}, "actions_in_input_normalized": false, "input_evaluation": ["is", "wis"], "postprocess_inputs": false, "shuffle_buffer_size": 0, "output": null, "output_compress_columns": ["obs", "new_obs"], "output_max_file_size": 67108864, "multiagent": {"policies": {"policy_adversary_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}], "policy_leadadversary_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}], "policy_agent_": ["<class 'ray.rllib.policy.policy_template.MAPPOTorchPolicy'>", "Dict(obs:Box([-100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.\n -100. -100. -100. -100. -100. -100. -100. -100. -100. -100.], [100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100. 100.\n 100. 100. 100. 100. 100. 100.], (34,), float32))", "Discrete(20)", {}]}, "policy_map_capacity": 100, "policy_map_cache": null, "policy_mapping_fn": "<function run_cc.<locals>.<lambda> at 0x7f88aff35790>", "policies_to_train": null, "observation_fn": null, "replay_mode": "independent", "count_steps_by": "env_steps"}, "logger_config": null, "_tf_policy_handles_more_than_one_loss": false, "_disable_preprocessor_api": false, "simple_optimizer": false, "monitor": -1, "use_critic": true, "use_gae": true, "lambda": 1.0, "kl_coeff": 0.2, "sgd_minibatch_size": 50, "shuffle_sequences": true, "num_sgd_iter": 2, "lr_schedule": null, "vf_loss_coeff": 1.0, "entropy_coeff": 0.01, "entropy_coeff_schedule": null, "clip_param": 0.3, "vf_clip_param": 10.0, "grad_clip": null, "kl_target": 0.01, "vf_share_layers": -1}, "time_since_restore": 13.746174335479736, "timesteps_since_restore": 0, "iterations_since_restore": 33, "perf": {"cpu_util_percent": 17.8, "ram_util_percent": 46.6}}
