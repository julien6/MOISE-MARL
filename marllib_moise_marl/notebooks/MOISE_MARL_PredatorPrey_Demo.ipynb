{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a87bfe3",
   "metadata": {},
   "source": [
    "# ðŸ§  Using MOISE+MARL in Multi-Particle Environment (Predator-Prey)\n",
    "\n",
    "This notebook demonstrates how to apply the **MOISE+MARL framework** to the Predator-Prey scenario using the **Multi-Particle Environment (MPE)** with predefined adversarial roles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "682fd46b",
   "metadata": {},
   "source": [
    "## 1. Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e003a20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import math\n",
    "from marllib import marl\n",
    "from mma_wrapper.label_manager import label_manager\n",
    "from mma_wrapper.organizational_model import (\n",
    "    organizational_model, structural_specifications,\n",
    "    functional_specifications, deontic_specifications,\n",
    "    deontic_specification, time_constraint_type\n",
    ")\n",
    "from mma_wrapper.organizational_specification_logic import role_logic\n",
    "from mma_wrapper.utils import label, observation, action, trajectory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b0587cd",
   "metadata": {},
   "source": [
    "## 2. Define Label Manager for MPE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d65fc044",
   "metadata": {},
   "outputs": [],
   "source": [
    "class mpe_label_manager(label_manager):\n",
    "\n",
    "    def __init__(self, action_space: gym.Space = None, observation_space: gym.Space = None):\n",
    "        super().__init__(action_space, observation_space)\n",
    "        self.action_encode = {\n",
    "            \"no_action\": 0, \"move_left\": 1, \"move_right\": 2, \"move_down\": 3, \"move_up\": 4\n",
    "        }\n",
    "        self.action_decode = {v: k for k, v in self.action_encode.items()}\n",
    "        self.normal_leader_adversary_sizes = {\n",
    "            'self_vel': 2, 'self_pos': 2, 'landmark_rel_positions': 10,\n",
    "            'other_agent_rel_positions': 10, 'other_agent_velocities': 4,\n",
    "            'self_in_forest': 2, 'leader_comm': 4\n",
    "        }\n",
    "\n",
    "    def one_hot_encode_observation(self, observation, agent=None):\n",
    "        return [val for val in observation.values()]\n",
    "\n",
    "    def one_hot_decode_observation(self, observation, agent=None):\n",
    "        sizes = self.normal_leader_adversary_sizes\n",
    "        extracted = {}\n",
    "        index = 0\n",
    "        for key, size in sizes.items():\n",
    "            extracted[key] = observation[index:index+size]\n",
    "            index += size\n",
    "        return extracted\n",
    "\n",
    "    def one_hot_encode_action(self, action, agent=None):\n",
    "        return self.action_encode[action]\n",
    "\n",
    "    def one_hot_decode_action(self, action, agent=None):\n",
    "        return self.action_decode[action]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6e593c5",
   "metadata": {},
   "source": [
    "## 3. Define Role Logic Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d529d20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def leader_adversary_fun(trajectory, observation, agent_name, label_manager):\n",
    "    data = label_manager.one_hot_decode_observation(observation, agent=agent_name)\n",
    "    other_positions = data[\"other_agent_rel_positions\"]\n",
    "    other_positions = {agent: (other_positions[i*2], other_positions[i*2+1])\n",
    "                       for i, agent in enumerate(['adversary_0', 'adversary_1', 'adversary_2', 'agent_0', 'agent_1'])}\n",
    "    min_dist, min_agent = 1e5, None\n",
    "    for good_agent in [\"agent_0\", \"agent_1\"]:\n",
    "        d = math.sqrt(sum([x**2 for x in other_positions[good_agent]]))\n",
    "        if d < min_dist:\n",
    "            min_dist, min_agent = d, good_agent\n",
    "    vec = other_positions[min_agent]\n",
    "    if abs(vec[0]) > abs(vec[1]):\n",
    "        return 2 if vec[0] > 0 else 1\n",
    "    else:\n",
    "        return 4 if vec[1] > 0 else 3\n",
    "    return 0\n",
    "\n",
    "def normal_adversary_fun(trajectory, observation, agent_name, label_manager):\n",
    "    agents = ['leadadversary_0', 'adversary_0', 'adversary_1', 'adversary_2', 'agent_0', 'agent_1']\n",
    "    agents.remove(agent_name)\n",
    "    data = label_manager.one_hot_decode_observation(observation, agent=agent_name)\n",
    "    positions = data[\"other_agent_rel_positions\"]\n",
    "    positions = {agent: (positions[i*2], positions[i*2+1]) for i, agent in enumerate(agents)}\n",
    "    min_dist, min_agent = 1e5, None\n",
    "    for good_agent in [\"agent_0\", \"agent_1\"]:\n",
    "        d = math.sqrt(sum([x**2 for x in positions[good_agent]]))\n",
    "        if d < min_dist:\n",
    "            min_dist, min_agent = d, good_agent\n",
    "    vec = positions[min_agent]\n",
    "    if abs(vec[0]) > abs(vec[1]):\n",
    "        return 2 if vec[0] > 0 else 1\n",
    "    else:\n",
    "        return 4 if vec[1] > 0 else 3\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d313e764",
   "metadata": {},
   "source": [
    "## 4. Create the Organizational Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bad757",
   "metadata": {},
   "outputs": [],
   "source": [
    "mpe_model = organizational_model(\n",
    "    structural_specifications(\n",
    "        roles={\n",
    "            \"role_leader\": role_logic(label_manager=mpe_label_manager).registrer_script_rule(leader_adversary_fun),\n",
    "            \"role_normal\": role_logic(label_manager=mpe_label_manager).registrer_script_rule(normal_adversary_fun),\n",
    "            \"role_good\": role_logic(label_manager=mpe_label_manager).registrer_script_rule(normal_adversary_fun)\n",
    "        },\n",
    "        role_inheritance_relations={}, root_groups={}\n",
    "    ),\n",
    "    functional_specifications=functional_specifications(\n",
    "        goals={}, social_scheme={}, mission_preferences=[]\n",
    "    ),\n",
    "    deontic_specifications=deontic_specifications(\n",
    "        permissions=[], obligations=[\n",
    "            deontic_specification(\"role_leader\", [\"leadadversary_0\"], [], time_constraint_type.ANY),\n",
    "            deontic_specification(\"role_normal\", [\"adversary_0\", \"adversary_1\", \"adversary_2\"], [], time_constraint_type.ANY)\n",
    "        ]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aac8f85",
   "metadata": {},
   "source": [
    "## 5. Create and Wrap the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401a609c",
   "metadata": {},
   "outputs": [],
   "source": [
    "env = marl.make_env(\n",
    "    environment_name=\"mpe\",\n",
    "    map_name=\"simple_world_comm\",\n",
    "    organizational_model=mpe_model\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562a8b75",
   "metadata": {},
   "source": [
    "## 6. Initialize Algorithm and Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595adfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "mappo = marl.algos.mappo(hyperparam_source=\"test\")\n",
    "model = marl.build_model(env, mappo, {\"core_arch\": \"mlp\", \"encode_layer\": \"128-256\"})\n",
    "\n",
    "# Optional: uncomment to train\n",
    "# mappo.fit(env, model, stop={\"timesteps_total\": 1e6})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3759c21",
   "metadata": {},
   "source": [
    "## 7. Render and Analyze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007003fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "mappo.render(env, model,\n",
    "    restore_path={\n",
    "        \"params_path\": \"./exp_results/mappo_mlp_simple_world_comm_copy/.../params.json\",\n",
    "        \"model_path\": \"./exp_results/mappo_mlp_simple_world_comm_copy/.../checkpoint-20\",\n",
    "        \"render_env\": True\n",
    "    },\n",
    "    local_mode=True,\n",
    "    share_policy=\"group\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d36c78a",
   "metadata": {},
   "source": [
    "## âœ… Conclusion\n",
    "In this notebook, we have:\n",
    "- Defined role-specific logic for predators\n",
    "- Created an organizational model with MOISE+\n",
    "- Executed rendering and role-based behavior analysis in the MPE Predator-Prey environment."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
